---
layout: default
title: AWS S3
description: This guide explains how to configure AWS S3 as the underlying storage layer.
parent: Prepare Your Storage
grand_parent: Set up lakeFS
nav_order: 10
has_children: false
---

# Prepare Your AWS S3 Bucket

1. From the S3 Administration console, choose `Create Bucket`.
2. Make sure that you:
   1. Block public access.
   2. Disable Object Locking.

## Metadata only (minimal permissions)

lakeFS requires permissions to interact with the `_lakefs` namespace under your bucket, in which the metadata
objects are stored ([learn more](../../understand/versioning-internals.md#constructing-a-consistent-view-of-the-keyspace-ie-a-commit)).  
The following is a minimal bucket policy. To add it, go to the `Permissions` tab, and paste it as :

```json
{
  "Id": "<POLICY_ID>",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "<STMT_ID>",
      "Action": [
         "s3:GetObject",
         "s3:PutObject",
         "s3:ListBucket",
         "s3:GetBucketLocation"
      ],
      "Effect": "Allow",
      "Resource": ["arn:aws:s3:::<STORAGE_NAMESPACE>/_lakefs", "arn:aws:s3:::<STORAGE_NAMESPACE>/_lakefs/*"],
      "Principal": {
        "AWS": ["arn:aws:iam::<ACCOUNT_ID>:role/<IAM_ROLE>"]
      }
    }
  ]
}
```

Replace `<STORAGE_NAMESPACE>` with your [storage namespace](../../understand/model.md#concepts-unique-to-lakefs) which will be in the form of `<BUCKET_NAME>/<PATH>` (`PATH` can be empty).
Replace `<ACCOUNT_ID>` and `<IAM_ROLE>` with values relevant to your environment.
`IAM_ROLE` should be the role assumed by your lakeFS installation.

Alternatively, if you use an AWS user's key-pair to authenticate lakeFS to AWS, change the policy's Principal to be the user:

```json
 "Principal": {
   "AWS": ["arn:aws:iam::<ACCOUNT_ID>:user/<IAM_USER>"]
 }
```

### Usage

By setting this policy you'll be able to perform only metadata operations through lakeFS, meaning that you'll not be able
to use lakeFS to upload or download objects.
This permission is useful if you upload/download objects to/from your bucket using external tools.
For example, you can use the [lakeFS Hadoop FileSystem Spark integration](../../integrations/spark.md#access-lakefs-using-the-lakefs-specific-hadoop-filesystem) 
to directly access your S3 bucket while performing metadata operations through lakeFS on the objects in that bucket.

## Data & Metadata (extended permissions)

If you plan to transfer data using lakeFS, you'll need to extend the [minimal permissions](#metadata-only-minimal-permissions)
with the following policy:

```json
{
   "Id": "<POLICY_ID>",
   "Version": "2012-10-17",
   "Statement": [
      {
         "Sid": "<STMT_ID>",
         "Action": [
            "s3:GetObject",
            "s3:PutObject",
            "s3:AbortMultipartUpload",
            "s3:ListMultipartUploadParts",
            "s3:ListBucket",
            "s3:GetBucketLocation",
            "s3:ListBucketMultipartUploads"
         ],
         "Effect": "Allow",
         "Resource": ["arn:aws:s3:::<STORAGE_NAMESPACE>", "arn:aws:s3:::<STORAGE_NAMESPACE>/*"],
         "Principal": {
            "AWS": ["arn:aws:iam::<ACCOUNT_ID>:role/<IAM_ROLE>"]
         }
      }
   ]
}
```

Notice that this time you allow lakeFS to access your storage namespace. This is necessary to upload/download objects to/from
your underlying bucket namespace.

### Usage

By setting this bucket policy you'll be able to perform metadata operations as well as to upload/download objects to/from your bucket using lakeFS.
To upload or download an object to your storage namespace using lakeFS, you can use the [API](../../reference/api.md), the CLI tool
[`lakectl`](../../reference#lakectl-fs), the [Spark metadata client](../../reference/spark-client.md),
or the [S3 gateway](../../integrations/spark.md#access-lakefs-using-the-s3a-gateway).

---

You're now ready to [create your first lakeFS repository](../create-repo.md).
