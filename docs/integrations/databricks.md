---
title: Databricks
description: This section points to the various integrations with Databricks.
parent: Integrations
---

# Using lakeFS with Databricks

{% include toc_2-3.html %}

## Overview

Databricks is a unified, open analytics platform for building, deploying, sharing,
and maintaining enterprise-grade data, analytics, and AI solutions at scale.

In this document, we will cover the various Databricks products and how they integrate
with lakeFS.

<p class="center">
    <img src="{{ site.baseurl }}/assets/img/Databricks-arch.png"/>
</p>

## Databricks Compute Options

Databricks offers several compute options for running workloads. All of these options
can be used with lakeFS. At a basic level, Databricks compute products are Spark clusters
that run on top of cloud infrastructure. They offer different ways of configuration
and manageability on top of Spark. From a lakeFS integration perspective, the main difference
is how you configure the storage operations that perform read/writes to lakeFS.

lakeFS storage operations will either use the [lakeFS Hadoop Filesystem](./spark#lakefs-hadoop-filesystem),
which utilizes the [lakeFS OpenAPI](../understand/architecture.md#openapi-server),
or the s3a Filesystem, which uses the [lakeFS S3 Gateway](../understand/architecture.md#s3-gateway).
In short, the lakeFS S3 Gateway is the fastest way to get started, but it routes
all traffic through the lakeFS server.
The lakeFS Hadoop Filesystem requires more setup, but all data transfers will occur
directly on the bucket. The lakeFS Hadoop Filesystem can write to storage
using the s3a filesystem or using [pre-signed](./spark#hadoop-filesystem-in-presigned-mode)
URLs generated by the lakeFS server. To read more about the alternatives, see
the [Spark integration page](./spark.md#using-lakefs-with-apache-spark).

### All-Purpose Compute

Provisioned compute used to analyze data in notebooks.
You can create, terminate, and restart this compute using the UI, CLI, or REST API.

When you create a Databricks compute cluster, you can configure it to use lakeFS
with the [lakeFS Hadoop Filesystem](./spark.md#lakefs-hadoop-filesystem) (see Databricks installation guide)
or the [lakeFS S3 Gateway](./spark.md#s3-compatible-api). The lakeFS S3 Gateway can be configured
in the [notebook](https://docs.lakefs.io/integrations/spark.html#configuration) or during
cluster setup (_Advanced Options -> Spark -> Spark config_).

### Jobs Compute

Provisioned compute used to run automated jobs.
The Databricks job scheduler automatically creates a job compute whenever a job is configured to run on new compute.

To use lakeFS with Databricks jobs, a compute cluster needs to be configured in the cluster setup,
just like with All-Purpose compute. Serverless compute for Databricks jobs is currently not supported.

### Warehouses

Classic & Pro warehouses are used to run SQL commands on data objects in the SQL editor or interactive notebooks.
Serverless warehouses do the same, except that they are on-demand elastic compute.

All warehouses do not allow the installation of external jars, such as the lakeFS Hadoop Filesystem.
To use SQL warehouses with lakeFS, utilize the [lakeFS S3 Gateway](./spark.md#configuring-databricks-sql-warehouse-with-the-s3-compatible-api).

## Unity Catalog

Unity Catalog is Databricks' metastore that provides centralized access control,
auditing, lineage, and data discovery capabilities across Databricks workspaces.

lakeFS can be used with Unity Catalog to provide a versioned view of the data and
the Unity Catalog metadata, like versioned catalogs, schemas, and tables.

lakeFS support for Unity Catalog differs between lakeFS OSS and lakeFS Enterprise & Cloud.

### Catalog Exports <span class="badge">Open-Source</span>

Leveraging the external tables feature within Unity Catalog,
you can register a Delta Lake table exported from lakeFS and access it through the unified catalog.

[Catalog Exports](../howto/catalog_exports.md) relies on [lakeFS Actions](../howto/hooks) and offers
a way to export changes from lakeFS to Unity Catalog.

For the full guide on how to use Catalog Exports with Unity Catalog, see the [documentation](./unity-catalog.md).

### lakeFS for Databricks <span class="badge">Cloud</span> <span class="badge">Enterprise</span>

An all-in-one solution that provides a seamless integration between lakeFS and Unity Catalog.

For more information, visit the lakeFS for Databricks [page](https://lakefs.io/lakefs-for-databricks/).

## Delta Lake

lakeFS can be used with Delta Lake to provide a versioned view of the data and the Delta Lake metadata.
Read the [Delta Lake docs](./delta.md) for more information.
