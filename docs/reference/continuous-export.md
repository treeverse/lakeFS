---
layout: default
title: Continuous export
parent: Reference
nav_order: 10
has_children: false
---
# Continuous Export

You can configure a lakeFS repository to store the latest of a branch on an external
object store using its native paths and access methods.  This allows clients and tools
read-only access to repository objects without any lakeFS-specific configuration.
Initially, only AWS S3 is supported.

For instance, the contents `lakefs://example@master` might be stored on
`s3://company-bucket/example/latest`.  Clients entirely unaware of lakeFS could use that
base URL to access latest files on `master`.  Clients aware of lakeFS can continue to use
the lakeFS S3 endpoint to access repository files on `s3://example/master`, as well as
other versions and uncommitted versions.

A one-time export copies all objects on a branch to a location on S3, once.  A continuous
export tracks the latest state of a branch on a location in S3: it updates that location
in S3 after each change in lakeFS (either a commit or a merge)

## Configuration

Configure continuous export for branch with a `POST` to
`/repositories/{repository}/branches/{branch}/continuous-export`.  Pass these JSON
parameters in the body:
```json
{
  "exportPath": "s3://company-bucket/path/to/export", // export objects to this path (required)
  "exportStatusPath": "s3://company-bucket/path/to/status", // write status to this path (optional)
  "lastKeysInPrefixRegexp": [
    // list of regexps of keys to exported last in each prefix (for signalling) (optional)
    ".*/_SUCCESS"
  ]
}
```
Once set on a branch, every commit or merge to that branch will be exported.

To stop continuous export, `DELETE` that URL.  `GET` also works to retrieve configuration
of continuous export.

You can configure continuous export on any number of your branches.

Export current branch state just once, without continuous operation, with `PUT` of a
similar object to `/repositories/{repository}/branches/{branch}/export`.

## Operation

When continuous export is enabled for a branch its current state is exported to S3.  From
then on, every commit or merge operation to that branch will be exported.  The commit or
merge operation on the branch returns as soon as export starts.  Multiple commits to the
same exported branch may cause some otherwise-overlapping exports to be skipped.
Eventually the last export will appear.  This is in line with "eventual consistency" of
S3.

The export process runs for a while and copies all files.  This process is highly parallel
and does not maintain the creation or modification orders of the source files.  For
example, a file `_SUCCESS` generated by Hadoop or Spark to lakeFS before the export might
by default be written before the other files of its run.  Set regexps matching such
trigger files in the optional list `lastKeysInPrefixRegexp`.  They will be written last in
their S3 prefix, allowing triggers based on S3 write ordering to succeed.

For more granular status tracking, set a `exportStatusPath`.  Export writes two files to that path:

1. A file `_STATUS_<timestamp>.json`, updated on every status change to hold a single JSON
   record with fields
  - `"status"`: one of `"success"`, `"failure"`, `"pending"`, or `"in-progress"`.
  - `"message"`: a human-readable message, the error message for status `"failure"`.
  - `"commit"`: the commit-ID of the export.
  - (maybe?) `"commit-message"`: the message associated with that commit.
2. A file `_MANIFEST_<timestamp>.json` written when entering state `in-progress`.  It
   contains newline-separated records for each exported file with fields
  - `"path"`: the path of the object within the bucket.  The exported object will be on
    that path under the export path specified.
  - `"etag"`: the etag of the exported object.  You can use this to ensure the correct
    version of the object is visible.
  - (maybe?) `"version-id"`: the version ID of the object, if versioned.  (Without a
    version ID it can be very hard to retrieve a non-latest version of the object!)

Note that S3 does not offer atomic operations.  Objects read are only valid while the
above `_STATUS` file shows `"success"` and the `"commit"` is unchanged!

## FAQs

#### What happens if a continuously exported branch changes while a previous export is still in progress?

lakeFS exports objects safely and will not export the same branch twice concurrently.
However when multiple concurrent exports occur intermediate exports cannot be observed and
may be dropped.  A dropped intermediate export does not create the `_SUCCESS` or
`_STATUS_<timestamp>.json` and `_MANIFEST_<timestamp>.json` files; conversely, once these
appear the export will run to completion.

#### How consistent is export once the `_STATUS_<timestamp>.json` file appears?

Once the `_STATUS_<timestamp>.json` file holds status `success`, the export has finished,
and S3 rules apply:

* Any file can be accessed.
* Files names appear in the `_MANIFEST_<timestamp>.json` file.
* However S3 file listings will only eventually show all accessible files.

lakeFS exports to S3 and is bound by its consistency guarantees.

#### How can I trigger processes external to lakeFS to run after an export?

The file `_STATUS_<timestamp>.json` contains success -- and a relevant commit-ID -- once
export is done.  You can use the file and its contents to trigger after an export ends.

You can also use existing workflows that are based on `_SUCCESS` files (like those created
by Hadoop or by Spark).  Just configure the a regular expression matching those files in
`lastKeysInPrefixRegexp`, to ensure that each appears last in its S3 prefix.

#### Can I export lakeFS metadata to S3?

Configuration option `exportStatusPath` exports some metadata about the exported commit to S3:
* Export status
* Commit ID and message
* Manifest of all objects in the commit, including:
  - all keys
  - ETag of each file
