version: "3.8"
services:
  lakefs:
    extends:
      file: docker-compose-common.yaml
      service: lakefs
    depends_on:
      - "dynamodb"
    volumes:
      - lakefs-app:/app:ro
    environment:
      - LAKEFS_DATABASE_TYPE=dynamodb
      - LAKEFS_DATABASE_DYNAMODB_ENDPOINT=http://dynamodb:8000
      - LAKEFS_DATABASE_DYNAMODB_AWS_REGION=us-east-1
      - LAKEFS_DATABASE_DYNAMODB_AWS_ACCESS_KEY_ID=AKIAIO5FODNN7EXAMPLE
      - LAKEFS_DATABASE_DYNAMODB_AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY
      - LAKEFS_BLOCKSTORE_TYPE=s3
      - LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}

  dynamodb:
    extends:
      file: ${OSS_ESTI_OPS_PATH:-esti/ops}/docker-compose-common.yaml
      service: dynamodb

  esti:
    extends:
      file: ${OSS_ESTI_OPS_PATH:-esti/ops}/docker-compose-common.yaml
      service: esti
    depends_on:
      spark:
        condition: service_healthy
    environment:
      - LAKEFS_ACCESS_KEY_ID=${LAKEFS_ACCESS_KEY_ID}
      - LAKEFS_SECRET_ACCESS_KEY=${LAKEFS_SECRET_ACCESS_KEY}
    volumes:
      - lakefs-code:/lakefs
      - lakefs-app:/app:ro
      - shared-spark:/opt/bitnami/spark

  spark:
    extends:
      file: ${OSS_ESTI_OPS_PATH:-esti/ops}/docker-compose-common.yaml
      service: spark
    container_name: lakefs-spark
    volumes:
      - shared-metaclient:/opt/metaclient
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - LAKEFS_ACCESS_KEY_ID=${LAKEFS_ACCESS_KEY_ID}
      - LAKEFS_SECRET_ACCESS_KEY=${LAKEFS_SECRET_ACCESS_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 5s
      timeout: 5s
      retries: 10
    command: >
      spark-submit
        --master spark://spark:7077
        --conf spark.hadoop.hadoop.security.authentication=Simple
        --conf spark.hadoop.lakefs.api.url=http://lakefs:8000/api/v1
        --conf spark.hadoop.lakefs.api.access_key=${LAKEFS_ACCESS_KEY_ID}
        --conf spark.hadoop.lakefs.api.secret_key=${LAKEFS_SECRET_ACCESS_KEY}
        --conf spark.driver.extraJavaOptions=-Dhadoop.job.ugi=spark
        --conf spark.executor.extraJavaOptions=-Dhadoop.job.ugi=spark
        --class io.treeverse.gc.GarbageCollection /opt/metaclient/spark-assembly.jar

volumes:
  lakefs-code:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${LAKEFS_ROOT:-.}
  lakefs-app:
  shared-spark:
  shared-metaclient:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./esti/ops/test/spark/metaclient
