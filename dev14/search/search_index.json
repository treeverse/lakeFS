{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Lake!","text":"lakeFS brings software engineering best practices and applies them to data <p>lakeFS provides version control over the data lake, and uses Git-like semantics to create and access those versions. If you know git, you'll be right at home with lakeFS.</p> <p>With lakeFS, you can apply concepts to your data lake such as branching to create an isolated version of the data, committing to create a reproducible point in time, and merging in order to incorporate your changes in one atomic action.</p> \ud83d\udcfd\ufe0f lakeFS in under 2 minutes <p> </p>"},{"location":"#how-do-i-get-started","title":"How Do I Get Started?","text":"<p>The hands-on quickstart guides you through some core features of lakeFS.</p> <p>These include branching, merging, and rolling back changes to data.</p> <p>Tip</p> <p>You can use the 30-day free trial of lakeFS Cloud if you want to try out lakeFS without installing anything.</p>"},{"location":"#key-lakefs-features","title":"Key lakeFS Features","text":"<ul> <li>It is format-agnostic and works with both structured and unstructured data</li> <li>It works with numerous data tools and platforms.</li> <li>Your data stays in place, with no need to copy existing data</li> <li>It eliminates the need for data duplication using zero-copy branching.</li> <li>It maintains high performance over data lakes of any size</li> <li>It includes configurable garbage collection capabilities</li> <li>It is proven in production and has an active community</li> </ul>"},{"location":"#how-does-lakefs-work-with-other-tools","title":"How Does lakeFS Work With Other Tools?","text":"<p>lakeFS is an open source project that supports managing data in AWS S3, Azure Blob Storage, Google Cloud Storage (GCS), S3-Compatible storage solutions and even locally mounted directories. It integrates seamlessly with popular data frameworks such as Spark, AWS SageMaker, Pandas, Tensorflow, Polars, HuggingFace Datasets and many more.</p> <p>With lakeFS, you can use any of the tools and libraries you are used to work with to read and write data directly from a repository.</p> <p>Example: lakeFS with Pandas</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; df = pd.read_csv('lakefs://example-repository/main-branch/path/to.csv')\n</code></pre> <p>Using this method, lakeFS acts as a metadata layer: it figures out which objects need to be fetched from the underlying storage for that version of the data and then lets the client read or write these files directly from the storage using pre-signed URLs. This allows lakeFS to be both very efficient but also highly secure:</p> <p> </p> <p>Additionally, lakeFS maintains compatibility with the S3 API to minimize adoption friction. You can use it as a drop-in replacement for S3 from the perspective of any tool interacting with a data lake.</p> <p>Example</p> <p>For example, take the common operation of reading unstructured data from the object store using Boto3 (Python):</p> <pre><code>&gt;&gt;&gt; import boto3\n&gt;&gt;&gt;\n&gt;&gt;&gt; s3 = boto3.resource('s3')\n&gt;&gt;&gt; obj = s3.Object('example-repository', 'main-branch/path/image.png')\n&gt;&gt;&gt; image_data = obj.get()['Body'].read()\n</code></pre> <p>You can use the same methods and syntax you are already using to read and write data when using a lakeFS repository. This simplifies the adoption of lakeFS - minimal changes are needed to get started, making further changes an incremental process.</p>"},{"location":"#lakefs-is-git-for-data","title":"lakeFS is Git for Data","text":"<p>Git became ubiquitous when it comes to code because it had best supported engineering best practices required by developers, in particular:</p> <ul> <li>Collaborate during development.</li> <li>Reproduce and troubleshoot issues with a given version of the code</li> <li>Develop and Test in isolation</li> <li>Revert code to a stable version in case of an error</li> <li>Continuously integrate and deploy new code (CI/CD)</li> </ul> <p>lakeFS provides these exact benefits, that data practitioners are missing today, and enables them a clear intuitive Git-like interface to easily manage data like they manage code. Through its versioning engine, lakeFS enables the following built-in operations familiar from Git:</p> <ul> <li> Branch a consistent copy of a repository, isolated from other branches and their changes. Initial creation of a branch is a metadata operation that does not duplicate objects.</li> <li> Commit an immutable checkpoint containing a complete snapshot of a repository.</li> <li> Merge performed between two branches \u2014 merges atomically update one branch with the changes from another.</li> <li> Revert returns a repo to the exact state of a previous commit.</li> <li> Tag a pointer to a single immutable commit with a readable, meaningful name.</li> <li> Hooks run validations and actions when actions occur (<code>pre-merge</code>, <code>post-create-branch</code>, etc).</li> </ul> <p>Info</p> <p>See the object model for an in-depth definition of these, and the CLI reference for the full list of commands.</p> <p>Incorporating these operations into your data and model development provides the same collaboration and organizational benefits you get when managing application code with source control.</p>"},{"location":"#how-can-lakefs-help-me","title":"How Can lakeFS Help Me?","text":"<p>lakeFS helps you maintain a tidy data lake in several ways, including:</p>"},{"location":"#reproducibility-what-did-my-data-look-like-at-a-point-in-time","title":"Reproducibility: What Did My Data Look Like at a Point In Time?","text":"<p>Being able to look at data as it was at a given point is particularly useful in at least two scenarios:</p> <ol> <li> <p>Reproducibility of ML experiments</p> <p>ML experimentation is iterative, requiring the ability to reproduce specific results. With lakeFS, you can version all aspects of an ML experiment, including the data. This enables:</p> <p>Data Lineage: Track the transformation of data from raw datasets to the final version used in experiments, ensuring transparency and traceability.</p> <p>Zero-Copy Branching: Minimize storage use by creating lightweight branches of your data, allowing for easy experimentation across different versions.</p> <p>Easy Integration: Seamlessly integrate with ML tools like MLFlow, linking experiments directly to the exact data versions used, making reproducibility straightforward.</p> <p>lakeFS enhances your ML workflow by ensuring that all versions of data are easily accessible, traceable, and reproducible.</p> </li> <li> <p>Troubleshooting production problems</p> <p>In some cases, a user might report inconsistencies, question the accuracy, or simply report data or inference results as incorrect.</p> <p>Since data continuously changes, it is challenging to understand its state at the time of the error.</p> <p>With lakeFS you can create a branch from a commit to debug an issue in isolation.</p> </li> </ol> <p>\ud83d\udc49\ud83c\udffb Read More</p>"},{"location":"#collaboration-during-development-and-training","title":"Collaboration during development and training","text":"<p>With lakeFS, each member of the team can create their own branch, isolated from other people's changes.</p> <p>This allows to iterate on changes to an algorithm or transformation, without stepping on eachother's toes. These branches are centralized - they could be share among users for collaboration, and can even be merged.</p> <p>With lakeFS you can even open pull requests, allowing you to easily share changes with other members and collaborate on them.</p>"},{"location":"#isolated-devtest-environments-with-zero-copy-branching","title":"Isolated Dev/Test Environments with zero-copy branching","text":"<p>lakeFS makes creating isolated dev/test environments for transformations, model development, parallel experiments, and ETL processes- achieved through the use of zero-copy branches. This enables you to test and validate code changes on production data without impacting it, as well as run analysis and experiments on production data in an isolated clone.</p> <p>\ud83d\udc49\ud83c\udffb Read More</p>"},{"location":"#rollback-of-data-changes-and-recovery-from-data-errors","title":"Rollback of Data Changes and Recovery from Data Errors","text":"<p>Human error or misconfigurations can lead to erroneous data making its way into production or critical data being accidentally deleted. Traditional backups are often inadequate for recovery in these situations, as they may be outdated and require time-consuming object-level sifting.</p> <p>With lakeFS, you can avoid these inefficiencies by committing snapshots of data at well-defined times. This allows for instant recovery: simply identify a good historical commit and restore or copy from it with a single operation.</p> <p>\ud83d\udc49\ud83c\udffb Read More</p>"},{"location":"#establishing-data-quality-guarantees-write-audit-publish","title":"Establishing data quality guarantees - Write-Audit-Publish","text":"<p>The best way to deal with mistakes is to avoid them. A data source that is ingested into the lake introducing low-quality data should be blocked before exposure if possible.</p> <p>With lakeFS, you can achieve this by tying data quality tests to commit and merge operations via lakeFS hooks.</p> <p>\ud83d\udc49\ud83c\udffb Read more</p>"},{"location":"#next-step","title":"Next Step","text":"<p>Try lakeFS on the cloud or run it locally</p>"},{"location":"cloud/","title":"lakeFS Cloud","text":""},{"location":"cloud/#what-is-lakefs-cloud","title":"What is lakeFS cloud?","text":"<p>lakeFS Cloud is a single tenant, fully-managed lakeFS solution, providing high availability, auto-scaling, support and production-grade features.</p>"},{"location":"cloud/#why-did-we-build-lakefs-cloud","title":"Why did we build lakeFS cloud?","text":"<p>We built lakeFS cloud for three main reasons:</p> <ol> <li>We wanted to provide organizations with the benefits of lakeFS without the need to manage it, saving them the investment in infrastructure and work related to installation, upgrades, uptime and scale .</li> <li>We wanted to provide lakeFS cloud users with security that meets their needs, with SSO, SCIM, and RBAC.</li> <li>We wanted to provide additional functionality that reduces friction and allows fast implementation of version controlled data/ML/AI pipelines throughout their data lifecycle.</li> </ol>"},{"location":"cloud/#what-is-the-value-of-using-lakefs-cloud-as-a-managed-service","title":"What is the value of using lakeFS Cloud as a managed service?","text":"<p>The main advantages of using lakeFS Cloud, the lakeFS managed service are:</p> <ol> <li>No installation required, no cloud costs and devops efforts on installing and maintaining a lakeFS installation.</li> <li>All lakeFS services are managed and run by us, including Managed Garbage Collection.</li> <li>lakeFS cloud is highly available and includes a commitment to an uptime SLA.</li> <li>lakeFS cloud auto scales according to your needs. See lakeFS cloud scalability model for more details.</li> <li>Upgrades are done transparently on your lakeFS cloud environment</li> <li>The lakeFS team is committed to supporting you with an SLA for both issues and product enhancements.</li> </ol>"},{"location":"cloud/#which-security-features-does-lakefs-cloud-provide","title":"Which security features does lakeFS Cloud provide?","text":"<ol> <li>lakeFS Cloud is SOC2 Type II compliant. Contact us to get the certification.</li> <li>lakeFS Cloud version controls your data, without accessing it, using pre-signed URLs! Read more here.</li> <li>When using lakeFS Cloud, you are provided with a rich Role-Based Access Control functionality that allows for fine-grained control by associating permissions with users and groups, granting them specific actions on specific resources. This ensures data security and compliance within an organization.</li> <li>To easily manage users and groups, lakeFS Cloud provides SSO integration (including support for SAML, OIDC, AD FS, Okta, and Azure AD), supporting existing credentials from a trusted provider, eliminating separate logins.</li> <li>lakeFS Cloud supports SCIM for automatically provisioning and deprovisioning users and group memberships to allow organizations to maintain a single source of truth for their user database.</li> <li>STS Auth offers temporary, secure logins using an Identity Provider, simplifying user access and enhancing security.</li> <li>Authentication with AWS IAM Roles allows authentication using AWS IAM roles instead of lakeFS credentials, removing the need to maintain static credentials for lakeFS Enterprise users running on AWS.</li> <li>Auditing provides a detailed action log of events happening within lakeFS, including who performed which action, on which resource - and when.</li> <li>Private-Link support to ensure network security by only allowing access to your lakeFS Cloud installation from your cloud accounts</li> </ol>"},{"location":"cloud/#what-additional-functionality-does-lakefs-cloud-provide","title":"What additional functionality does lakeFS Cloud provide?","text":"<p>Using lakeFS cloud is not just a secure and managed way of using lakeFS OSS; it is much more than that. With lakeFS Cloud you enjoy:</p> <ol> <li>lakeFS Mount allows users to virtually mount a remote lakeFS repository onto a local directory. Once mounted, users can access the data as if it resides on their local filesystem, using any tool, library, or framework that reads from a local filesystem.</li> <li>lakeFS Metadata Search - Allows a granular search API to filter and query versioned objects based on attached metadata. This is especially useful for machine learning environments to filter by labels and file attributes</li> <li>Iceberg REST Catalog - Provides full support for managing Iceberg tables alongside other data formats in the same lakeFS repository. Built using open standards and works with any Iceberg client.</li> <li>lakeFS for Snowflake - Provides full integration into the Snowflake ecosystem, including full support for Iceberg managed tables.</li> <li>lakeFS Cross-Cloud - Allows central management of repositories that span across multiple cloud providers including Azure, AWS, GCP and on-prem environments.</li> <li>Transactional Mirroring - allows replicating lakeFS repositories into consistent read-only copies in remote locations.</li> </ol> Feature OSS Cloud Format-agnostic data version control \u2705 \u2705 Cloud-agnostic \u2705 \u2705 Zero Clone copy for isolated environment \u2705 \u2705 Atomic Data Promotion (via merges) \u2705 \u2705 Data stays in one place \u2705 \u2705 Configurable Garbage Collection \u2705 \u2705 Data CI/CD using lakeFS hooks \u2705 \u2705 Integrates with your data stack \u2705 \u2705 Role Based Access Control (RBAC) \u2705 Single Sign On (SSO) \u2705 SCIM Support \u2705 IAM Roles \u2705 (AWS) Mount Capability \u2705 Audit Logs \u2705 Transactional Mirroring (cross-region) \u2705 Managed Service (auto updates, scaling) \u2705 Managed Garbage Collection \u2705 SOC2 Compliant \u2705 Support SLA \u2705"},{"location":"cloud/#how-lakefs-cloud-interacts-with-your-infrastructure","title":"How lakeFS Cloud interacts with your infrastructure","text":"<p>Treeverse hosts and manages a dedicated lakeFS instance that interfaces with data held in your object store, such as S3.</p> <pre><code>flowchart TD\n    U[Users] --&gt; LFC\n\n    subgraph Your Infrastructure\n    IAMM[lakeFS Managed GC IAM Role] --&gt; ObjectStore[Client's Object Store]\n    IAMA[lakeFS Application IAM Role] --&gt; ObjectStore\n    end\n\n    subgraph Treeverse's Infrastructure\n    MGC[Managed Garbage Collection] --&gt; EMR[Elastic Map Reduce]\n    EMR --&gt; IAMM\n    MGC --&gt; CP\n    CP[Control Plane]\n    LFC --&gt; CP\n\n        subgraph Client's Tenant\n        LFC[lakeFS Cloud] --&gt; DB[Refstore Database]\n        end\n\n    LFC --&gt; IAMC[lakeFS Connector IAM Role]\n    IAMC --&gt;|ExternalID| IAMA\n    end</code></pre>"},{"location":"cloud/#setting-up-lakefs-cloud","title":"Setting up lakeFS Cloud","text":""},{"location":"cloud/#aws-azure","title":"AWS / Azure","text":"<p>Please follow the self-service setup wizard on lakeFS Cloud.</p>"},{"location":"cloud/#gcp","title":"GCP","text":"<p>Please contact us for onboarding instructions.</p>"},{"location":"cloud/#lakefs-cloud-scalability-model","title":"lakeFS Cloud Scalability Model","text":"<p>By default, a lakeFS Cloud installation supports:</p> <ul> <li>1,500 read operations/second across all branches on all repositories within a region</li> <li>1,500 write operations per second across all branches on all repositories within a region</li> </ul> <p>This limit can be increased by contacting support. </p> <p>Each lakeFS branch can sustain up to a maximum of 1,000 write operations/second and 3,000 read operations per second.  This scales horizontally, so for example, with 10 concurrent branches, a repository could sustain 10k writes/second and 30k reads/second, assuming load is distributed evenly between them.</p> <p>Reading committed data (e.g. from a commit ID or tag) could be scaled up horizontally to any desired capacity, and defaults to ~5,000 reads/second.</p>"},{"location":"datamanagment/metadata-search/","title":"Metadata Search","text":"<p>Info</p> <p>Available in lakeFS Enterprise</p> <p>Note</p> <p>lakeFS Metadata search is currently in private preview for lakeFS Enterprise customers. Contact us to get started!</p>"},{"location":"datamanagment/metadata-search/#overview","title":"Overview","text":"<p>lakeFS Metadata Search makes large-scale data lakes easily searchable by object metadata, while adding the power of versioning to search. This enables reproducible queries which are essential in collaborative  and ML-driven environments where data evolves constantly and metadata is key to making informed decisions. </p> <p>With Metadata Search, you can query object metadata by both:</p> <ul> <li>System metadata: Automatically captured properties such as object path, size, last modified time, and committer.</li> <li>User-defined metadata: Custom labels, annotations, or tags stored as lakeFS object metadata, typically added during ingestion, processing, or curation.</li> </ul> <p></p> <p>To enable simple and scalable search, lakeFS exposes object metadata as versioned Iceberg tables, fully compatible with clients like DuckDB, PyIceberg, Spark, Trino, and others - enabling fast, expressive queries across any lakeFS version.  See How It Works for details. </p> <p>```</p>"},{"location":"datamanagment/metadata-search/#benefits","title":"Benefits","text":"<ul> <li>Scalable: Search metadata across millions or billions of objects.</li> <li>Query Reproducibility: Run metadata queries against specific commits or tags for consistent results.</li> <li>No infrastructure burden: lakeFS manages metadata collection and indexing natively: no need to build, deploy or  maintain a separate metadata tracking system.</li> </ul>"},{"location":"datamanagment/metadata-search/#use-cases","title":"Use cases","text":"<ul> <li>Data Discovery &amp; Exploration: Quickly find relevant data using flexible filters (e.g., annotations, object size, timestamps).</li> <li>Data Governance: Audit metadata tags, detect sensitive data (like PII), and ensure objects are properly labeled with  ownership or classification to support internal policies and external compliance requirements.</li> <li>Operational Troubleshooting: Filter and inspect data using metadata like workflow ID or publish time to trace lineage,  debug pipeline issues, and understand how data was created or modified - all within a specific lakeFS version. </li> </ul>"},{"location":"datamanagment/metadata-search/#how-it-works","title":"How it Works","text":"<p>lakeFS Metadata Search is built on top of lakeFS Iceberg support,  using catalog-level system tables to manage and expose versioned object metadata for querying.</p> <p>Metadata Search works by enabling metadata indexing on selected repositories and branches (see  Configuration). Once configured, lakeFS automatically:</p> <ol> <li>Creates a metadata repository for each selected data repository, following the naming convention: <code>&lt;repo&gt;-metadata</code>,  where <code>repo</code> is the data repository id.  </li> <li>Creates matching branches in the metadata repository for each configured branch in the data repository. For example, a <code>dev</code> branch in data repository <code>my-repo</code>, will have a corresponding <code>dev</code> branch in <code>my-repo-metadata</code>.</li> <li>Maintains a versioned Iceberg object metadata table on each matching branch in the metadata repository.</li> <li>Continuously syncs metadata via a background processing pipeline that keeps the object metadata tables    eventually consistent with changes in the corresponding data repository branches.</li> <li>Wraps the lakeFS Iceberg REST catalog to translate table queries issued against data repository references (branches, commits, or tags) and resolve them to the corresponding tables in the metadata repository. This indirection allows metadata queries to be expressed entirely in the data repository namespace, while lakeFS handles the mapping to the underlying metadata  storage.</li> </ol>"},{"location":"datamanagment/metadata-search/#querying-metadata","title":"Querying Metadata","text":"<p>Metadata queries are always performed through the data repository namespace, not the metadata repository that lakeFS manages internally. You can query metadata by: </p> <ul> <li>Branch name: <code>&lt;repo&gt;.&lt;branch&gt;.system.object_metadata</code>, to search the latest metadata state on a given branch.</li> <li>Commit ID: <code>&lt;repo&gt;.&lt;commit_id&gt;.system.object_metadata</code>, to retrieve the metadata snapshot at a specific commit.</li> <li>Tag name: <code>&lt;repo&gt;.&lt;tag_name&gt;.system.object_metadata</code>, to retrieve the metadata snapshot at a tagged commit.</li> </ul> <p>Tip</p> <p>Using commit IDs or tags ensures reproducible queries , letting you always access the  exact metadata state from an immutable point in time.</p> <p>Queries are executed through the lakeFS Iceberg REST catalog, which is fully compatible with standard engines like Trino, DuckDB, Spark, PyIceberg, and others.</p> <p>Info</p> <p>You can use Metadata Search even without a license for full Iceberg support in lakeFS. The feature relies on the lakeFS-managed Iceberg REST catalog for querying object metadata, but it can work  side-by-side with any other catalog you use to manage additional Iceberg tables. </p>"},{"location":"datamanagment/metadata-search/#object-metadata-table-schema","title":"Object Metadata Table Schema","text":"<p>Each row in the lakeFS object metadata table represents the latest metadata version for a given object on the corresponding branch. The table contains at most one row per object, ensuring that query performance remains consistent and predictable.</p> Column name Required Data Type Description repository yes string The name of the repository where the object is stored. path yes string The unique path identifying the object within the repository. commit_id yes string The latest commit ID where the object was added or modified. size yes Long The object's size in bytes. last_modified yes Timestamp The time the object was last modified. etag yes string The object\u2019s ETag (content hash). This reflects changes to the object's content only, not its metadata. user_metadata no Map User-defined metadata (e.g., annotations, tags). If none exists, an empty map is stored. committer yes string The user who committed the object\u2019s latest change. content_type no string The MIME type of the object (e.g., <code>application/json</code>, <code>image/png</code>)."},{"location":"datamanagment/metadata-search/#consistency","title":"Consistency","text":"<p>lakeFS object metadata tables are eventually consistent, which means that it may take up to a few minutes for newly committed  objects to become searchable. Metadata becomes searchable atomically \u2014 either all object metadata from the commit  is available, or none of it is. Commits are processed sequentially: a child commit will only be processed after its parent has been fully ingested.</p> <p>Tip</p> <p>To check whether the most recent state of your branch is available for metadata search queries, check if the following  query returns results: <pre><code>USE \"&lt;repo&gt;.&lt;branch&gt;.system\";\nSELECT * FROM object_metadata\nWHERE commit_id = &lt;head_commit&gt; -- Replace with the head commit ID of the branch you are looking at \nLIMIT 1;\n</code></pre></p>"},{"location":"datamanagment/metadata-search/#getting-started","title":"Getting Started","text":""},{"location":"datamanagment/metadata-search/#configuration","title":"Configuration","text":"<p>lakeFS Metadata Search runs as a separate service that integrates with your lakeFS server.  </p> <p>If you are self-hosting lakeFS Enterprise:</p> <ol> <li>Contact us to enable the feature.  </li> <li>Add the configuration below to your Helm values file.</li> <li>Install or upgrade the Helm chart with the updated configuration.</li> </ol> <p>If you are using lakeFS cloud:</p> <p>Contact us to enable the feature. We\u2019ll request the information included in the  sample configuration below.</p> <p>The Metadata Search service requires:</p> <ul> <li>lakeFS server connection settings: so the service can communicate with your lakeFS instance.</li> <li>Metadata-specific settings: to control how metadata is captured and which repositories and branches are searchable.</li> </ul>"},{"location":"datamanagment/metadata-search/#configuration-reference","title":"Configuration Reference","text":"<ul> <li><code>lakefs.endpoint</code> <code>(string : \"\")</code>- the lakeFS server endpoint</li> <li><code>lakefs.access_key_id</code> <code>(string : \"\")</code>- a lakeFS access key </li> <li><code>lakefs.secret_access_key</code> <code>(string : \"\")</code>- a lakeFS secret access key</li> <li><code>metadata_settings.since</code> <code>(string : \"\")</code>- ISO 8601 timestamp (e.g., <code>2025-07-15T00:00:00+00</code>) that sets the earliest point in time from which to process commits for metadata extraction. If omitted, metadata will be captured from the time  the branch was created.</li> <li><code>metadata_settings.max_commits</code> <code>(int : 0)</code> - The maximum number of commits to process per searchable branch.   Uses 0 by default that disables the limit.</li> <li><code>repositories</code> <code>(map[string]branches:string[] : {})</code> - A mapping of repositories and the branches in each where metadata  search should be enabled. You can specify full branch names or use branch prefixes for flexibility. Prefixes should be expressed using a trailing asterisk, e.g., <code>dev-*</code>.  </li> </ul> <p>Note</p> <p>Metadata search is disabled by default. You must explicitly configure which repositories and branches to include.</p> <p>Tip</p> <p>Use branch name prefixes (e.g., <code>feature-*</code>) to reduce the need for manual updates when new branches are added.</p>"},{"location":"datamanagment/metadata-search/#sample-configuration","title":"Sample Configuration","text":"<p>Example</p> <pre><code>lakefs:\n  endpoint: \"https://example.lakefs.io\"\n  access_key_id: \"AKIAIOSFOLEXAMPLE\"\n  secret_access_key: \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\nmetadata_settings:\n  since: \"2025-07-15T00:00:00+00\"\n  max_commits: 100\nrepositories:\n  \"example-repo-1\":\n    branches:\n     - \"main\"\n     - \"dev\"\n  \"example-repo-2\":\n    branches:  \n     - \"main\"\n     - \"feature-*\"\n</code></pre>"},{"location":"datamanagment/metadata-search/#how-to-search-by-metadata","title":"How to Search by Metadata","text":"<p>To search by object metadata in lakeFS, query the Iceberg <code>object_metadata</code> tables that lakeFS automatically creates and manages. These tables are always available under the data repository namespace: </p> <pre><code>&lt;repo&gt;.&lt;ref&gt;.system.object_metadata\n</code></pre> <p>You can use any Iceberg-compatible engine, such as DuckDB, Trino, Spark, PyIceberg, or others.</p> <p>If you're using DuckDB, note the Iceberg REST Catalog guide for  details on how to reference <code>object_metadata</code> tables.</p> <p>Requirements</p> <p>Your Iceberg client must meet the authorization requirements of the lakeFS Iceberg REST catalog.  To perform metadata search queries, ensure users have access to the appropriate metadata repository (see  How it Works). </p>"},{"location":"datamanagment/metadata-search/#search-steps","title":"Search Steps","text":"<ol> <li>Initialize lakeFS Iceberg catalog, and authenticate. The following example uses PyIceberg: <pre><code>from pyiceberg.catalog.rest import RestCatalog\n\ncatalog = RestCatalog(name = \"my_catalog\", **{\n    'prefix': 'lakefs',\n    'uri': f'{lakefs_endpoint}/mds/iceberg/api',\n    'oauth2-server-uri': f'{lakefs_endpoint}/mds/iceberg/api/v1/oauth/tokens',\n    'credential': f'{lakefs_client_key}:{lakefs_client_secret}',\n})\n</code></pre></li> <li>Load the object metadata table that represents the reference you would like to query.</li> <li>Use SQL to search by system or user-defined metadata.</li> </ol> <p>Here\u2019s an example using PyIceberg and DuckDB:</p> <p>Requirements</p> <p>This requires duckdb to be installed. </p> <pre><code>from pyiceberg.catalog import load_catalog\n\n# Initialize the catalog\ncatalog = RestCatalog(name = \"my_catalog\", **{\n    'prefix': 'lakefs',\n    'uri': 'https://lakefs.example.com/mds/iceberg/api',\n    'oauth2-server-uri': 'https://lakefs.example.com/mds/iceberg/api/iceberg/api/v1/oauth/tokens',\n    'credential': f'AKIAlakefs12345EXAMPLE:abc/lakefs/1234567bPxRfiCYEXAMPLEKEY',\n})\n\n# `repo` is the repository name we would like to search \ncon = catalog.load_table('repo.main.system.object_metadata').scan().to_duckdb('object_metadata')\n\nquery = f\"\"\"\nSELECT path   \nFROM object_metadata\nWHERE user_metadata['animal'] = 'cat' \n  AND last_modified &gt; (now() - INTERVAL '20 days') \n\"\"\"\n\ndf = con.execute(query).df()\n</code></pre> <p>This query finds all newly added cat images (added in the past week), demonstrating how you can combine user-defined and system metadata fields in powerful, version-aware searches.</p> <p>Note</p> <p>This catalog initialization differs from the classic lakeFS Iceberg catalog. Notice the additional <code>mds</code> path segment in the endpoints, which routes requests through the Metadata Search catalog.</p>"},{"location":"datamanagment/metadata-search/#writing-reproducible-queries","title":"Writing Reproducible Queries","text":"<p>In collaborative environments or during iterative development, it's important to ensure that metadata queries return consistent, reproducible results. To achieve this, you should query object metadata tables using commit IDs or tag names, which are  immutable references, instead of branch names.</p> <p>Why not use Branch names?</p> <p>Querying metadata tables using a branch name, e.g., <code>repo.main.system.object_metadata</code> return results based on the state of the branch\u2019s HEAD commit at the time of the query, assuming the metadata has already been ingested (within  eventual consistency constraints). However, because branch heads are mutable and advance with each new commit, the results of such queries can change over time.</p> <p>Use commit ID or tag name for stability</p> <p>To ensure stability and reproducibility, use lakeFS commit IDs or tag names in your queries. Each commit or tag created  on the data repository reference a specific, fixed snapshot of the repository state, including its metadata.  This guarantees that the same query always returns the same results, regardless of subsequent changes to the branch.</p>"},{"location":"datamanagment/metadata-search/#using-commit-id","title":"Using Commit ID","text":"<ol> <li>Identify the relevant commit ID from the data repository (e.g., <code>dc3117ec3a727104226c896bf7ab9350ee5da06ae052406262840e9a4a8c9ffb</code> on branch <code>dev</code> in repo <code>my-repo</code>). </li> <li>Query the object metadata table using the following pattern:  <pre><code>&lt;repo&gt;.&lt;commit_id&gt;.system.object_metadata\n</code></pre> Examples: <pre><code>my-repo.dc3117e.system.object_metadata\nmy-repo.dc3117ec3a727104226c896bf7ab9350ee5da06ae052406262840e9a4a8c9ffb.system.object_metadata\n</code></pre> Both table paths return the metadata for commit <code>dc3117ec3a727104226c896bf7ab9350ee5da06ae052406262840e9a4a8c9ffb</code>.</li> </ol> <p>Tip</p> <p>For readability, prefer using the short commit SHA (e.g., <code>dc3117e</code>).  </p>"},{"location":"datamanagment/metadata-search/#using-tag-name","title":"Using Tag Name","text":"<ol> <li>Identify the relevant tag from the data repository (e.g., <code>v0.2.1</code> on branch <code>main</code> in repo <code>my-repo</code>).</li> <li>Query the object metadata table using the following pattern: <pre><code>&lt;repo&gt;.&lt;tag_name&gt;.system.object_metadata\n</code></pre> Examples: <pre><code>my-repo.v0.2.1.system.object_metadata\n</code></pre></li> </ol> <p>Tip</p> <p>For end-to-end reproducibility, include the commit ID or tag name directly in your queries, and version those queries with Git.</p>"},{"location":"datamanagment/metadata-search/#example-queries","title":"Example Queries","text":"<p>This section showcases how to use lakeFS Metadata Search to answer different types of questions using standard SQL.</p> <p>For simplicity and readability, the examples are written in Trino SQL. If you're using another engine (e.g., DuckDB,  Spark, or PyIceberg), you may need to adjust the syntax accordingly.</p> <p>The examples below use branch names as references for simplicity. However, for reproducible results, it is recommended to use commit IDs or tag names instead. You can convert any example into a reproducible query by replacing the branch reference accordingly.</p> Object Annotation and Labeling Filter by Object Labels <p>The following example returns images labeled as dogs that are not in a sitting position: <pre><code>USE \"repo.main.system\";\n\nSELECT * FROM object_metadata\nWHERE path LIKE `%.jpg`\n  AND user_metadata['animal'] = 'dog'\n  AND user_metadata['position'] != 'sitting';\n</code></pre></p> View Metadata from AI-Powered Annotators <p>Assume that any object annotated by an AI-powered tool includes the object metadata key-value pair <code>source: autolabel</code> to  indicate its origin. The following example returns all such AI-annotated objects: </p> <pre><code>USE \"repo.main.system\";\n\nSELECT *\nFROM object_metadata\nWHERE user_metadata['source'] = 'autolabel';\n</code></pre> File Properties and Storage Filter by File Extension &amp; Size <p>Find all <code>.png</code> files larger than 2MB.</p> <pre><code>USE \"repo.main.system\";\n\nSELECT *\nFROM object_metadata\nWHERE path LIKE '%.png'\n  AND size::INT &gt; 2000000;\n</code></pre> Filter objects by addition Time <p>This example finds all objects added in the last 7 days.</p> <pre><code>USE \"repo.main.system\";\n\nSELECT *\nFROM object_metadata\nWHERE last_modified &gt;= current_timestamp - interval '7' day;\n</code></pre> Audit and Governance Detect Errors in Sensitive Data Tagging <p>Assume all objects under <code>customers/</code> must have user metadata <code>PII=true</code>.  This example returns objects where <code>PII=false</code>, or PII key is missing. </p> <pre><code>USE \"repo.main.system\";\n\nSELECT *\nFROM object_metadata\nWHERE path LIKE 'customers/%'\n  AND (\n    user_metadata['PII'] = 'false'\n        OR user_metadata['PII'] IS NULL\n    );\n</code></pre>"},{"location":"enterprise/","title":"lakeFS Enterprise Features","text":"<p>Changelog</p> <p>For the latest updates and changes to lakeFS Enterprise features, see the changelog.</p>"},{"location":"enterprise/#what-is-lakefs-enterprise","title":"What is lakeFS Enterprise?","text":"<p>lakeFS Enterprise is a commercially-supported version of lakeFS, offering additional features and functionalities that meet the needs of organizations from a production-grade system.</p>"},{"location":"enterprise/#why-did-we-build-lakefs-enterprise","title":"Why did we build lakeFS Enterprise?","text":"<p>lakeFS Enterprise was built for organizations that require the support, security standards and features required of a production-grade system.</p>"},{"location":"enterprise/#what-is-the-value-of-using-lakefs-enterprise","title":"What is the value of using lakeFS Enterprise?","text":"<ol> <li>Support: the lakeFS team is committed to supporting you under an SLA for both issues and product enhancements.</li> <li>Security: Full support for a suite of security features and additional lakeFS functionality.</li> <li>Advanced Functionality for AI, ML and Data engineering use cases. See Features below.</li> </ol>"},{"location":"enterprise/#what-security-features-does-lakefs-enterprise-provide","title":"What security features does lakeFS Enterprise provide?","text":"<p>With lakeFS Enterprise you\u2019ll receive access to the security package containing the following features:</p> <ol> <li>A rich Role-Based Access Control permission system that allows for fine-grained control by associating permissions with users and groups, granting them specific actions on specific resources. This ensures data security and compliance within an organization.</li> <li>To easily manage users and groups, lakeFS Enterprise provides SSO integration (including support for SAML, OIDC, ADFS, Okta, and Azure AD), supporting existing credentials from a trusted provider, eliminating separate logins.</li> <li>lakeFS Enterprise supports SCIM for automatically provisioning and de-provisioning users and group memberships, allowing organizations to maintain a single source of truth for their user database.</li> <li>STS Auth offers temporary, secure logins using an Identity Provider, simplifying user access and enhancing security.</li> <li>Authentication with AWS IAM Roles allows authentication using AWS IAM roles instead of lakeFS credentials, removing the need to maintain static credentials for lakeFS Enterprise users running on AWS.</li> <li>Auditing provides a detailed action log of events happening within lakeFS, including who performed which action, on which resource - and when.</li> </ol>"},{"location":"enterprise/#what-additional-functionality-does-lakefs-enterprise-provide","title":"What additional functionality does lakeFS Enterprise provide?","text":"<ol> <li>lakeFS Mount - allows users to virtually mount a remote lakeFS repository onto a local directory. Once mounted, users can access the data as if it resides on their local filesystem, using any tool, library, or framework that reads from a local filesystem.</li> <li>Iceberg REST Catalog - Provides full support for managing Iceberg tables alongside other data formats in the same lakeFS repository. Built using open standards and works with any Iceberg client.</li> <li>lakeFS for Snowflake - Provides full integration into the Snowflake ecosystem, including full support for Iceberg managed tables.</li> <li>Transactional Mirroring - Allows replicating lakeFS repositories into consistent read-only copies in remote locations.</li> <li>Multiple Storage Backends - Allows managing data stored across multiple storage locations: on-prem, hybrid, or multi-cloud.</li> <li>Metadata Search - Enables powerful object search using object metadata, from system metadata (like path and size) to user-defined metadata.  </li> </ol> Feature OSS Enterprise Format-agnostic data version control \u2705 \u2705 Cloud-agnostic \u2705 \u2705 Zero Clone copy for isolated environment \u2705 \u2705 Atomic Data Promotion (via merges) \u2705 \u2705 Data stays in one place \u2705 \u2705 Configurable Garbage Collection \u2705 \u2705 Data CI/CD using lakeFS hooks \u2705 \u2705 Integrates with your data stack \u2705 \u2705 Role Based Access Control (RBAC) \u2705 Single Sign On (SSO) \u2705 SCIM Support \u2705 IAM Roles \u2705 (AWS) Mount Capability \u2705 Iceberg REST Catalog \u2705 Metadata Search \u2705 Audit Logs \u2705 Transactional Mirroring (cross-region) \u2705 Support SLA \u2705 <p>Tip</p> <p>You can learn more about the lakeFS Enterprise architecture, or follow the examples in the Quickstart guide.</p>"},{"location":"enterprise/architecture/","title":"Architecture","text":"<p>lakeFS Enterprise extends the open-source lakeFS foundation, delivering a complete data versioning and governance solution with seamlessly integrated enterprise features like SSO, RBAC, mounting capabilities, and more.</p> <p></p> <p>[1] Any user request to lakeFS via Browser or Programmatic access (SDK, HTTP API, lakectl).</p> <p>[2] A reverse proxy (e.g., NGINX, Traefik, Kubernetes Ingress, Load Balanacer) will distribute requests between lakeFS server instances, SSL termination etc. Required when using more than 1 lakeFS instance.</p> <p>[3] lakeFS Enterprise - lakeFS with additional enterprise functionality, including advanced security, SSO authentication, RBAC authorization, compliance, audit logging, and enterprise support.</p> <p>[4] The KV Store - Where metadata is stored, used by both core lakeFS and enterprise features.</p> <p>[5] SSO IdP - External identity provider (e.g. Azure AD, Okta, JumpCloud).  lakeFS Enterprise implements SAML, OAuth2, and OIDC protocols.</p> <p>For more details and pricing, please contact sales.</p> <p>Info</p> <p>Setting up lakeFS enterprise with an SSO IdP (OIDC, SAML or LDAP) requires configuring access from the IdP too.</p>"},{"location":"enterprise/configuration/","title":"lakeFS Enterprise Configuration Reference","text":"<p>lakeFS Enterprise configuration extends lakeFS's configuration and uses the same config file.</p>"},{"location":"enterprise/configuration/#lakefs-configuration","title":"lakeFS Configuration","text":"<p>For a complete list of configuration options, see the lakeFS Server Configuration. The sections below provide additional configuration references that complement the main configuration guide.</p>"},{"location":"enterprise/configuration/#reference","title":"Reference","text":"<p>This reference uses <code>.</code> to denote the nesting of values.</p>"},{"location":"enterprise/configuration/#database","title":"database","text":"<p>Configuration section for lakeFS Enterprise database options.</p>"},{"location":"enterprise/configuration/#databaseredis","title":"database.redis","text":"<ul> <li><code>database.redis.endpoint</code> <code>(string : \"localhost:6379\")</code> - Redis server endpoint in host:port format</li> <li><code>database.redis.enable_tls</code> <code>(bool : false)</code> - Enable TLS for Redis connections (required for AWS MemoryDB)</li> <li><code>database.redis.cluster_mode</code> <code>(bool : false)</code> - Enable Redis Cluster mode support</li> <li><code>database.redis.username</code> <code>(string : \"\")</code> - Username for Redis authentication</li> <li><code>database.redis.password</code> <code>(string : \"\")</code> - Password for Redis authentication</li> <li><code>database.redis.pool_size</code> <code>(int : 10)</code> - Connection pool size for Redis connections</li> <li><code>database.redis.dial_timeout</code> <code>(duration : 5s)</code> - Timeout for establishing new connections</li> <li><code>database.redis.read_timeout</code> <code>(duration : 3s)</code> - Timeout for socket reads</li> <li><code>database.redis.write_timeout</code> <code>(duration : 3s)</code> - Timeout for socket writes</li> <li><code>database.redis.database</code> <code>(int : 0)</code> - Database number to select (0-15 for most Redis configurations)</li> <li><code>database.redis.namespace</code> <code>(string : \"\")</code> - Prefix for all keys used by the application</li> <li><code>database.redis.tls_skip_verify</code> <code>(bool : false)</code> - Skip certificate verification (for development only)</li> <li><code>database.redis.batch_size</code> <code>(int : 1000)</code> - Default batch size for Redis KV operations</li> </ul> <p>Note</p> <p>If you are using Redis or a compatible service such as Amazon MemoryDB, enable durable writes. This ensures data is persisted and prevents data loss in case of node restarts or failures.</p>"},{"location":"enterprise/configuration/#auth","title":"auth","text":"<p>Configuration section for SSO authentication services, like SAML or OIDC.</p> <ul> <li><code>auth.logout_redirect_url</code> <code>(string : \"/auth/login\")</code> - The URL to redirect to after logout when using SSO authentication services, like SAML or OIDC.  The configuration depends on the authentication provider:  <ul> <li>For OIDC: The logout URL of the OIDC provider (e.g., Auth0 logout endpoint).</li> <li>For SAML: The URL within lakeFS where the IdP should redirect after logout (e.g., <code>/auth/login</code>).</li> </ul> </li> </ul>"},{"location":"enterprise/configuration/#authui_config","title":"auth.ui_config","text":"<ul> <li><code>auth.ui_config.login_url_method</code> <code>(string : \"redirect\")</code> - Controls how lakeFS handles login when an <code>auth.ui_config.login_url</code> (SSO via OIDC or SAML) is configured. This parameter is only relevant when <code>auth.ui_config.login_url</code> is set. Supported values:<ul> <li><code>\"redirect\"</code> - On login, users are redirected to the configured <code>auth.ui_config.login_url</code> (default).</li> <li><code>\"select\"</code> - On login, login selection page is used to select between build-in lakeFS and SSO login.</li> </ul> </li> </ul>"},{"location":"enterprise/configuration/#authproviders","title":"auth.providers","text":"<p>Configuration section for external identity providers used for authentication services, such as LDAP, SAML or OIDC.</p>"},{"location":"enterprise/configuration/#authprovidersldap","title":"auth.providers.ldap","text":"<p>Configuration section for LDAP</p> <ul> <li><code>auth.providers.ldap.server_endpoint</code> <code>(string : \"\" - required)</code> - The LDAP server address, e.g. <code>'ldaps://ldap.company.com:636'</code>.</li> <li><code>auth.providers.ldap.bind_dn</code> <code>(string : \"\" - required)</code> - The bind string, e.g. <code>'uid=&lt;bind-user-name&gt;,ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com'</code>.</li> <li><code>auth.providers.ldap.bind_password</code> <code>(string : \"\" - required)</code> - The password for the user to bind.</li> <li><code>auth.providers.ldap.username_attribute</code> <code>(string : \"\" - required)</code> - The user name attribute, e.g. 'uid'.</li> <li><code>auth.providers.ldap.user_base_dn</code> <code>(string : \"\" - required)</code> - The search request base dn, e.g. <code>'ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com'</code>.</li> <li><code>auth.providers.ldap.user_filter</code> <code>(string : \"\" - required)</code> - The search request user filter, e.g. <code>'(objectClass=inetOrgPerson)'</code>.</li> <li><code>auth.providers.ldap.connection_timeout_seconds</code> <code>(int : 0 - required)</code> - The timeout for a single connection.</li> <li><code>auth.providers.ldap.request_timeout_seconds</code> <code>(int : 0 - required)</code> - The timeout for a single request.</li> <li><code>auth.providers.ldap.default_user_group</code> <code>(string : \"\" - required)</code> - The default group for the users initially authenticated by the remote service.</li> </ul>"},{"location":"enterprise/configuration/#authproviderssaml","title":"auth.providers.saml","text":"<p>Configuration section for SAML</p> <ul> <li><code>auth.providers.saml.sp_root_url</code> <code>(string : \"\")</code> - The base lakeFS-URL, e.g. <code>'https://&lt;lakefs-url&gt;'</code>.</li> <li><code>auth.providers.saml.sp_x509_key_path</code> <code>(string : \"\")</code> - The path to the private key, e.g <code>'/etc/saml_certs/rsa_saml_private.cert'</code>.</li> <li><code>auth.providers.saml.sp_x509_cert_path</code> <code>(string : \"\")</code> - The path to the public key, '/etc/saml_certs/rsa_saml_public.pem'.</li> <li><code>auth.providers.saml.sp_sign_request</code> <code>(bool : false)</code> Some IdP require the SLO request to be signed.</li> <li><code>auth.providers.saml.sp_signature_method</code> <code>(string : \"\")</code> Optional valid signature values depending on the IdP configuration, e.g. 'http://www.w3.org/2001/04/xmldsig-more#rsa-sha256'.</li> <li><code>auth.providers.saml.idp_metadata_url</code> <code>(string : \"\")</code> - The URL for the metadata server, e.g. <code>'https://&lt;adfs-auth.company.com&gt;/federationmetadata/2007-06/federationmetadata.xml'</code>.</li> <li><code>auth.providers.saml.idp_metadata_file_path</code> <code>(string : \"\")</code> - The path to the Identity Provider (IdP) metadata XML file, e.g. '/etc/saml/idp-metadata.xml'.</li> <li><code>auth.providers.saml.idp_skip_verify_tls_cert</code> <code>(bool : false)</code> - Insecure skip verification of the IdP TLS certificate, like when signed by a private CA.</li> <li><code>auth.providers.saml.idp_authn_name_id_format</code> <code>(string : \"\")</code> - The format used in the NameIDPolicy for authentication requests. (e.g., \"urn:oasis:namesSAML:1.1:nameid-format:unspecified\")</li> <li><code>auth.providers.saml.idp_request_timeout</code> <code>(time duration : 10s)</code> The timeout for remote authentication requests.</li> <li><code>auth.providers.saml.post_login_redirect_url</code> <code>(string : \"\")</code> - The URL to redirect users to after successful SAML authentication, e.g. <code>'http://localhost:8000/'</code>.</li> </ul>"},{"location":"enterprise/configuration/#authprovidersoidc","title":"auth.providers.oidc","text":"<p>Configuration section for OIDC</p> <ul> <li><code>auth.providers.oidc.url</code> <code>(string : \"\")</code> - The OIDC provider url, e.g. <code>'https://oidc-provider-url.com/'</code></li> <li><code>auth.providers.oidc.client_id</code> <code>(string : \"\")</code> - The application's ID</li> <li><code>auth.providers.oidc.client_secret</code> <code>(string : \"\")</code> - The application's secret</li> <li><code>auth.providers.oidc.callback_base_url</code> <code>(string : \"\")</code> - A default callback address of the lakeFS server</li> <li><code>auth.providers.oidc.callback_base_urls</code> <code>(string[] : [])</code> - If callback_base_urls is configured, check current host is whitelisted otherwise use callback_base_url (without 's'). These config keys are mutually exclusive</li> </ul> <p>Note</p> <p>You may configure a list of URLs that the OIDC provider may redirect to. This allows lakeFS to be accessed from multiple hostnames while retaining federated auth capabilities. If the provider redirects to a URL not in this list, the login will fail. This property and callback_base_url are mutually exclusive.</p> <ul> <li><code>auth.providers.oidc.authorize_endpoint_query_parameters</code> <code>(map[string]string : {})</code> - key/value parameters that are passed to a provider's authorization endpoint</li> <li><code>auth.providers.oidc.logout_endpoint_query_parameters</code> <code>(string[] : [])</code> - The query parameters that will be used to redirect the user to the OIDC provider after logout, e.g. <code>[\"returnTo\", \"https://&lt;lakefs.ingress.domain&gt;/oidc/login\"]</code></li> <li><code>auth.providers.oidc.logout_client_id_query_parameter</code> <code>(string : \"\")</code> - The claim name that represents the client identifier in the OIDC provider</li> <li><code>auth.providers.oidc.additional_scope_claims</code> <code>(string[] : [])</code> - Specifies optional requested permissions, other than <code>openid</code> and <code>profile</code> that are being used</li> <li><code>auth.providers.oidc.post_login_redirect_url</code> <code>(string : \"\")</code> - The URL to redirect users to after successful OIDC authentication, e.g. <code>'http://localhost:8000/'</code></li> </ul>"},{"location":"enterprise/configuration/#authexternal_aws_auth","title":"auth.external_aws_auth","text":"<p>Configuration section for authentication to lakeFS using the AWS presigned get-caller-identity request:  External Principals AWS Auth</p> <ul> <li><code>auth.external_aws_auth.enabled</code> <code>(bool : false)</code> - If true, external principals API will be enabled, e.g auth service and login api's</li> <li><code>auth.external_aws_auth.get_caller_identity_max_age</code> <code>(time duration : 15m)</code> - The maximum age in seconds for the GetCallerIdentity request to be valid, the max is 15 minutes enforced by AWS, smaller TTL can be set</li> <li><code>auth.external_aws_auth.valid_sts_hosts</code> <code>(string[] : [])</code> - The default are all the valid AWS STS hosts (<code>sts.amazonaws.com</code>, <code>sts.us-east-2.amazonaws.com</code> etc.)</li> <li><code>auth.external_aws_auth.required_headers</code> <code>(map[string]string : {})</code> - Headers that must be present by the client when doing login request. For security reasons it is recommended to set <code>X-LakeFS-Server-ID: &lt;lakefs.ingress.domain&gt;</code>, lakeFS clients assume that's the default</li> <li><code>auth.external_aws_auth.optional_headers</code> <code>(map[string]string : {})</code> - Optional headers that can be present by the client when doing login request</li> <li><code>auth.external_aws_auth.http_client.timeout</code> <code>(time duration : 10s)</code> - The timeout for the HTTP client used to communicate with AWS STS</li> <li><code>auth.external_aws_auth.http_client.skip_verify</code> <code>(bool : false)</code> - Skip SSL verification with AWS STS</li> </ul>"},{"location":"enterprise/configuration/#authprefetched_cache","title":"auth.prefetched_cache","text":"<p>Configuration section for the prefetched cache. The prefetched cache is used to cache entries from the auth KV store in memory to reduce the number of calls to the KV store.</p> <p>Note</p> <p>When prefetched cache is enabled, the auth catch used to cache responses from the auth service is disabled.</p> <ul> <li><code>auth.prefetched_cache.enabled</code> <code>(bool : false)</code> - Enable the prefetched cache</li> <li><code>auth.prefetched_cache.interval</code> <code>(duration : 30s)</code> - The interval at which to refresh the cache</li> <li><code>auth.prefetched_cache.ttl</code> <code>(duration : 1m)</code> - The time to live for each cache entry</li> <li><code>auth.prefetched_cache.size</code> <code>(int : 10000)</code> - The maximum number of entries in the cache</li> <li><code>auth.prefetched_cache.eviction_jitter</code> <code>(duration : 3s)</code> - The maximum random jitter to add to the eviction interval to prevent thundering herds</li> <li><code>auth.prefetched_cache.negative_cache_size</code> <code>(int : 40000)</code> - The maximum number of missing entries the cache will remember</li> </ul>"},{"location":"enterprise/configuration/#backpressure","title":"backpressure","text":"<p>Configuration section for backpressure settings. When enabled, backpressure will limit the number of requests each server will handle concurrently through s3 gateway</p> <ul> <li><code>backpressure.enabled</code> <code>(bool : false)</code> - Enable backpressure</li> <li><code>backpressure.upload_part_max_concurrent</code> <code>(int : 0)</code> - The maximum number of concurrent upload part requests. 0 means no limit</li> </ul>"},{"location":"enterprise/configuration/#blockstores","title":"blockstores","text":"<p>Info</p> <p>The <code>blockstores</code> configuration is required for multi-storage backend setups and replaces the previous <code>blockstore</code> configuration.</p> <ul> <li><code>blockstores.signing.secret_key</code> <code>(string : required)</code> - A random generated string that is used for HMAC signing when using get/link physical address</li> <li><code>blockstores.stores</code> <code>([{id: string, type: string, ...}] : required)</code> - Defines multiple storage backends used in a multi-storage backend setup. Each storage backend must have a unique id and a valid configuration.</li> </ul>"},{"location":"enterprise/configuration/#common-fields-for-all-stores","title":"Common Fields for All Stores","text":"<ul> <li><code>blockstores.stores[].id</code> <code>(string : required)</code> - Unique identifier for the storage backend.</li> <li><code>blockstores.stores[].backward_compatible</code> <code>(bool : false)</code> - Optional. Defaults to false. Used to migrate from a single to a multi-storage backend setup.</li> <li><code>blockstores.stores[].description</code> <code>(string : )</code> - A human-readable description of the storage backend.</li> <li><code>blockstores.stores[].type</code> <code>(string : required)</code> - <code>(one of [\"local\", \"s3\", \"gs\", \"azure\", \"mem\"] : required)</code>. Block adapter to use. This controls where the underlying data will be stored.</li> </ul> <code>blockstores.stores.local</code><code>blockstores.stores.s3</code><code>blockstores.azure</code><code>blockstores.gs</code> <ul> <li><code>blockstores.stores[].local.path</code> <code>(string: \"~/lakefs/data\")</code> - When using the local Block Adapter, which directory to store files in</li> <li><code>blockstores.stores[].local.import_enabled</code> <code>(bool: false)</code> - Enable import for local Block Adapter, relevant only if you are using shared location</li> <li><code>blockstores.stores[].local.import_hidden</code> <code>(bool: false)</code> - When enabled import will scan and import any file or folder that starts with a dot character.</li> <li><code>blockstores.stores[].local.allowed_external_prefixes</code> <code>([]string: [])</code> - List of absolute path prefixes used to match any access for external location (ex: /var/data/). Empty list mean no access to external location.</li> </ul> <ul> <li><code>blockstores.stores[].s3.region</code> <code>(string : \"us-east-1\")</code> - Default region for lakeFS to use when interacting with S3.</li> <li><code>blockstores.stores[].s3.profile</code> <code>(string : )</code> - If specified, will be used as a named credentials profile</li> <li><code>blockstores.stores[].credentials_file</code> <code>(string : )</code> - If specified, will be used as a credentials file</li> <li><code>blockstores.stores[].credentials.access_key_id</code> <code>(string : )</code> - If specified, will be used as a static set of credential</li> <li><code>blockstores.stores[].credentials.secret_access_key</code> <code>(string : )</code> - If specified, will be used as a static set of credential</li> <li><code>blockstores.stores[].s3.credentials.session_token</code> <code>(string : )</code> - If specified, will be used as a static session token</li> <li><code>blockstores.stores[].s3.endpoint</code> <code>(string : )</code> - If specified, custom endpoint for the AWS S3 API (https://s3_compatible_service_endpoint:port)</li> <li><code>blockstores.stores[].s3.force_path_style</code> <code>(bool : false)</code> - When true, use path-style S3 URLs (https:/// instead of https://.) <li><code>blockstores.stores[].s3.discover_bucket_region</code> <code>(bool : true)</code> - (Can be turned off if the underlying S3 bucket doesn't support the GetBucketRegion API).</li> <li><code>blockstores.stores[].s3.skip_verify_certificate_test_only</code> <code>(bool : false)</code> - Skip certificate verification while connecting to the storage endpoint. Should be used only for testing.</li> <li><code>blockstores.stores[].s3.server_side_encryption</code> <code>(string : )</code> - Server side encryption format used (Example on AWS using SSE-KMS while passing \"aws:kms\")</li> <li><code>blockstores.stores[].s3.server_side_encryption_kms_key_id</code> <code>(string : )</code> - Server side encryption KMS key ID</li> <li><code>blockstores.stores[].s3.pre_signed_expiry</code> <code>(time duration : \"15m\")</code> - Expiry of pre-signed URL.</li> <li><code>blockstores.stores[].s3.pre_signed_endpoint</code> <code>(string : )</code> - Custom endpoint for pre-signed URLs.</li> <li><code>blockstores.stores[].s3.disable_pre_signed</code> <code>(bool : false)</code> - Disable use of pre-signed URL.</li> <li><code>blockstores.stores[].s3.disable_pre_signed_ui</code> <code>(bool : true)</code> - Disable use of pre-signed URL in the UI.</li> <li><code>blockstores.stores[].s3.disable_pre_signed_multipart</code> <code>(bool : )</code> - Disable use of pre-signed multipart upload experimental, enabled on S3 block adapter with presign support.</li> <li><code>blockstores.stores[].s3.client_log_request</code> <code>(bool : false)</code> - Set SDK logging bit to log requests</li> <li><code>blockstores.stores[].s3.client_log_retries</code> <code>(bool : false)</code> - Set SDK logging bit to log retries</li> <ul> <li><code>blockstores.stores[].azure.storage_account</code> <code>(string : )</code> - If specified, will be used as the Azure storage account</li> <li><code>blockstores.stores[].azure.storage_access_key</code> <code>(string : )</code> - If specified, will be used as the Azure storage access key</li> <li><code>blockstores.stores[].azure.pre_signed_expiry</code> <code>(time duration : \"15m\")</code> - Expiry of pre-signed URL.</li> <li><code>blockstores.stores[].azure.disable_pre_signed</code> <code>(bool : false)</code> - Disable use of pre-signed URL.</li> <li><code>blockstores.stores[].azure.disable_pre_signed_ui</code> <code>(bool : true)</code> - Disable use of pre-signed URL in the UI.</li> <li><code>blockstores.stores[].azure.domain</code> <code>(string : blob.core.windows.net)</code> - Enables support of different Azure cloud domains. Current supported domains (in Beta stage): [<code>blob.core.chinacloudapi.cn</code>, <code>blob.core.usgovcloudapi.net</code>]</li> </ul> <ul> <li><code>blockstores.stores[].gs.credentials_file</code> <code>(string : )</code> - If specified will be used as a file path of the JSON file that contains your Google service account key</li> <li><code>blockstores.stores[].gs.credentials_json</code> <code>(string : )</code> - If specified will be used as JSON string that contains your Google service account key (when credentials_file is not set)</li> <li><code>blockstores.stores[].gs.pre_signed_expiry</code> <code>(time duration : \"15m\")</code> - Expiry of pre-signed URL.</li> <li><code>blockstores.stores[].gs.disable_pre_signed</code> <code>(bool : false)</code> - Disable use of pre-signed URL.</li> <li><code>blockstores.stores[].gs.disable_pre_signed_ui</code> <code>(bool : true)</code> - Disable use of pre-signed URL in the UI.</li> <li><code>blockstores.stores[].gs.server_side_encryption_customer_supplied</code> <code>(string : )</code> - Server side encryption with AES key in hex format, exclusive with key ID below</li> <li><code>blockstores.stores[].gs.server_side_encryption_kms_key_id</code> <code>(string : )</code> - Server side encryption KMS key ID, exclusive with above</li> </ul>"},{"location":"enterprise/configuration/#features","title":"features","text":"<ul> <li><code>features.local_rbac</code> <code>(bool: true)</code> - Backward compatibility if you use an external RBAC service (such as legacy fluffy). If <code>false</code> lakeFS will expect to use <code>auth.api</code> and all fluffy related configuration for RBAC.</li> </ul>"},{"location":"enterprise/configuration/#iceberg_catalog","title":"iceberg_catalog","text":"<p>Configuration section for the Iceberg REST Catalog</p> <ul> <li><code>iceberg_catalog.token_duration</code> <code>(duration : 1h)</code> - Authenticated token duration</li> </ul>"},{"location":"enterprise/configuration/#using-environment-variables","title":"Using Environment Variables","text":"<p>All the configuration variables can be set or overridden using environment variables. To set an environment variable, prepend <code>LAKEFS_</code> to its name, convert it to upper case, and replace <code>.</code> with <code>_</code>:</p> <p>For example, <code>auth.logout_redirect_url</code> becomes <code>LAKEFS_AUTH_LOGOUT_REDIRECT_URL</code>, <code>auth.external.aws_auth.enabled</code> becomes <code>LAKEFS_AUTH_EXTERNAL_AWS_AUTH_ENABLED</code>, etc.</p> <p>To set a value for a <code>map[string]string</code> type field, use the syntax <code>key1=value1,key2=value2,...</code>.</p>"},{"location":"enterprise/troubleshooting/","title":"Troubleshooting lakeFS Enterprise","text":"<p>A lakeFS Enterprise deployment includes various configuration components that must be set up correctly, especially during initial setup. To help troubleshoot configuration and deployment issues, lakeFS includes the <code>flare</code> command.</p>"},{"location":"enterprise/troubleshooting/#the-flare-command","title":"The <code>flare</code> command","text":""},{"location":"enterprise/troubleshooting/#synopsis","title":"Synopsis","text":"<p>The <code>lakeFS</code> binary include the flare command</p> <pre><code>lakefs flare [flags]\n</code></pre>"},{"location":"enterprise/troubleshooting/#flags","title":"Flags","text":"<pre><code>--env-var-filename      the name of the file environment variables will be written to (default: lakefs-env.txt)\n--include-env-vars      should environment variables be collected by flare (default: true)\n-o, --output                Output path relative to the current path\n-p, --package               Package generated artifacts into a .zip file (default: false)\n--stdout                Output to stdout instead of files (default: false)\n--zip-filename          The name of the zip file created when the -p option is used (default: lakefs-flare.zip)\n</code></pre>"},{"location":"enterprise/troubleshooting/#example-usage","title":"Example Usage","text":"<pre><code># This will run flare with output set to stdout, which is redirected into a file\n$ ./lakefs flare --stdout &gt; lakefs.flare\n</code></pre>"},{"location":"enterprise/troubleshooting/#what-information-does-the-flare-command-collect","title":"What Information Does the <code>flare</code> Command Collect?","text":""},{"location":"enterprise/troubleshooting/#configuration","title":"Configuration","text":"<p>lakeFS Enterprise allow configuration to be supplied in multiple ways: configuration file, environment variables, and command flags. The <code>flare</code> command collects the fully resolved final configuration used by the lakeFS process.</p>"},{"location":"enterprise/troubleshooting/#environment-variables","title":"Environment Variables","text":"<p>When troubleshooting, it's important to get a view of the environment in which lakeFS are running. This is especially true for container-based deployment environments, like Kubernetes, where env vars are used extensively. The <code>flare</code> command collects environment variables with the following prefixes:</p> <ul> <li><code>LAKEFS_</code></li> <li><code>HTTP_</code></li> <li><code>HOSTNAME</code></li> </ul>"},{"location":"enterprise/troubleshooting/#sanitization-and-secret-redaction","title":"Sanitization and Secret Redaction","text":"<p>Both configuration and env var include sensitive secrets. The <code>flare</code> command has a multi-step process for redacting secrets from what it collects. The <code>flare</code> command is able to detect the following types of secrets:</p> <ul> <li>AWS static credentials</li> <li>Azure storage keys</li> <li>Basic HTTP auth username/password</li> <li>JWTs</li> <li>Bearer auth headers</li> <li>GitHub tokens</li> <li>Certificate private keys</li> </ul> <p>Aside from the specific secret type listed above, <code>flare</code> also has the ability to detect and redact generic high-entropy strings, which are likely to be secrets.</p> <p>Redacted secrets are replaced by a <code>SHA512</code> hash of the value without exposing the actual values.</p>"},{"location":"enterprise/troubleshooting/#usage-collect-and-send-flare","title":"Usage - Collect and Send Flare","text":"<p>The following script is intended to be run locally and assumes that lakeFS Enterprise is deployed to a Kubernetes cluster, since this is the recommended setup. Running this script requires that <code>kubectl</code> be installed on the machine it is being run from and that <code>kubectl</code> is configured with the correct context and credentials to access the cluster. Aside from running the <code>flare</code> command on lakeFS Enterprise, this script also fetches the logs from all running pods of lakeFS.</p>"},{"location":"enterprise/troubleshooting/#step-1-set-script-variables","title":"Step 1 - Set Script Variables","text":"<p>At the top of the script you'll find the <code>Variables</code> block. It is important to change these values according to how lakeFS is deployed in your cluster.  </p> <p><code>NAMESPACE</code> - The K8s namespace where lakeFS is deployed <code>LAKEFS_DEPLOYMENT</code> - The name of the lakeFS K8s deployment <code>LAKEFS_LOGS_OUTPUT_FILE</code> - The name of the local file where lakeFS logs will be saved <code>LAKEFS_FLARE_FILE</code> - The name of the local file where the lakeFS <code>flare</code> result will be saved  </p>"},{"location":"enterprise/troubleshooting/#step-2-execute-the-script","title":"Step 2 - Execute the Script","text":"<pre><code>#!/bin/bash\n\nRED='\\033[0;31m'\nNC='\\033[0m'\n\n# Variables\nNAMESPACE=lakefs-prod\nLAKEFS_DEPLOYMENT=lakefs-server\nLAKEFS_LOGS_OUTPUT_FILE=lakefs.log\nLAKEFS_FLARE_FILE=lakefs.flare\n\n# Find kubectl\nKUBECTLCMD=$(which kubectl)\nif [ -z \"$KUBECTLCMD\" ]\nthen\n    echo -e \"${RED}Couldn't find kubectl in path${NC}\"\n    exit 1\nfi\n\n$KUBECTLCMD get pods -o name -n $NAMESPACE | grep pod/$LAKEFS_DEPLOYMENT | xargs -I {} $KUBECTLCMD logs -n $NAMESPACE --all-containers=true --prefix --ignore-errors --timestamps {} &gt; $LAKEFS_LOGS_OUTPUT_FILE\n\n$KUBECTLCMD exec deployment/$LAKEFS_DEPLOYMENT -- ./lakefs flare --stdout &gt; $LAKEFS_FLARE_FILE\n</code></pre>"},{"location":"enterprise/troubleshooting/#step-3-inspect-the-output-files","title":"Step 3 - Inspect the Output Files","text":"<p>After executing the script you should have two files: lakeFS logs and lakeFS flare output. Before sharing these files, please review them to make sure all secrets were correctly redacted and that all the collected information is shareable.</p>"},{"location":"enterprise/troubleshooting/#step-4-zip-output-files-and-attach-to-support-ticket","title":"Step 4 - Zip Output Files and Attach to Support Ticket","text":"<p>Once you're done inspecting the files, you can zip them into a single file and attach it to a Support Ticket in the support portal.</p>"},{"location":"enterprise/troubleshooting/#new-ticket","title":"New Ticket","text":"<p>When filing a new ticket, you can attach the zip file using the file upload input at the bottom of the new ticket form.</p> <p></p>"},{"location":"enterprise/troubleshooting/#existing-ticket","title":"Existing Ticket","text":"<p>To add a file to an existing ticket, click the ticket subject in the support portal. On the ticket details attach a file as a new response. You can also add text to the response, if you'd like to add further details.</p> <p></p>"},{"location":"enterprise/upgrade/","title":"Upgrade","text":"<p>For upgrading from lakeFS enterprise to a newer version see lakefs migration.</p>"},{"location":"enterprise/upgrade/#migrate-from-fluffy-to-lakefs-enterprise","title":"Migrate From Fluffy to lakeFS Enterprise","text":"<p>The new lakeFS Enterprise integrates all enterprise features directly into a single binary, eliminating the need for the separate Fluffy service. This simplifies deployment, configuration, and maintenance.</p>"},{"location":"enterprise/upgrade/#prerequisites","title":"Prerequisites","text":"<ol> <li>You're using lakeFS enterprise binary or the image in Dockerhub treeverse/lakefs-enterprise with fluffy.</li> <li>Your lakeFS-Enterprise version is &gt;= 1.63.0</li> <li>You possess a lakeFS Enterprise license.</li> </ol> <p>Note</p> <p>Contact us to gain access to lakeFS Enterprise. You will be granted a token that enables downloading dockerhub/lakeFS-Enterprise  from Docker Hub, and a license to run lakeFS Enterprise.</p> <p>To migrate from fluffy to lakeFS Enterprise, follow the steps below:</p> <ol> <li>Sanity Test (Optional): Install a new test lakeFS Enterprise before moving your current production setup. Make sure to include your lakeFS Enterprise license in the configuration before setup. Test the setup \u2192 login \u2192 create repository, etc. Once everything seems to work, delete and cleanup the test setup and we will move to the migration process.</li> <li>Update configuration: Unlike lakeFS + Fluffy, lakeFS Enterprise uses only one configuration file. See Configuration Changes, make sure to add the license to the configuration.</li> <li>Spin down lakeFS and fluffy, and run lakeFS Enterprise!</li> </ol> <p>Warning</p> <p>Please note that there will be a short downtime while replacing the lakeFS instances.</p>"},{"location":"enterprise/upgrade/#configuration-changes","title":"Configuration Changes","text":""},{"location":"enterprise/upgrade/#authentication-configuration","title":"Authentication configuration","text":"<p>Most Fluffy <code>auth.*</code> settings migrate directly to lakeFS Enterprise with the same structure. Below are the differences between the configurations.</p> <p>SAML</p> lakeFS &amp; Fluffy (old)lakeFS Enterprise (new) <p><pre><code># fluffy.yaml\nauth:  \n  logout_redirect_url: https://lakefs.company.com\n  post_login_redirect_url: https://lakefs.company.com\n  saml:\n    enabled: true \n    sp_root_url: https://lakefs.company.com\n    sp_x509_key_path: dummy_saml_rsa.key\n    sp_x509_cert_path: dummy_saml_rsa.cert\n    sp_sign_request: true\n    sp_signature_method: http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\n    idp_metadata_url: https://my.saml-provider.com/federationmetadata/2007-06/federationmetadata.xml\n    # idp_authn_name_id_format: \"urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified\"\n    external_user_id_claim_name: samName\n    # idp_metadata_file_path: \n    # idp_skip_verify_tls_cert: true\n</code></pre> <pre><code># lakefs.yaml\nauth:\n  logout_redirect_url: https://lakefs.company.com\n  cookie_auth_verification:\n    auth_source: saml\n    friendly_name_claim_name: displayName\n    persist_friendly_name: true\n    external_user_id_claim_name: samName\n    validate_id_token_claims:\n      department: r_n_d\n    default_initial_groups:\n    - \"Developers\"\nui_config:\n  login_url: https://lakefs.company.com/sso/login-saml\n  logout_url: https://lakefs.company.com/sso/logout-saml\n  login_cookie_names:\n  - internal_auth_session\n  - saml_auth_session\n</code></pre></p> <pre><code># lakefs.yaml\nauth:\n  logout_redirect_url: https://lakefs.company.com/ # optional, URL to redirect to after logout\n  cookie_auth_verification:\n    auth_source: saml\n    friendly_name_claim_name: displayName\n    default_initial_groups: [\"Admins\"]\n    external_user_id_claim_name: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name \n    validate_id_token_claims:\n      department: r_n_d\n  providers:\n    saml:\n      # enabled: true  # This field was dropped! \n      sp_root_url: https://lakefs.company.com\n      sp_x509_key_path: dummy_saml_rsa.key\n      sp_x509_cert_path: dummy_saml_rsa.cert\n      sp_sign_request: true\n      sp_signature_method: http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\n      idp_metadata_url: https://my.saml-provider.com/federationmetadata/2007-06/federationmetadata.xml\n      post_login_redirect_url: / # Where to redirect after successful SAML login\n      # external_user_id_claim_name: # This field was moved to auth.cookie_auth_verification\n  ui_config:\n    login_url: https://lakefs.company.com/sso/login-saml\n    logout_url: https://lakefs.company.com/sso/logout-saml\n    login_cookie_names:\n      - internal_auth_session\n      - saml_auth_session\n</code></pre> <p>OIDC + OIDC STS</p> lakeFS + Fluffy (old)lakeFS Enterprise (new) <pre><code># fluffy.yaml\nauth:\n  post_login_redirect_url: /\n  logout_redirect_url: https://oidc-provider-url.com/logout/url\n  oidc:\n    enabled: true\n    url: https://oidc-provider-url.com/\n    client_id: &lt;oidc-client-id&gt;\n    client_secret: &lt;oidc-client-secret&gt;\n    callback_base_url: https://lakefs.company.com\n    is_default_login: true\n    logout_client_id_query_parameter: client_id\n    logout_endpoint_query_parameters:\n    - returnTo \n    - https://lakefs.company.com/oidc/login\n</code></pre> <pre><code># lakefs.yaml\nauth:\n  oidc:\n    friendly_name_claim_name: \"name\"\n    persist_friendly_name: true\n    default_initial_groups: [\"Developers\"]\n  ui_config:\n    login_url: /oidc/login\n    logout_url: /oidc/logout\n    login_cookie_names:\n    - internal_auth_session\n    - oidc_auth_session\n</code></pre> <pre><code># lakefs.yaml\nauth:\n  logout_redirect_url:  https://oidc-provider-url.com/logout/url # optional, URL to redirect to after logout \n  ui_config:\n    login_url: /oidc/login\n    logout_url: /oidc/logout\n    login_cookie_names:\n      - internal_auth_session\n      - oidc_auth_session\n  oidc:\n    friendly_name_claim_name: \"nickname\"\n    default_initial_groups: [\"Admins\"]\n  providers:\n    oidc:\n      # enabled: true  # This field was dropped!\n      post_login_redirect_url: / # This field was moved here!\n      url: https://oidc-provider-url.com/\n      client_id: &lt;oidc-client-id&gt;\n      client_secret: &lt;oidc-client-secret&gt;\n      callback_base_url: https://lakefs.company.com\n      logout_client_id_query_parameter: client_id\n      logout_endpoint_query_parameters:\n        - returnTo\n        - http://lakefs.company.com/oidc/login\n</code></pre> <p>LDAP</p> lakeFS + Fluffy (old)lakeFS Enterprise (new) <p><pre><code># fluffy.yaml\nauth:\n  post_login_redirect_url: /\n  ldap: \n    server_endpoint: ldaps://ldap.company.com:636\n    bind_dn: uid=&lt;bind-user-name&gt;,ou=&lt;some-ou&gt;,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\n    bind_password: '&lt;ldap password&gt;'\n    username_attribute: uid\n    user_base_dn: ou=&lt;some-ou&gt;,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\n    user_filter: (objectClass=inetOrgPerson)\n    connection_timeout_seconds: 15\n    request_timeout_seconds: 7\n</code></pre> <pre><code># lakefs.yaml\nauth:\n  remote_authenticator:\n    enabled: true\n    endpoint: http://&lt;Fluffy URL&gt;:&lt;Fluffy http port&gt;/api/v1/ldap/login\n    default_user_group: \"Developers\" # Value needs to correspond with an existing group in lakeFS\n  ui_config:\n    logout_url: /logout\n    login_cookie_names:\n    - internal_auth_session\n</code></pre></p> <pre><code># lakefs.yaml\nauth:\n  ui_config:\n    logout_url: /logout\n    login_cookie_names:\n      - internal_auth_session\n  providers:\n    ldap:\n      server_endpoint: ldaps://ldap.company.com:636\n      bind_dn: uid=&lt;bind-user-name&gt;,ou=&lt;some-ou&gt;,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\n      bind_password: '&lt;ldap password&gt;'\n      username_attribute: uid \n      user_base_dn: ou=&lt;some-ou&gt;,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com \n      user_filter: (objectClass=inetOrgPerson)\n      connection_timeout_seconds: 15\n      request_timeout_seconds: 7\n      default_user_group: \"Developers\" # This field moved here!\n</code></pre> <p>AWS IAM</p> lakeFS + Fluffy (old)lakeFS Enterprise (new) <p><pre><code># fluffy.yaml\nserve_listen: \"localhost:9001\"\nauth:\n  external:\n    aws_auth:\n      enabled: true\n      required_headers:\n        X-LakeFS-Server-ID: \"localhost\"\n</code></pre> <pre><code># lakefs.yaml\nauth:\n  authentication_api:\n    endpoint: http://localhost:9001/api/v1\n    external_principals_enabled: true\n</code></pre></p> <pre><code># lakefs.yaml\nauth:\n  external_aws_auth:\n    enabled: true\n    required_headers:\n      X-LakeFS-Server-ID: \"localhost\"\n</code></pre>"},{"location":"enterprise/upgrade/#authorization-configuration","title":"Authorization configuration","text":"<p>RBAC</p> lakeFS + Fluffy (old)lakeFS Enterprise (new) <pre><code># fluffy.yaml\nauth:\n  serve_listen_address: \"localhost:9000\"\n  cache:\n    enabled: true\n</code></pre> <pre><code># lakefs.yaml\nauth:\n  api:\n   endpoint: http://localhost:9000/api/v1\n</code></pre> <pre><code># lakefs.yaml\nauth:\n  # serve_disable_authentication: false      # this field was dropped!\n  # serve_listen_address: \"localhost:9000\"   # this field was dropped!\n  # api:                                     # this field was dropped! \n  #   endpoint: http://localhost:9000/api/v1 # this field was dropped!\n  cache:\n    enabled: true\n</code></pre>"},{"location":"enterprise/upgrade/#kubernetes-migrating-with-helm-from-fluffy-to-new-lakefs-enterprise","title":"Kubernetes: Migrating with Helm from Fluffy to new lakeFS Enterprise","text":""},{"location":"enterprise/upgrade/#overview","title":"Overview","text":"<p>Starting with lakeFS Helm chart version 1.5.0, the Fluffy authentication service has been deprecated and replaced with native lakeFS Enterprise authentication. This migration consolidates authentication into the main lakeFS application, simplifying deployment and maintenance.</p>"},{"location":"enterprise/upgrade/#whats-changing","title":"What's Changing","text":"<p>When you upgrade to lakeFS Enterprise:</p> <ul> <li>Fluffy Deployment Removed: The separate Fluffy deployment, service, and associated Kubernetes resources are no longer needed</li> <li>Simplified Architecture: Authentication is now handled directly by lakeFS Enterprise, reducing the number of pods and services</li> <li>Streamlined Ingress: No more routing between Fluffy and lakeFS - all traffic goes directly to lakeFS</li> <li>Updated values.yaml Structure: Authentication configuration moves from <code>fluffy.*</code> to <code>enterprise.auth.*</code> and <code>lakefsConfig.auth.providers.*</code></li> </ul>"},{"location":"enterprise/upgrade/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Current lakeFS deployment using Fluffy authentication (chart version &lt; 1.5.0)</li> <li>Access to update Helm values</li> <li>lakeFS Enterprise Docker Hub token</li> <li>Backup of your current values.yaml</li> </ul>"},{"location":"enterprise/upgrade/#step-by-step-migration-guide","title":"Step-by-Step Migration Guide","text":""},{"location":"enterprise/upgrade/#step-1-update-helm-repository","title":"Step 1: Update Helm Repository","text":"<pre><code>helm repo update lakefs\n</code></pre> <p>Verify you have access to chart version 1.5.0 or later: <pre><code>helm search repo lakefs/lakefs --versions\n</code></pre></p>"},{"location":"enterprise/upgrade/#step-2-review-new-chart-values","title":"Step 2: Review New Chart Values","text":"<p>Examine all available configuration options in the new chart: <pre><code>helm show values lakefs/lakefs --version 1.5.0 &gt; new-values-reference.yaml\n</code></pre></p>"},{"location":"enterprise/upgrade/#step-3-update-your-image-configuration","title":"Step 3: Update Your Image Configuration","text":"<p>If you're overriding the image in your values.yaml, update it to use lakeFS Enterprise:</p> <pre><code>image:\n  repository: treeverse/lakefs-enterprise\n  tag: 1.63.0 \n  privateRegistry:\n    enabled: true\n    secretToken: &lt;your-dockerhub-token&gt;\n</code></pre> <p>Note: If you're not overriding the image, the chart will automatically use the correct Enterprise image.</p>"},{"location":"enterprise/upgrade/#step-35-license-configuration","title":"Step 3.5: License Configuration","text":"<p>Note</p> <p>You can proceed without this step at the moment. However, licensing enforcement will be introduced soon. Contact support to receive your installation license.</p> <p>lakeFS Enterprise requires a valid license to work. In the helm chart the license is provided as a JWT token either from an existing secret or explicitly. The following demonstrates how to configure the license in the <code>values</code> file:</p> License with token provided in secretsLicense with token provided from existing secret <pre><code>enterprise:\n  enabled: true\n\nsecrets:\n    licenseContents: &lt;Your licese JWT token&gt;\n</code></pre> <pre><code>enterprise:\n  enabled: true\n\n# Name of existing secret to use\nexistingSecret: &lt;Name of existing secret&gt;\n\nsecretKeys:\n    # Use to fetch license token from an existing secret:\n    licenseContentsKey: &lt;Name of license contents key from existing secret&gt;\n</code></pre>"},{"location":"enterprise/upgrade/#step-4-migrate-your-authentication-configuration","title":"Step 4: Migrate Your Authentication Configuration","text":"<p>Using the configuration examples below, update your values.yaml file: 1. Remove all <code>fluffy.*</code> configuration sections 2. Add the new <code>enterprise.auth.*</code> configuration for your authentication method 3. Move authentication settings to <code>lakefsConfig.auth.providers.*</code></p> <p>Refer to the complete examples in the lakeFS Helm chart repository.</p>"},{"location":"enterprise/upgrade/#step-5-validate-with-dry-run","title":"Step 5: Validate with Dry Run","text":"<p>Before applying changes, validate your configuration: <pre><code>helm upgrade &lt;release-name&gt; lakefs/lakefs \\\n  --version 1.5.0 \\\n  --namespace &lt;namespace&gt; \\\n  --values &lt;your-updated-values.yaml&gt; \\\n  --dry-run\n</code></pre></p> <p>Review the output to ensure: - No Fluffy resources are being created - lakeFS Enterprise deployment is configured correctly - Ingress configuration is simplified</p>"},{"location":"enterprise/upgrade/#step-6-perform-the-upgrade","title":"Step 6: Perform the Upgrade","text":"<p>Once validated, perform the actual upgrade: <pre><code>helm upgrade &lt;release-name&gt; lakefs/lakefs \\\n  --version 1.5.0 \\\n  --namespace &lt;namespace&gt; \\\n  --values &lt;your-updated-values.yaml&gt;\n</code></pre></p>"},{"location":"enterprise/upgrade/#step-7-verify-the-migration","title":"Step 7: Verify the Migration","text":"<p>After the upgrade completes:</p> <ol> <li> <p>Check Pod Status:    <pre><code>kubectl get pods -n &lt;namespace&gt;\n# Fluffy pods should no longer exist\n</code></pre></p> </li> <li> <p>Verify lakeFS Health:    <pre><code>kubectl exec -n &lt;namespace&gt; &lt;lakefs-pod&gt; -- curl http://localhost:8000/_health\n</code></pre></p> </li> <li> <p>Check Logs:    <pre><code>kubectl logs -n &lt;namespace&gt; &lt;lakefs-pod&gt;\n# Look for successful authentication provider initialization\n</code></pre></p> </li> <li> <p>Test Authentication:</p> <ul> <li>Navigate to your lakeFS URL</li> <li>Verify SSO login works correctly</li> <li>Confirm RBAC permissions are preserved</li> </ul> </li> <li> <p>Verify Fluffy Resources Removed:    <pre><code>kubectl get all -n &lt;namespace&gt; | grep fluffy\n# Should return no results\n</code></pre></p> </li> </ol>"},{"location":"enterprise/upgrade/#step-8-rollback-if-needed","title":"Step 8: Rollback (if needed)","text":"<p>If you encounter issues, rollback to the previous version: <pre><code># Find the previous revision\nhelm history &lt;release-name&gt; -n &lt;namespace&gt;\n\n# Rollback to previous revision\nhelm rollback &lt;release-name&gt; &lt;previous-revision&gt; -n &lt;namespace&gt;\n</code></pre></p>"},{"location":"enterprise/upgrade/#configuration-examples","title":"Configuration Examples","text":"<p>Below are complete configuration examples for each authentication method, showing both the old (Fluffy) and new (Enterprise) configurations:</p>"},{"location":"enterprise/upgrade/#oidc-with-helm","title":"OIDC with Helm","text":"<p>OIDC with Helm</p> lakeFS + Fluffy (old)lakeFS Enterprise (new) <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    # the ingress that will be created for lakeFS\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths: \n        - /\n\nfluffy:\n  enabled: true\n  image:\n    privateRegistry:\n      enabled: true\n      secretToken: &lt;dockerhub-token-fluffy-image&gt;\n  fluffyConfig: |\n    auth:\n      logout_redirect_url: https://oidc-provider-url.com/logout/example\n      oidc:\n        enabled: true\n        url: https://oidc-provider-url.com/\n        client_id: &lt;oidc-client-id&gt;\n        callback_base_url: https://&lt;lakefs.ingress.domain&gt;\n        # the claim name that represents the client identifier in the OIDC provider (e.g Okta)\n        logout_client_id_query_parameter: client_id\n        # the query parameters that will be used to redirect the user to the OIDC provider (e.g Okta) after logout\n        logout_endpoint_query_parameters:\n          - returnTo\n          - https://&lt;lakefs.ingress.domain&gt;/oidc/login\n  secrets:\n    create: true\n  sso:\n    enabled: true\n    oidc:\n      enabled: true\n      # secret given by the OIDC provider (e.g auth0, Okta, etc)\n      client_secret: &lt;oidc-client-secret&gt;\n  rbac:\n    enabled: true\n\nlakefsConfig: |\n  database:\n    type: local\n  blockstore:\n    type: local\n  auth:\n    ui_config:\n      login_cookie_names:\n        - internal_auth_session\n        - oidc_auth_session\n    oidc:\n      friendly_name_claim_name: &lt;some-oidc-provider-claim-name&gt;\n      default_initial_groups: [\"Developers\"]\n</code></pre> <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    # the ingress that will be created for lakeFS\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths: \n        - /\n\nenterprise:\n  enabled: true\n  auth:\n    oidc:\n      enabled: true\n      # secret given by the OIDC provider (e.g auth0, Okta, etc)\n      client_secret: &lt;oidc-client-secret&gt;\n\nimage:\n  privateRegistry:\n    enabled: true\n    secretToken: &lt;dockerhub-token&gt;\n\nlakefsConfig: |\n  blockstore:\n    type: local\n  auth:\n    logout_redirect_url: https://oidc-provider-url.com/logout/example\n    oidc:\n      friendly_name_claim_name: &lt;some-oidc-provider-claim-name&gt;\n      default_initial_groups: [\"Developers\"]\n    providers:\n      oidc:\n        post_login_redirect_url: /\n        url: https://oidc-provider-url.com/\n        client_id: &lt;oidc-client-id&gt;\n        callback_base_url: https://&lt;lakefs.ingress.domain&gt;\n        # the claim name that represents the client identifier in the OIDC provider (e.g Okta)\n        logout_client_id_query_parameter: client_id\n        # the query parameters that will be used to redirect the user to the OIDC provider (e.g Okta) after logout\n        logout_endpoint_query_parameters:\n          - returnTo\n          - https://&lt;lakefs.ingress.domain&gt;/oidc/login\n</code></pre>"},{"location":"enterprise/upgrade/#saml-with-helm","title":"SAML with Helm","text":"<p>SAML with Helm</p> lakeFS + Fluffy (old)lakeFS Enterprise (new) <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    # the ingress that will be created for lakeFS\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths: \n        - /\n\nfluffy:\n  enabled: true\n  image:\n    privateRegistry:\n      enabled: true\n      secretToken: &lt;dockerhub-token-fluffy-image&gt;\n  fluffyConfig: |\n    auth:  \n      # logout_redirect_url: https://&lt;lakefs.ingress.domain&gt;\n      # post_login_redirect_url: https://&lt;lakefs.ingress.domain&gt;\n      saml:\n        sp_sign_request: true\n        # depends on IDP\n        sp_signature_method: \"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\"\n        # url to the metadata of the IDP\n        idp_metadata_url: \"https://&lt;adfs-auth.company.com&gt;/federationmetadata/2007-06/federationmetadata.xml\"\n        # IDP SAML claims format default unspecified\n        # idp_authn_name_id_format: \"urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified\"\n        # claim name from IDP to use as the unique user name\n        external_user_id_claim_name: samName\n        # depending on IDP setup, if CA certs are self signed and not trusted by a known CA\n        idp_skip_verify_tls_cert: true\n  rbac:\n    enabled: true\n  secrets:\n    create: true\n  sso:\n    enabled: true\n    saml:\n      enabled: true\n      createSecret: true\n      lakeFSServiceProviderIngress: https://&lt;lakefs.ingress.domain&gt;\n      certificate:\n        saml_rsa_public_cert: |\n          -----BEGIN CERTIFICATE-----\n          ...\n          -----END CERTIFICATE-----\n        saml_rsa_private_key: |\n          -----BEGIN PRIVATE KEY-----\n          ...\n          -----END PRIVATE KEY-----\n\nlakefsConfig: | \n  blockstore:\n    type: local\n  auth:\n    cookie_auth_verification:\n    # claim name to display user in the UI\n      friendly_name_claim_name: displayName\n      # claim name from IDP to use as the unique user name\n      external_user_id_claim_name: samName\n      default_initial_groups:\n        - \"Developers\"\n    ui_config:\n      login_cookie_names:\n        - internal_auth_session\n        - saml_auth_session\n</code></pre> <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    # the ingress that will be created for lakeFS\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths: \n        - /\n\nenterprise:\n  enabled: true\n  auth:\n    saml:\n      enabled: true\n      createCertificateSecret: true\n      certificate:\n        samlRsaPublicCert: |\n          -----BEGIN CERTIFICATE-----\n          ...\n          -----END CERTIFICATE-----\n        samlRsaPrivateKey: |\n          -----BEGIN PRIVATE KEY-----\n          ...\n          -----END PRIVATE KEY-----\n\nimage:\n  privateRegistry:\n    enabled: true\n    secretToken: &lt;dockerhub-token&gt;\n\nlakefsConfig: |\n  blockstore:\n    type: local\n  auth:\n    logout_redirect_url: https://&lt;lakefs.ingress.domain&gt;\n    cookie_auth_verification:\n      auth_source: saml\n      # claim name to display user in the UI\n      friendly_name_claim_name: displayName\n      # claim name from IDP to use as the unique user name\n      external_user_id_claim_name: samName\n      default_initial_groups:\n        - \"Developers\"\n    providers:\n      saml:\n        post_login_redirect_url: https://&lt;lakefs.ingress.domain&gt;\n        sp_root_url: https://&lt;lakefs.ingress.domain&gt;\n        sp_sign_request: true\n        # depends on IDP\n        sp_signature_method: \"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\"\n        # url to the metadata of the IDP\n        idp_metadata_url: \"https://&lt;adfs-auth.company.com&gt;/federationmetadata/2007-06/federationmetadata.xml\"\n        # IDP SAML claims format default unspecified\n        idp_authn_name_id_format: \"urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified\"\n        # depending on IDP setup, if CA certs are self signed and not trusted by a known CA\n        #idp_skip_verify_tls_cert: true\n</code></pre>"},{"location":"enterprise/upgrade/#ldap-with-helm","title":"LDAP with Helm","text":"<p>LDAP with Helm</p> lakeFS + Fluffy (old)lakeFS Enterprise (new) <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    # the ingress that will be created for lakeFS\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths: \n       - /\n\nfluffy:\n  enabled: true\n  image:\n    privateRegistry:\n      enabled: true\n      secretToken: &lt;dockerhub-token-fluffy-image&gt;\n  fluffyConfig: |\n    auth:\n      post_login_redirect_url: /\n      ldap: \n        server_endpoint: ldaps://ldap.company.com:636\n        bind_dn: uid=&lt;bind-user-name&gt;,ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\n        username_attribute: uid\n        user_base_dn: ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\n        user_filter: (objectClass=inetOrgPerson)\n        connection_timeout_seconds: 15\n        request_timeout_seconds: 7\n\n  secrets:\n    create: true\n\n  sso:\n    enabled: true\n    ldap:\n      enabled: true\n      bind_password: &lt;ldap bind password&gt;\n  rbac:\n    enabled: true\n\nlakefsConfig: |\n  blockstore:\n    type: local\n  auth:\n    remote_authenticator:\n      enabled: true\n      default_user_group: \"Developers\"\n    ui_config:\n      login_cookie_names:\n        - internal_auth_session\n</code></pre> <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    # the ingress that will be created for lakeFS\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths:\n        - /\n\nenterprise:\n  enabled: true\n  auth:\n    ldap:\n      enabled: true\n      bindPassword: &lt;ldap bind password&gt;\n\nimage:\n  privateRegistry:\n    enabled: true\n    secretToken: &lt;dockerhub-token&gt;\n\nlakefsConfig: |\n  blockstore:\n    type: local\n  auth:\n    ui_config:\n      login_cookie_names:\n        - internal_auth_session\n    providers:\n      ldap:\n        server_endpoint: ldaps://ldap.company.com:636\n        bind_dn: uid=&lt;bind-user-name&gt;,ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com        \n        username_attribute: uid\n        user_base_dn: ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com        \n        user_filter: (objectClass=inetOrgPerson)\n        default_user_group: \"Developers\"\n        connection_timeout_seconds: 15\n        request_timeout_seconds: 7\n</code></pre>"},{"location":"enterprise/upgrade/#aws-iam-with-helm","title":"AWS IAM with Helm","text":"<p>AWS IAM with Helm</p> lakeFS + Fluffy (old)lakeFS Enterprise (new) <pre><code>lakefsConfig: |\n  auth:\n    authentication_api:\n      external_principals_enabled: true\ningress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    # the ingress that will be created for lakeFS\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths:\n        - /\n\nfluffy:\n  enabled: true\n  image:\n    repository: treeverse/fluffy\n    pullPolicy: IfNotPresent\n    privateRegistry:\n      enabled: true\n      secretToken: &lt;dockerhub-token-fluffy-image&gt;\n  fluffyConfig: |\n    auth:\n      external:\n        aws_auth:\n          enabled: true\n          # the maximum age in seconds for the GetCallerIdentity request\n          #get_caller_identity_max_age: 60\n          # headers that must be present by the client when doing login request\n          required_headers:\n            # same host as the lakeFS server ingress\n            X-LakeFS-Server-ID: &lt;lakefs.ingress.domain&gt;\n  secrets:\n    create: true\n  sso:\n    enabled: true\n  rbac:\n    enabled: true\n</code></pre> <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    # the ingress that will be created for lakeFS\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths:\n        - /\n\nlakefsConfig: |\n  auth:\n    external_aws_auth:\n      enabled: true\n      # the maximum age in seconds for the GetCallerIdentity request\n      #get_caller_identity_max_age: 60\n      # headers that must be present by the client when doing login request\n      required_headers:\n        # same host as the lakeFS server ingress\n        X-LakeFS-Server-ID: &lt;lakefs.ingress.domain&gt;\n</code></pre>"},{"location":"enterprise/upgrade/#important-notes","title":"Important Notes","text":"<ul> <li>Complete configuration examples for each authentication method are available in the lakeFS Helm chart repository</li> <li>The examples include local blockstore for quick-start - replace with S3/Azure/GCS for production deployments</li> <li>Configure the <code>image.privateRegistry.secretToken</code> with your DockerHub token for accessing enterprise images</li> <li>Update all placeholder values (marked with <code>&lt;&gt;</code>) with your actual configuration</li> </ul>"},{"location":"enterprise/upgrade/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues during migration:</p> <ol> <li>Authentication Failures: Check that all authentication settings have been properly moved to the new configuration structure</li> <li>Image Pull Errors: Ensure your DockerHub token has access to the lakeFS Enterprise image</li> <li>Ingress Issues: Confirm that your ingress is pointing directly to lakeFS (not Fluffy)</li> </ol> <p>For additional support, consult the lakeFS documentation or contact lakeFS support.</p>"},{"location":"enterprise/getstarted/install/","title":"Install","text":"<p>For production deployments of lakeFS Enterprise, follow this guide.</p>"},{"location":"enterprise/getstarted/install/#lakefs-enterprise-architecture","title":"lakeFS Enterprise Architecture","text":"<p>We recommend reviewing the lakeFS Enterprise architecture to understand the components you will be deploying.</p> <p>Note</p> <p>Fluffy service is deprecated in chart version 1.5.0 and later. For more information, see the Upgrade Guide.</p>"},{"location":"enterprise/getstarted/install/#deploy-lakefs-enterprise-on-kubernetes","title":"Deploy lakeFS Enterprise on Kubernetes","text":"<p>The guide is using the lakeFS Helm Chart to deploy a fully functional lakeFS Enterprise.</p> <p>The guide includes example configurations, follow the steps below and adjust the example configurations according to:</p> <ul> <li>The platform you run on: among the platform supported by lakeFS</li> <li>Type of KV store you use</li> <li>Your SSO IdP and protocol</li> </ul>"},{"location":"enterprise/getstarted/install/#prerequisites","title":"Prerequisites","text":"<ol> <li>You have a Kubernetes cluster running in one of the platforms supported by lakeFS.</li> <li>Helm is installed</li> <li>Access to download treeverse/lakefs-enterprise from Docker Hub. Contact us to gain access to lakeFS Enterprise features.</li> <li>A KV Database. The available options are dependent in your deployment platform.</li> <li>A method to route traffic into lakeFS from outside of the cluster (via Ingress or Service).</li> </ol>"},{"location":"enterprise/getstarted/install/#optional","title":"Optional","text":"<p>Access to configure your SSO IdP supported by lakeFS Enterprise.</p> <p>Info</p> <p>You can install lakeFS Enterprise without configuring SSO and still benefit from all other lakeFS Enterprise features.</p>"},{"location":"enterprise/getstarted/install/#lakefs-enterprise-license","title":"lakeFS Enterprise License","text":"<p>Using lakeFS Enterprise requires a valid license. This license is a JSON Web Token (JWT) that provides access to the  lakeFS Enterprise server and paid features. A license is tied to a specific installation via an installation ID, and to  a specific organization via an organization ID.</p>"},{"location":"enterprise/getstarted/install/#licensed-features","title":"Licensed Features","text":"<p>The following lakeFS Enterprise features must be included in the lakeFS Enterprise license in order to be available:</p>"},{"location":"enterprise/getstarted/install/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<ul> <li>SSO (Single Sign-On)</li> <li>RBAC (Role-Based Access Control)</li> <li>SCIM (System for Cross-domain Identity Management)</li> <li>IAM (Identity and Access Management) Role Authentication</li> </ul>"},{"location":"enterprise/getstarted/install/#advanced-functionality","title":"Advanced Functionality","text":"<ul> <li>Mount</li> <li>Iceberg Catalog</li> <li>Metadata Search</li> <li>MSB (Multiple Storage Backends)</li> <li>Transactional Mirroring</li> <li>Sparkless GC (Garbage Collection)</li> </ul> <p>Warning</p> <p>Without a license for these features, they will be disabled, and any attempt to access them will result in a 'feature not licensed' error.</p>"},{"location":"enterprise/getstarted/install/#license-configuration-in-lakefs-enterprise","title":"License Configuration in lakeFS Enterprise","text":"<ol> <li> <p>Contact Support for a License \ud83d\udce7 Email: support@treeverse.io</p> </li> <li> <p>Receive Your License Token You will receive a license token that contains:  </p> <ul> <li>Organization ID  </li> <li>Installation ID  </li> <li>Issue date  </li> <li>Expiry date  </li> <li>Enabled features  </li> <li>Feature limitations  </li> </ul> </li> <li> <p>Configure lakeFS Enterprise Server </p> <ol> <li>Save the license token to a file.  </li> <li>Provide the file path in the lakeFS Enterprise configuration file:   <pre><code>license:\n  path: \"/path/to/your/license.txt\"\n</code></pre></li> </ol> <p>Tip</p> <p>Instead of configuring the license token path via the lakeFS Enterprise configuration file, you can set it via the environment variable: <code>LAKEFS_LICENSE_PATH</code>.</p> </li> </ol> Reading a Currently Installed License Token via API <p>Request: <pre><code>GET https://your-lakefs-server/api/v1/license\n</code></pre></p> <p>Response: <pre><code>{\n  \"token\": \"eyJhbGciOiJSUzI1NiIs...\"\n}\n</code></pre></p> <p>The returned token can be decoded using any JWT decoding tool to view the license information.</p>"},{"location":"enterprise/getstarted/install/#license-monitoring-updates","title":"License Monitoring &amp; Updates","text":""},{"location":"enterprise/getstarted/install/#updating-your-license","title":"Updating Your License","text":"<ol> <li>Replace the content of your license file with the new license token.  </li> <li>lakeFS Enterprise will automatically detect and reload the new license within 1 minute.  </li> </ol> <p>Warning</p> <p>You cannot change the file path itself to point to a new file while the server is running.</p>"},{"location":"enterprise/getstarted/install/#automatic-monitoring","title":"Automatic Monitoring","text":"<ul> <li>Validation and Expiry Check: Occurs periodically to verify license validity and expiry.</li> <li>File Monitoring: The server checks periodically to detect license file changes so that when the license token gets updated in the file, there is no need to restart the server and the license updates automatically.</li> </ul>"},{"location":"enterprise/getstarted/install/#troubleshooting","title":"Troubleshooting","text":""},{"location":"enterprise/getstarted/install/#server-wont-start","title":"Server Won't Start","text":"<p><pre><code>Error: no valid license found for this lakeFS Enterprise Server\n</code></pre> Solution: Ensure you have configured the <code>license.path</code> in the lakeFS Enterprise configuration file or environment variables.</p>"},{"location":"enterprise/getstarted/install/#license-signature-verification-failed","title":"License Signature Verification Failed","text":"<p><pre><code>Error: token signature did not match any known public key: token signature is invalid: crypto/rsa\n</code></pre> Solutions:     1. Re-download your license token from the original source.     2. Verify the license file contains only the license token with no extra characters or line breaks.     3. Contact support@treeverse.io to get a valid license token.  </p>"},{"location":"enterprise/getstarted/install/#license-has-expired","title":"License Has Expired","text":"<p><pre><code>Error: license has expired. Please contact support immediately: invalid license found\n</code></pre> <pre><code>Error: license expired: invalid expiry date\n</code></pre> Solutions:      1. If recently renewed, ensure you've updated the license file with the new token.     2. Contact support@treeverse.io immediately for license renewal.  </p>"},{"location":"enterprise/getstarted/install/#license-file-not-found","title":"License File Not Found","text":"<p><pre><code>Error: open /path/to/license/file/you/provided: no such file or directory\n</code></pre> Solutions:     1. Verify the exact path and filename in your <code>license.path</code> configuration.     2. Check for typos in filename.     3. Check that license file wasn't moved or removed after configuration.     4. Check that the license file exists at the specified location.     5. Ensure the file extension is included.     6. Use absolute paths instead of relative paths.  </p>"},{"location":"enterprise/getstarted/install/#license-file-permission-denied","title":"License File Permission Denied","text":"<p><pre><code>Error: open /path/to/license/file/you/provided: permission denied\n</code></pre> Solutions:     1. Check file permissions and set appropriate permissions.     2. Ensure lakeFS Enterprise process has permission to read the file and access parent directories.  </p>"},{"location":"enterprise/getstarted/install/#installation-id-mismatch","title":"Installation ID Mismatch","text":"<p><pre><code>Error: license belongs to installation ID X (current installation ID: Y)\n</code></pre> Solutions:     1. Check if you have the correct license file for this specific installation.     2. Check that your installation ID didn't change.     3. Contact support@treeverse.io to get a correct new license.  </p>"},{"location":"enterprise/getstarted/install/#malformed-or-empty-license-token","title":"Malformed or Empty License Token","text":"<p><pre><code>Error: parsing token: token is malformed: token contains an invalid number of segments\n</code></pre> <pre><code>Error: license has invalid expiry (no license?): invalid license found\n</code></pre> Solutions:     1. Verify the license file contains the complete license token.     2. Remove any extra whitespace, newlines, or characters from the license file.     3. Re-copy the license token from the original source.     4. Ensure the file contains only the license token and nothing else.  </p>"},{"location":"enterprise/getstarted/install/#feature-not-available","title":"Feature Not Available","text":"<p><pre><code>Error: feature not licensed - to enable, contact support@treeverse.io\n</code></pre> Solution: Your current license doesn't include this feature. Contact support@treeverse.io to upgrade your license.</p> <p>Need Help?</p> <p>If you encounter an issue not covered here, contact our support team at support@treeverse.io.</p>"},{"location":"enterprise/getstarted/install/#add-the-lakefs-helm-chart","title":"Add the lakeFS Helm Chart","text":"<ul> <li>Add the lakeFS Helm repository with <code>helm repo add lakefs https://charts.lakefs.io</code></li> <li>The chart contains a values.yaml file you can customize to suit your needs as you follow this guide. Use <code>helm show values lakefs/lakefs</code> to see the default values.</li> <li>Configure <code>image.privateRegistry.secretToken</code> with the Docker Hub token you received.</li> </ul>"},{"location":"enterprise/getstarted/install/#license-configuration","title":"License Configuration","text":"<p>Note</p> <p>You can proceed without this step at the moment. However, licensing enforcement will be introduced soon.  Contact support to receive your installation license.</p> <p>lakeFS Enterprise requires a valid license to work. In the helm chart the license is provided as a JWT token either from an existing secret or explicitly. The following demonstrates how to configure the license in the <code>values</code> file:</p> License with token provided in secretsLicense with token provided from existing secret <pre><code>enterprise:\n  enabled: true\n\nsecrets:\n    licenseContents: &lt;Your licese JWT token&gt;\n</code></pre> <pre><code>enterprise:\n  enabled: true\n\n# Name of existing secret to use\nexistingSecret: &lt;Name of existing secret&gt;\n\nsecretKeys:\n  # Use to fetch license token from an existing secret:\n   licenseContentsKey: &lt;Name of license contents key from existing secret&gt;\n</code></pre>"},{"location":"enterprise/getstarted/install/#authentication-configuration","title":"Authentication Configuration","text":"<p>Authentication in lakeFS Enterprise is handled directly by the lakeFS Enterprise service. This section explains the configurations required for setting up SSO.</p> <p>See SSO for lakeFS Enterprise for the supported identity providers and protocols.</p> <p>The examples below include example configuration for each of the supported SSO protocols. Note the IdP-specific details you'll need to replace with your IdP details.</p> OpenID ConnectSAML (With Azure AD)LDAP <p>The following <code>values</code> file will run lakeFS Enterprise with OIDC integration.</p> <p>Tip</p> <p>The full OIDC configurations explained here.</p> <pre><code>enterprise:\n  enabled: true\n  auth:\n    oidc:\n      enabled: true\n      # secret given by the OIDC provider (e.g auth0, Okta, etc)\n      client_secret: &lt;oidc-client-secret&gt;\n\nimage:\n  privateRegistry:\n    enabled: true\n    secretToken: &lt;dockerhub-token&gt;\n\nlakefsConfig: |\n  logging:\n    level: \"INFO\"\n  blockstore:\n    type: s3\n  auth:\n    logout_redirect_url: https://oidc-provider-url.com/logout/example\n    oidc:\n      # the claim that's provided by the OIDC provider (e.g Okta) that will be used as the username according to OIDC provider claims provided after successful authentication\n      friendly_name_claim_name: \"&lt;some-oidc-provider-claim-name&gt;\"\n      default_initial_groups: [\"Developers\", \"Admins\"]\n      # if true then the value of friendly_name_claim_name will be refreshed during each login to maintain the latest value\n      # and the the claim value (i.e user name) will be stored in the lakeFS database\n      persist_friendly_name: true\n    providers:\n      oidc:\n        post_login_redirect_url: /\n        url: https://oidc-provider-url.com/ \n        client_id: &lt;oidc-client-id&gt;         \n        callback_base_url: https://&lt;lakefs.acme.com&gt;\n        # the claim name that represents the client identifier in the OIDC provider (e.g Okta)\n        logout_client_id_query_parameter: client_id\n        # the query parameters that will be used to redirect the user to the OIDC provider after logout\n        logout_endpoint_query_parameters:\n          - returnTo\n          - https://&lt;lakefs.acme.com&gt;/oidc/login\n\ningress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    - host: &lt;lakefs.acme.com&gt;\n      paths:\n        - /\n</code></pre> <p>The following <code>values</code> file will run lakeFS Enterprise with SAML using Azure AD as the IdP.</p> <p>You can use this example configuration to configure Active Directory Federation Services (AD FS) with SAML.</p> <p>Tip</p> <p>The full SAML configurations explained here.</p> <p>The following <code>values</code> file will run lakeFS Enterprise with LDAP.</p> <p>Tip</p> <p>The full LDAP configurations explained here.</p> <pre><code>enterprise:\n  enabled: true\n  auth:\n    ldap:\n      enabled: true\n      bindPassword: &lt;ldap bind password&gt;\n\nimage:\n  privateRegistry:\n    enabled: true\n    secretToken: &lt;dockerhub-token&gt;\n\nlakefsConfig: |\n  logging:\n    level: \"INFO\"\n  blockstore:\n    type: local\n  auth:\n    ui_config:\n      login_url: /auth/login\n      logout_url: /logout\n      login_cookie_names:\n        - internal_auth_session\n    providers:\n      ldap:\n        server_endpoint: 'ldaps://ldap.company.com:636'\n        bind_dn: uid=&lt;bind-user-name&gt;,ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\n        username_attribute: uid\n        user_base_dn: ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\n        user_filter: (objectClass=inetOrgPerson)\n        connection_timeout_seconds: 15\n        request_timeout_seconds: 17\n        # RBAC group for first time users\n        default_user_group: \"Developers\"\n\ningress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    - host: &lt;lakefs.acme.com&gt;\n      paths:\n        - /\n</code></pre> <p>See additional examples on GitHub we provide for each authentication method (oidc, saml, ldap, rbac, external AWS IAM).</p>"},{"location":"enterprise/getstarted/install/#azure-app-configuration","title":"Azure App Configuration","text":"<ol> <li>Create an Enterprise Application with SAML toolkit - see Azure quickstart</li> <li>Add users: App &gt; Users and groups: Attach users and roles from their existing AD users   list - only attached users will be able to login to lakeFS.</li> <li>Configure SAML: App &gt;  Single sign-on &gt; SAML:</li> <li>Entity ID: Add 2 ID's, lakefs-url + lakefs-url/saml/metadata (e.g. https://lakefs.acme.com and https://lakefs.acme.com/saml/metadata)</li> <li>Reply URL: lakefs-url/saml (e.g. https://lakefs.acme.com/saml)</li> <li>Sign on URL: lakefs-url/sso/login-saml (e.g. https://lakefs.acme.com/sso/login-saml)</li> <li>Relay State (Optional, controls where to redirect after login): /</li> </ol>"},{"location":"enterprise/getstarted/install/#saml-configuration","title":"SAML Configuration","text":"<ol> <li>Configure SAML application in your IdP (i.e Azure AD) and replace the required parameters into the <code>values.yaml</code> below.</li> <li>To generate certificates keypair use: <code>openssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=lakefs.acme.com\"</code></li> </ol> <pre><code>enterprise:\n  enabled: true\n  auth:\n    saml:\n      enabled: true\n      createCertificateSecret: true  # NEW: Auto-creates secret\n      certificate:\n        # certificate and private key for the SAML service provider to sign outgoing SAML requests\n        samlRsaPublicCert: |          # RENAMED: from saml_rsa_public_cert\n          -----BEGIN CERTIFICATE-----\n          ...\n          -----END CERTIFICATE-----\n        samlRsaPrivateKey: |          # RENAMED: from saml_rsa_private_key\n          -----BEGIN PRIVATE KEY-----\n          ...\n          -----END PRIVATE KEY-----\n\nsecrets:\n  authEncryptSecretKey: \"some random secret string\"\n\nimage:\n  privateRegistry:\n    enabled: true\n    secretToken: &lt;dockerhub-token&gt;\n\nlakefsConfig: |\n  logging:\n    level: \"DEBUG\"\n  blockstore:\n    type: local\n  auth:\n    logout_redirect_url: https://&lt;lakefs.acme.com&gt;\n    cookie_auth_verification:\n      auth_source: saml\n      # claim name to use for friendly name in lakeFS UI\n      friendly_name_claim_name: displayName\n      external_user_id_claim_name: samName\n      default_initial_groups:\n        - \"Developers\"\n    providers:\n      saml:\n        post_login_redirect_url: https://&lt;lakefs.acme.com&gt;\n        sp_root_url: https://&lt;lakefs.acme.com&gt;\n        sp_sign_request: false \n        sp_signature_method: \"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\"\n        idp_metadata_url: \"https://&lt;adfs-auth.company.com&gt;/federationmetadata/2007-06/federationmetadata.xml\" \n        # the default id format urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified\n        # idp_authn_name_id_format: \"urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified\"\n        idp_skip_verify_tls_cert: true\n\ningress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  annotations: {}\n  hosts:\n    - host: &lt;lakefs.acme.com&gt;\n      paths:\n        - /\n</code></pre>"},{"location":"enterprise/getstarted/install/#database-configuration","title":"Database Configuration","text":"<p>In this section, you will learn how to configure lakeFS Enterprise to work with the KV Database you created (see prerequisites).</p> <p>Notes:</p> <ul> <li>By default, the lakeFS Helm chart comes with <code>useDevPostgres: false</code>, you can change it to <code>useDevPostgres: true</code> for dev use. This setup is useful when you want to run a setup with multiple replicas or want to prevent data loss between containers restarts.</li> <li>See lakeFS database configuration.</li> </ul> <p>The database configuration can be set directly via <code>lakefsConfig</code>, via K8S Secret Kind, or via environment variables.</p> Postgres via environment variablesVia lakefsConfigPostgres via shared Secret kind <p>This example uses Postgres as KV Database configured via environment variables.</p> <pre><code>extraEnvVars:\n  - name: LAKEFS_DATABASE_TYPE\n    value: postgres\n  - name: LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING\n    value: '&lt;postgres connection string&gt;'\n</code></pre> <p>This example uses DynamoDB as KV Database.</p> <pre><code>lakefsConfig: |\n  database:\n    type: dynamodb\n    dynamodb:\n      table_name: &lt;table&gt;\n      aws_profile: &lt;profile&gt;\n      aws_region: &lt;region&gt;\n</code></pre> <p>This example uses Postgres as KV Database. The chart will create a <code>kind: Secret</code> holding the database connection string.</p> <pre><code>secrets:\n  authEncryptSecretKey: shared-key-hello\n  databaseConnectionString: &lt;postgres connection string&gt;\n\nlakefsConfig: |\n  database:\n    type: postgres\n</code></pre>"},{"location":"enterprise/getstarted/install/#install-the-lakefs-helm-chart","title":"Install the lakeFS Helm Chart","text":"<p>After populating your values.yaml file with the relevant configuration, in the desired K8S namespace run <code>helm install lakefs lakefs/lakefs -f values.yaml</code></p>"},{"location":"enterprise/getstarted/install/#access-the-lakefs-ui","title":"Access the lakeFS UI","text":"<p>In your browser, go to the Ingress host to access lakeFS UI.</p>"},{"location":"enterprise/getstarted/install/#log-collection","title":"Log Collection","text":"<p>The recommended practice for collecting logs would be sending them to the container std (default configuration) and letting an external service to collect them to a sink. An example for logs collector would be fluentbit that can collect container logs, format them and ship them to a target like S3.</p> <p>There are 2 kinds of logs: - Regular logs like an API error or some event description used for debugging - Audit logs that describe user actions (i.e create branch)</p> <p>The distinction between regular logs and audit_logs is in the boolean field <code>log_audit</code>.</p>"},{"location":"enterprise/getstarted/install/#advanced-deployment-configurations","title":"Advanced Deployment Configurations","text":"<p>The following example demonstrates a scenario where you need to configure an HTTP proxy for lakeFS, TLS certificates for the Ingress and extending the K8S manifests without forking the Helm chart.</p> <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  # configure TLS certificate for the Ingress\n  tls:\n    - hosts:\n      - lakefs.acme.com\n      secretName: somesecret\n  hosts:\n    - host: lakefs.acme.com\n      paths:\n       - /\n\n# configure proxy for lakeFS\nextraEnvVars:\n  - name: HTTP_PROXY\n    value: 'http://my.company.proxy:8081'\n  - name: HTTPS_PROXY\n    value: 'http://my.company.proxy:8081'\n\n# advanced: extra manifests to extend the K8S resources\nextraManifests:\n  - apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: '{% raw %}{{ .Values.lakefs.name }}{% endraw %}-extra-config'\n    data:\n      config.yaml: my-data\n</code></pre>"},{"location":"enterprise/getstarted/migrate-from-oss/","title":"Migrate From lakeFS Open-Source to lakeFS Enterprise","text":"<p>To migrate from lakeFS Open Source to lakeFS Enterprise, follow the steps below:</p> <ol> <li>Make sure you have the lakeFS Enterprise Docker token. if not, contact us to gain access to lakeFS Enterprise. You will be granted a token that enables downloading dockerhub/lakefs-enterprise from Docker Hub.</li> <li>Update the lakeFS Docker image to the enterprise version. Replace <code>treeverse/lakefs</code> with <code>treeverse/lakefs-enterprise</code> in your configuration. The enterprise image can be pulled using your lakeFS Enterprise token.</li> <li>Sanity Test (Optional): Install a new test lakeFS Enterprise before moving your current production setup. Test the setup &gt; login &gt; Create repository etc. Once everything seems to work, delete and cleanup the test setup and we will move to the migration process.</li> <li>Make sure to configure the lakeFS enterprise properly, see Enterprise installation guide<ol> <li>Update your existing lakeFS configuration for enterprise (i.e <code>values.yaml</code> file for your deployment if using helm)</li> </ol> </li> <li>DB Migration: We are going to use the same database for lakeFS Enterprise, so we need to migrate the database schema.</li> <li>Make sure to SSH / exec into the lakeFS server (old pre-upgrade version); the point is to use the same lakeFS configuration file when running a migration.<ol> <li>If upgrading <code>lakefs</code> version do this or skip to the next step: Install the new lakeFS binary, if not use the existing one (the one you are running).</li> <li>Run the command: <code>LAKEFS_AUTH_UI_CONFIG_RBAC=internal lakefs migrate up</code> (use the new binary if upgrading lakeFS version).</li> <li>You should expect to see a log message saying Migration completed successfully.</li> <li>During this short DB migration process, please make sure not to make any policy / RBAC related changes.</li> </ol> </li> <li>Once the migration completed - Upgrade your helm release with the modified <code>values.yaml</code> and the new version and run <code>helm ugprade</code>.</li> <li> <p>Login to the new lakeFS pod: Execute the following command, make sure you have proper credentials, or discard to get new ones:</p> <pre><code>lakefs setup --user-name &lt;admin&gt; --access-key-id &lt;key&gt; --secret-access-key &lt;secret&gt; --no-check\n</code></pre> </li> </ol> <p>Warning</p> <p>Please note that the newly set-up lakeFS instance remains inaccessible to users until full setup completion, due to the absence of established credentials within the system.</p>"},{"location":"enterprise/getstarted/quickstart/","title":"Quickstart","text":"<p>Follow these quickstarts to try out lakeFS Enterprise.</p> <p>Warning</p> <p>lakeFS Enterprise Quickstarts are not suitable for production use-cases. See the installation guide to set up a production-grade lakeFS Enterprise installation</p>"},{"location":"enterprise/getstarted/quickstart/#lakefs-enterprise-sample","title":"lakeFS Enterprise Sample","text":"<p>The lakeFS Enterprise Sample is the quickest way to experience the value of lakeFS Enterprise features in a containerized environment. This Docker-based setup is ideal if you want to easily interact with lakeFS without the hassle of integration and experiment with lakeFS without writing code.</p> <p>By running the lakeFS Enterprise Sample, you will be getting a ready-to-use environment including the following containers:</p> <ul> <li>lakeFS Enterprise (includes additional features)</li> <li>Postgres: used by lakeFS as a KV store</li> <li>MinIO container: used as the storage connected to lakeFS</li> <li>Jupyter notebooks setup: Pre-populated with notebooks that demonstrate lakeFS Enterprise' capabilities</li> <li>Apache Spark: this is useful for interacting with data you'll manage with lakeFS</li> </ul> <p>Checkout the RBAC demo notebook to see lakeFS Enterprise Role-Based Access Control capabilities in action.</p>"},{"location":"enterprise/getstarted/quickstart/#docker-quickstart","title":"Docker Quickstart","text":""},{"location":"enterprise/getstarted/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Note</p> <p>In order to use lakeFS enterprise you must have:</p> <ul> <li>Access token to download binaries from Docker hub</li> <li>License to run lakeFS Enterprise</li> </ul> <p>Contact us to gain access for both.</p> <ol> <li>You have installed Docker Compose version <code>2.23.1</code> or higher on your machine.</li> <li>Access to download treeverse/lakefs-enterprise from Docker Hub.</li> <li>With the token you've been granted, login locally to Docker Hub with <code>docker login -u externallakefs -p &lt;TOKEN&gt;</code>.</li> </ol> <p> The quickstart docker-compose file below creates a lakeFS server that's connected to a local blockstore and spin up the following containers:</p> <ul> <li>lakeFS Enterprise</li> <li>Postgres: used by lakeFS as a KV store</li> </ul> <p>You can choose from the following options:</p> <ol> <li>Recommended: A fully functional lakeFS Enterprise setup without SSO support</li> <li> <p>Advanced: A fully functional lakeFS Enterprise setup including SSO support with OIDC integration configured</p> <p>Info</p> <p>If you can postpone the evaluation of the SSO integration, we suggest starting without it to speed up overall testing. The SSO integration requires additional configurations and is best addressed later.</p> </li> </ol> Recommended (SSO Disabled)Advanced (SSO Enabled) <ol> <li>Create a <code>docker-compose.yaml</code> file with the following content</li> <li>Run <code>docker compose up</code> in the same directory as the <code>docker-compose.yaml</code> file.</li> <li>In your browser, go to http://localhost:8080 to access lakeFS UI.</li> </ol> <pre><code>version: \"3\"\nservices:\n  lakefs:\n    image: \"treeverse/lakefs-enterprise:latest\"\n    command: \"RUN\"\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - \"postgres\"\n    environment:\n      - LAKEFS_LISTEN_ADDRESS=0.0.0.0:8000\n      - LAKEFS_LOGGING_LEVEL=DEBUG\n      - LAKEFS_AUTH_ENCRYPT_SECRET_KEY=random_secret\n      - LAKEFS_AUTH_UI_CONFIG_RBAC=internal\n      - LAKEFS_DATABASE_TYPE=postgres\n      - LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=postgres://lakefs:lakefs@postgres:5432/postgres?sslmode=disable\n      - LAKEFS_BLOCKSTORE_TYPE=local\n      - LAKEFS_BLOCKSTORE_LOCAL_PATH=/home/lakefs\n      - LAKEFS_BLOCKSTORE_LOCAL_IMPORT_ENABLED=true\n      - LAKEFS_AUTH_POST_LOGIN_REDIRECT_URL=http://localhost:8000/\n      - LAKEFS_FEATURES_LOCAL_RBAC=true\n      - LAKEFS_LICENSE_CONTENTS=&lt;license token&gt; # for production use, we recommend using LAKEFS_LICENSE_PATH instead\n    configs:\n      - source: lakefs.yaml\n        target: /etc/lakefs/config.yaml\n  postgres:\n    image: \"postgres:11\"\n    ports:\n      - \"5433:5432\"\n    environment:\n      POSTGRES_USER: lakefs\n      POSTGRES_PASSWORD: lakefs\n\nconfigs:\n  lakefs.yaml:\n    content: |\n      auth:\n        ui_config:\n          login_cookie_names:\n            - internal_auth_session\n</code></pre> <p>This setup uses OIDC as the SSO authentication method, thus requiring a valid OIDC configuration.</p> <ol> <li>Create a <code>docker-compose.yaml</code> with the content below.</li> <li>Create a <code>.env</code> file with the configurations below in the same directory as the <code>docker-compose.yaml</code>, docker compose will automatically use that.</li> <li>Run <code>docker compose up</code> in the same directory as the <code>docker-compose.yaml</code> file.</li> <li>Validate the OIDC configuration:</li> <li>In your browser, go to http://localhost:8080 to access lakeFS UI</li> <li>Complete the Setup process, and login with your Admin credentials</li> <li>Logout and try to login again, you will be redirected to the OIDC login page.</li> </ol> <p><code>.env</code></p> <pre><code>LAKEFS_AUTH_PROVIDERS_OIDC_CLIENT_ID=&lt;your-oidc-client-id&gt;\nLAKEFS_AUTH_PROVIDERS_OIDC_CLIENT_SECRET=&lt;your-oidc-client-secret&gt;\n# The name of the query parameter that is used to pass the client ID to the logout endpoint of the SSO provider, i.e client_id\nLAKEFS_AUTH_PROVIDERS_OIDC_LOGOUT_CLIENT_ID_QUERY_PARAMETER=\nLAKEFS_AUTH_PROVIDERS_OIDC_URL=https://my-sso.com/\nLAKEFS_AUTH_LOGOUT_REDIRECT_URL=https://my-sso.com/logout\n# Optional: display a friendly name in the lakeFS UI by specifying which claim from the provider to show (i.e name, nickname, email etc)\nLAKEFS_AUTH_OIDC_FRIENDLY_NAME_CLAIM_NAME=\nLAKEFS_LICENSE_CONTENTS=&lt;license token&gt; # for production use, we recommend using LAKEFS_LICENSE_PATH instead\n</code></pre> <p><code>docker-compose.yaml</code></p> <pre><code>version: \"3\"\nservices:\n  lakefs:\n    image: \"treeverse/lakefs-enterprise:latest\"\n    command: \"RUN\"\n    ports:\n      - \"8080:8080\"\n    depends_on:\n      - \"postgres\"\n    environment:\n      - LAKEFS_LISTEN_ADDRESS=0.0.0.0:8000\n      - LAKEFS_LOGGING_LEVEL=DEBUG\n      - LAKEFS_LOGGING_AUDIT_LOG_LEVEL=INFO\n      - LAKEFS_AUTH_ENCRYPT_SECRET_KEY=shared-secret-key\n      - LAKEFS_AUTH_LOGOUT_REDIRECT_URL=${LAKEFS_AUTH_LOGOUT_REDIRECT_URL}\n      - LAKEFS_AUTH_UI_CONFIG_LOGIN_URL=http://localhost:8000/oidc/login\n      - LAKEFS_AUTH_UI_CONFIG_LOGOUT_URL=http://localhost:8000/oidc/logout\n      - LAKEFS_AUTH_UI_CONFIG_RBAC=internal\n      - LAKEFS_AUTH_OIDC_FRIENDLY_NAME_CLAIM_NAME=${LAKEFS_AUTH_OIDC_FRIENDLY_NAME_CLAIM_NAME}\n      - LAKEFS_AUTH_PROVIDERS_OIDC_ENABLED=true\n      - LAKEFS_AUTH_PROVIDERS_OIDC_POST_LOGIN_REDIRECT_URL=http://localhost:8000/\n      - LAKEFS_AUTH_PROVIDERS_OIDC_URL=${LAKEFS_AUTH_PROVIDERS_OIDC_URL}\n      - LAKEFS_AUTH_PROVIDERS_OIDC_CLIENT_ID=${LAKEFS_AUTH_PROVIDERS_OIDC_CLIENT_ID}\n      - LAKEFS_AUTH_PROVIDERS_OIDC_CLIENT_SECRET=${LAKEFS_AUTH_PROVIDERS_OIDC_CLIENT_SECRET}\n      - LAKEFS_AUTH_PROVIDERS_OIDC_CALLBACK_BASE_URL=http://localhost:8000\n      - LAKEFS_AUTH_PROVIDERS_OIDC_LOGOUT_CLIENT_ID_QUERY_PARAMETER=${LAKEFS_AUTH_OIDC_LOGOUT_CLIENT_ID_QUERY_PARAMETER}\n      - LAKEFS_LICENSE_CONTENTS=${LAKEFS_LICENSE_CONTENTS} # for production use, we recommend using LAKEFS_LICENSE_PATH instead\n      - LAKEFS_AUTH_PROVIDERS_OIDC_LOGOUT_CLIENT_ID_QUERY_PARAMETER=${LAKEFS_AUTH_PROVIDERS_OIDC_LOGOUT_CLIENT_ID_QUERY_PARAMETER}\n      - LAKEFS_DATABASE_TYPE=postgres\n      - LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=postgres://lakefs:lakefs@postgres:5432/postgres?sslmode=disable\n      - LAKEFS_BLOCKSTORE_TYPE=local\n      - LAKEFS_BLOCKSTORE_LOCAL_PATH=/tmp/lakefs/data\n      - LAKEFS_BLOCKSTORE_LOCAL_IMPORT_ENABLED=true\n      - LAKEFS_FEATURES_LOCAL_RBAC=true\n    entrypoint: [\"/app/wait-for\", \"postgres:5432\", \"--\", \"/app/lakefs\", \"run\"]\n    configs:\n      - source: lakefs.yaml\n        target: /etc/lakefs/config.yaml\n  postgres:\n    image: \"postgres:11\"\n    ports:\n      - \"5433:5432\"\n    environment:\n      POSTGRES_USER: lakefs\n      POSTGRES_PASSWORD: lakefs\n\n#This tweak is unfortunate but also necessary. logout_endpoint_query_parameters is a list\n#of strings which isn't parsed nicely as env vars.\nconfigs:\n  lakefs.yaml:\n    content: |\n      auth:\n        ui_config:\n          login_cookie_names:\n            - internal_auth_session\n            - oidc_auth_session\n        oidc:\n          # friendly_name_claim_name: \"name\"\n          default_initial_groups:\n            - Admins\n        providers:\n          oidc:\n            logout_endpoint_query_parameters:\n              - returnTo\n              - http://localhost:8000/oidc/login\n</code></pre>"},{"location":"enterprise/getstarted/quickstart/#kubernetes-helm-chart-quickstart","title":"Kubernetes Helm Chart Quickstart","text":"<p>In order to use lakeFS Enterprise, we provided out of the box setup, see lakeFS Helm chart configuration.</p> <p>The values below create a fully functional lakeFS Enterprise setup without SSO support. The created setup is connected to a local blockstore, and spins up the following pods:</p> <ul> <li>lakeFS Enterprise</li> <li>Postgres: used by lakeFS as a KV store</li> </ul> <p>Info</p> <p>If you can postpone the evaluation of the SSO integration, we suggest starting without it to speed up overall testing. The SSO integration requires additional configurations and is best addressed later. To try lakeFS Enterprise SSO capability on a Kubernetes cluster, check out the production deployment guide.</p>"},{"location":"enterprise/getstarted/quickstart/#prerequisites_1","title":"Prerequisites","text":"<ol> <li>You have a Kubernetes cluster running in one of the platforms supported by lakeFS.</li> <li>Helm is installed</li> <li>Access to download treeverse/lakefs-enterprise from Docker Hub.</li> <li>lakeFS Enterprise license    Contact us to gain access to lakeFS Enterprise.</li> </ol>"},{"location":"enterprise/getstarted/quickstart/#instructions","title":"Instructions","text":"<ol> <li>Add the lakeFS Helm repository with <code>helm repo add lakefs https://charts.lakefs.io</code></li> <li>Create a <code>values.yaml</code> file with the following content and make sure to replace <code>&lt;lakefs-enterprise-docker-registry-token&gt;</code> with the Docker Hub token you received, <code>&lt;lakefs.acme.com&gt;</code> and <code>&lt;ingress-class-name&gt;</code>.</li> <li>In the desired K8S namespace run <code>helm install lakefs lakefs/lakefs -f values.yaml</code></li> <li>In your browser, go to the Ingress host to access lakeFS UI.</li> </ol> <pre><code>enterprise:\n  enabled: true\n\nimage:\n  privateRegistry:\n    enabled: true\n    secretToken: &lt;lakefs-enterprise-docker-registry-token&gt;\n\nlakefsConfig: |\n  logging:\n    level: \"DEBUG\"\n  blockstore:\n    type: local\n  auth:\n    ui_config:\n      rbac: internal\n\ningress:\n  enabled: true\n  ingressClassName: &lt;ingress-class-name&gt;\n  annotations: {}\n  hosts:\n    - host: &lt;lakefs.acme.com&gt;\n      paths:\n       - /\n\n# useDevPostgres is false by default and will override any other db configuration, \n# set false or remove for configuring your own db\nuseDevPostgres: true\n</code></pre>"},{"location":"howto/backup-and-restore/","title":"Backup and Restore Repository","text":"<p>This section explains how to backup and restore lakeFS repository for different use-cases:</p> <ol> <li>Disaster Recovery: you want to backup the repository regularly so you can restore it in case of any disaster.  You'd also need to make sure to backup the repository's storage namespace to another, preferably geographically separate location.</li> <li>Migrate Repository: you want to migrate a repository from one environment to another lakeFS environment.</li> <li>Clone Repository: you want to clone a repository.</li> </ol> <p>Tip</p> <p>Refer to Python Sample Notebooks to backup, migrate or clone a lakeFS repository</p>"},{"location":"howto/backup-and-restore/#commit-changes","title":"Commit Changes","text":"<p>Backup process doesn't backup uncommitted data, so make sure to commit any staged writes before running the backup. But this is an optional process.</p> <p>You can manually commit the changes by using lakeFS UI or you can programmatically commit any uncommitted changes. This Python example shows how to programmatically loop through all branches in lakeFS and commit any uncommitted data but this might take a lot of time if you have many branches in the repo:</p> <pre><code>import lakefs\n\nrepo = lakefs.Repository(\"example-repo\")\n\nfor branch in repo.branches():\n    for diff in repo.branch(branch.id).uncommitted():\n        repo.branch(branch.id).commit(message='Committed changes to backup the repository')\n</code></pre>"},{"location":"howto/backup-and-restore/#backup-repository","title":"Backup Repository","text":""},{"location":"howto/backup-and-restore/#dump-metadata","title":"Dump Metadata","text":"<p>Dump metadata/refs of the repository by using lakeFS API or CLI.</p> <ul> <li>Example code to dump metadata by using lakeFS Python SDK (this process will create <code>_lakefs/refs_manifest.json</code> file in your storage namespace for the repository):</li> </ul> <pre><code>lakefs_sdk_client.internal_api.dump_refs(\"example-repo\")\n</code></pre> <ul> <li>Example commands to dump metadata by using lakeFS CLI and upload to S3 storage for the repository:</li> </ul> <pre><code>lakectl refs-dump lakefs://example-repo &gt; refs_manifest.json\n\naws s3 cp refs_manifest.json s3://source-bucket-name/example-repo/_lakefs/refs_manifest.json\n</code></pre> <ul> <li>Example commands to dump metadata by using lakeFS CLI and upload to Azure Blob storage for the repository:</li> </ul> <pre><code>lakectl refs-dump lakefs://example-repo &gt; refs_manifest.json\n\naz storage blob upload --file refs_manifest.json --container-name sourceContainer --name example-repo/_lakefs/refs_manifest.json --account-name source-storage-account-name --account-key &lt;source-storage-account-key&gt;\n</code></pre> <p>Warning</p> <p>Shutdown lakeFS services immediately after dumping the metadata so nobody can make any changes in the source repository.</p>"},{"location":"howto/backup-and-restore/#copy-data-to-backup-storage-location","title":"Copy Data to Backup Storage Location","text":"<p>Copy the repository's storage namespace to another, preferably geographically separate location. Copy command depends on the type of object storage and the tool that you use.</p> <ul> <li>Example S3 command:</li> </ul> <pre><code>aws s3 sync s3://source-bucket-name/example-repo s3://target-bucket-name/example-repo\n</code></pre> <ul> <li>Example Azure azcopy command:</li> </ul> <pre><code>azcopy copy 'https://source-storage-account-name.blob.core.windows.net/sourceContainer/example-repo/*?source_container_SAS_token' 'https://target-storage-account-name.blob.core.windows.net/targetContainer/example-repo?target_container_SAS_token' --recursive\n</code></pre> <p>Info</p> <p>You can restart lakeFS services after copying the data to backup storage location.</p>"},{"location":"howto/backup-and-restore/#restore-repository","title":"Restore Repository","text":""},{"location":"howto/backup-and-restore/#create-a-new-bare-repository","title":"Create a new Bare Repository","text":"<p>Create a bare lakeFS repository with a new name if you want to clone the repository or use the same repository name if you want to migrate or restore the repository.</p> <ul> <li>Python example to create a bare lakeFS repository using S3 storage:</li> </ul> <pre><code>lakefs.Repository(\"target-example-repo\").create(bare=True, storage_namespace=\"s3://target-bucket-name/example-repo\", default_branch=\"same-default-branch-as-in-source-repo\")\n</code></pre> <ul> <li>Python example to create a bare lakeFS repository using Azure storage:</li> </ul> <pre><code>lakefs.Repository(\"target-example-repo\").create(bare=True, storage_namespace=\"https://target-storage-account-name.blob.core.windows.net/targetContainer/example-repo\", default_branch=\"same-default-branch-as-in-source-repo\")\n</code></pre> <ul> <li>lakeFS CLI command to create a bare lakeFS repository using S3 storage:</li> </ul> <pre><code>lakectl repo create-bare lakefs://target-example-repo s3://target-bucket-name/example-repo --default-branch \"same-default-branch-as-in-source-repo\"\n</code></pre> <ul> <li>lakeFS CLI command to create a bare lakeFS repository using Azure storage:</li> </ul> <pre><code>lakectl repo create-bare lakefs://target-example-repo https://target-storage-account-name.blob.core.windows.net/targetContainer/example-repo --default-branch \"same-default-branch-as-in-source-repo\"\n</code></pre>"},{"location":"howto/backup-and-restore/#restore-metadata-to-new-repository","title":"Restore Metadata to new Repository","text":"<p>Run restore_refs to load back all commits, tags and branches.</p> <ul> <li>Python example to restore metadata to new repository. First download metadata(refs_manifest.json) file created by metadata dump process:</li> </ul> <pre><code>aws s3 cp s3://target-bucket-name/example-repo/_lakefs/refs_manifest.json .\n</code></pre> <pre><code>azcopy copy 'https://target-storage-account-name.blob.core.windows.net/targetContainer/example-repo/_lakefs/refs_manifest.json?&lt;target_container_SAS_token&gt;' .\n</code></pre> <p>Then read refs_manifest.json file and restore metadata to new repository:</p> <pre><code>with open('./refs_manifest.json') as file:\n    refs_manifest_json = json.load(file)\n    print(refs_manifest_json)\n\ntarget_lakefs_sdk_client.internal_api.restore_refs(target_repo_name, refs_manifest_json)\n</code></pre> <ul> <li>lakeFS CLI command to restore metadata to new repository using S3 storage:</li> </ul> <pre><code>aws s3 cp s3://target-bucket-name/example-repo/_lakefs/refs_manifest.json - | lakectl refs-restore lakefs://target-example-repo --manifest -\n</code></pre> <ul> <li>lakeFS CLI command to restore metadata to new repository using Azure storage:</li> </ul> <pre><code>az storage blob download --container-name targetContainer --name example-repo/_lakefs/refs_manifest.json --account-name target-storage-account-name --account-key &lt;target-storage-account-key&gt; | lakectl refs-restore lakefs://target-example-repo --manifest -\n</code></pre> <p>Tip</p> <p>If you are running backups regularly, it is highly advised to test the restore process periodically to make sure that you are able to restore the repository in case of disaster.</p>"},{"location":"howto/backup-and-restore/#python-helper-script-for-backup-and-restore","title":"Python Helper Script for Backup and Restore","text":"<p>For more streamlined repository backup and restore operations, you can use the <code>lakefs-refs.py</code> script available in the lakeFS repository.</p>"},{"location":"howto/backup-and-restore/#overview","title":"Overview","text":"<p>The <code>lakefs-refs.py</code> script automates the backup and restore procedures described in this document. It handles all the necessary steps to dump and restore lakeFS repository references, making the process simpler and less error-prone for repository migration and backup purposes.</p>"},{"location":"howto/backup-and-restore/#prerequisites","title":"Prerequisites","text":"<p>Install the required dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"howto/backup-and-restore/#usage-examples","title":"Usage Examples","text":""},{"location":"howto/backup-and-restore/#dump-repository-references","title":"Dump Repository References","text":"<p>The script provides an easy way to dump repository metadata:</p> <pre><code>python lakefs-refs.py dump &lt;repository-name&gt; [--all] [--commit] [--rm] [--endpoint-url &lt;url&gt;] [--access-key-id &lt;key&gt;] [--secret-access-key &lt;secret&gt;]\n</code></pre> <p>Options:</p> <ul> <li><code>&lt;repository-name&gt;</code>: Name of the specific repository to dump</li> <li><code>--all</code>: Dump all repositories instead of a specific one</li> <li><code>--commit</code>: Commit any uncommitted changes before dumping</li> <li><code>--rm</code>: Delete repository definition after successful dump</li> <li>Authentication options (see below)</li> </ul>"},{"location":"howto/backup-and-restore/#restore-repository-references","title":"Restore Repository References","text":"<p>For restoring repository metadata from manifest files:</p> <pre><code>python lakefs-refs.py restore &lt;manifest-file&gt; [&lt;manifest-file2&gt; ...] [--ignore-storage-id] [--endpoint-url &lt;url&gt;] [--access-key-id &lt;key&gt;] [--secret-access-key &lt;secret&gt;]\n</code></pre> <p>Options:</p> <ul> <li><code>&lt;manifest-file&gt;</code>: One or more manifest files to restore</li> <li><code>--ignore-storage-id</code>: Create repository without storage_id (useful when migrating to a different storage backend)</li> <li>Authentication options (see below)</li> </ul>"},{"location":"howto/backup-and-restore/#authentication","title":"Authentication","text":"<p>The script uses the same authentication method as <code>lakectl</code>, supporting authentication via command line parameters, environment variables (<code>LAKECTL_*</code>), or the <code>~/.lakectl.yaml</code> configuration file.</p>"},{"location":"howto/catalog_exports/","title":"Data Catalogs Export","text":""},{"location":"howto/catalog_exports/#about-data-catalogs-export","title":"About Data Catalogs Export","text":"<p>Data Catalog Export is all about integrating query engines (like Spark, AWS Athena, Presto, etc.) with lakeFS.</p> <p>Data Catalogs (such as Hive Metastore or AWS Glue) store metadata for services (such as Spark, Trino and Athena). They contain metadata such as the location of the table, information about columns, partitions and much more.</p> <p>With Data Catalog Exports, one can leverage the versioning capabilities of lakeFS in external data warehouses and query engines to access tables with branches and commits. </p> <p>At the end of this guide, you will be able to query lakeFS data from Athena, Trino and other catalog-dependent tools:</p> <pre><code>USE main;\nUSE my_branch; -- any branch\nUSE v101; -- or tag\n\nSELECT * FROM users \nINNER JOIN events \nON users.id = events.user_id; -- SQL stays the same, branch or tag exist as schema\n</code></pre>"},{"location":"howto/catalog_exports/#how-it-works","title":"How it works","text":"<p>Several well known formats exist today let you export existing tables in lakeFS into a \"native\" object store representation which does not require copying the data outside of lakeFS.</p> <p>These are metadata representations and can be applied automatically through hooks.</p>"},{"location":"howto/catalog_exports/#table-declaration","title":"Table Declaration","text":"<p>After creating a lakeFS repository, configure tables as table descriptor objects on the repository on the path <code>_lakefs_tables/TABLE.yaml</code>. Note: the Glue exporter can currently only export tables of <code>type: hive</code>.  We expect to add more.</p>"},{"location":"howto/catalog_exports/#hive-tables","title":"Hive tables","text":"<p>Hive metadata server tables are essentially just a set of objects that share a prefix, with no table metadata stored on the object store.  You need to configure prefix, partitions, and schema.</p> <pre><code>name: animals\ntype: hive\npath: path/to/animals/\npartition_columns: ['year']\nschema:\n  type: struct\n  fields:\n    - name: year\n      type: integer\n      nullable: false\n      metadata: {}\n    - name: page\n      type: string\n      nullable: false\n      metadata: {}\n    - name: site\n      type: string\n      nullable: true\n      metadata:\n        comment: a comment about this column\n</code></pre> <p>Tip</p> <p>Useful types recognized by Hive include <code>integer</code>, <code>long</code>, <code>short</code>, <code>string</code>, <code>double</code>, <code>float</code>, <code>date</code>, and <code>timestamp</code>.</p>"},{"location":"howto/catalog_exports/#catalog-exporters","title":"Catalog Exporters","text":"<p>Exporters are code packages accessible through Lua integration. Each exporter is exposed as a Lua function under the package namespace <code>lakefs/catalogexport</code>.  Call them from hooks to connect lakeFS tables to various catalogs.</p>"},{"location":"howto/catalog_exports/#currently-supported-exporters","title":"Currently supported exporters","text":"Exporter Description Notes Symlink exporter Writes metadata for the table using Hive's SymlinkTextInputFormat AWS Glue Catalog (+ Athena) exporter Creates a table in Glue using Hive's format and updates the location to symlink files (reuses Symlink Exporter). See a step-by-step guide on how to integrate with Glue Exporter Delta Lake table exporter Export Delta Lake tables from lakeFS to an external storage Unity Catalog exporter The Unity Catalog exporter serves the purpose of registering a Delta Lake table in Unity Catalog. It operates in conjunction with the Delta Lake exporter. In this workflow, the Delta Lake exporter is utilized to export a Delta Lake table from lakeFS. Subsequently, the obtained result is passed to the Unity Catalog exporter to facilitate its registration within Unity Catalog. See a step-by-step guide on how to integrate with Unity Catalog ExporterCurrently, only AWS S3 storage is supported"},{"location":"howto/catalog_exports/#running-an-exporter","title":"Running an Exporter","text":"<p>Exporters are meant to run as Lua hooks.</p> <p>Configure the actions trigger by using events and branches.  Of course, you can add additional custom filtering logic to the Lua script if needed. The default table name when exported is <code>${repository_id}_${_lakefs_tables/TABLE.md(name field)}_${ref_name}_${short_commit}</code>.</p> <p>Example of an action that will be triggered when a <code>post-commit</code> event happens in the <code>export_table</code> branch.</p> <pre><code>name: Glue Table Exporter\ndescription: export my table to glue  \non:\n  post-commit:\n    branches: [\"export_table\"]\nhooks:\n  - id: my_exporter\n    type: lua\n    properties:\n      # exporter script location\n      script_path: \"scripts/my_export_script.lua\"\n      args:\n        # table descriptor\n        table_source: '_lakefs_tables/my_table.yaml'\n</code></pre> <p>Tip: Actions can be extended to customize any desired behavior, for example validating branch names since they are part of the table name: </p> <pre><code># _lakefs_actions/validate_branch_name.yaml\nname: validate-lower-case-branches \non:\n  pre-create-branch:\nhooks:\n  - id: check_branch_id\n    type: lua\n    properties:\n      script: |\n        regexp = require(\"regexp\")\n        if not regexp.match(\"^[a-z0-9\\\\_\\\\-]+$\", action.branch_id) then\n          error(\"branches must be lower case, invalid branch ID: \" .. action.branch_id)\n        end\n</code></pre>"},{"location":"howto/catalog_exports/#flow","title":"Flow","text":"<p>The following diagram demonstrates what happens when a lakeFS Action triggers runs a lua hook that calls an exporter.</p> <pre><code>sequenceDiagram\n    note over Lua Hook: lakeFS Action trigger. &lt;br&gt; Pass Context for the export.\n    Lua Hook-&gt;&gt;Exporter: export request\n    note over Table Registry: _lakefs_tables/TABLE.yaml\n    Exporter-&gt;&gt;Table Registry: Get table descriptor\n    Table Registry-&gt;&gt;Exporter: Parse table structure\n    Exporter-&gt;&gt;Object Store: materialize an exported table\n    Exporter-&gt;&gt;Catalog: register object store location\n    Query Engine--&gt;Catalog: Query\n    Query Engine--&gt;Object Store: Query</code></pre>"},{"location":"howto/copying/","title":"Copying data to/from lakeFS","text":""},{"location":"howto/copying/#using-distcp","title":"Using DistCp","text":"<p>Apache Hadoop DistCp (distributed copy) is a tool used for large inter/intra-cluster copying. You can easily use it with your lakeFS repositories.</p> <p>Info</p> <p>In the following examples, we set AWS credentials on the command line for clarity.  In production, you should set these properties using one of Hadoop's standard ways of Authenticating with S3. </p>"},{"location":"howto/copying/#between-lakefs-repositories","title":"Between lakeFS repositories","text":"<p>You can use DistCP to copy between two different lakeFS repositories. Replace the access key pair with your lakeFS access key pair:</p> <pre><code>hadoop distcp \\\n  -Dfs.s3a.path.style.access=true \\\n  -Dfs.s3a.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\\n  -Dfs.s3a.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\\n  -Dfs.s3a.endpoint=\"https://lakefs.example.com\" \\\n  \"s3a://example-repo-1/main/example-file.parquet\" \\\n  \"s3a://example-repo-2/main/example-file.parquet\"\n</code></pre>"},{"location":"howto/copying/#between-s3-buckets-and-lakefs","title":"Between S3 buckets and lakeFS","text":"<p>To copy data from an S3 bucket to a lakeFS repository, use Hadoop's per-bucket configuration. In the following examples, replace the first access key pair with your lakeFS key pair, and the second one with your AWS IAM key pair:</p>"},{"location":"howto/copying/#from-s3-to-lakefs","title":"From S3 to lakeFS","text":"<pre><code>hadoop distcp \\\n  -Dfs.s3a.path.style.access=true \\\n  -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\\n  -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\\n  -Dfs.s3a.bucket.example-repo.endpoint=\"https://lakefs.example.com\" \\\n  -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\\n  -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\\n  \"s3a://example-bucket/example-file.parquet\" \\\n  \"s3a://example-repo/main/example-file.parquet\"\n</code></pre>"},{"location":"howto/copying/#from-lakefs-to-s3","title":"From lakeFS to S3","text":"<pre><code>hadoop distcp \\\n  -Dfs.s3a.path.style.access=true \\\n  -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\\n  -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\\n  -Dfs.s3a.bucket.example-repo.endpoint=\"https://lakefs.example.com\" \\\n  -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\\n  -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\\n  \"s3a://example-repo/main/myfile\" \\\n  \"s3a://example-bucket/myfile\"\n</code></pre>"},{"location":"howto/copying/#using-rclone","title":"Using Rclone","text":"<p>Rclone is a command line program to sync files and directories between cloud providers. To use it with lakeFS, create an Rclone remote as describe below and then use it as you would any other Rclone remote.</p>"},{"location":"howto/copying/#creating-a-remote-for-lakefs-in-rclone","title":"Creating a remote for lakeFS in Rclone","text":"<p>To add the remote to Rclone, choose one of the following options:</p>"},{"location":"howto/copying/#option-1-add-an-entry-in-your-rclone-configuration-file","title":"Option 1: Add an entry in your Rclone configuration file","text":"<ul> <li> <p>Find the path to your Rclone configuration file and copy it for the next step.</p> <pre><code>rclone config file\n# output:\n# Configuration file is stored at:\n# /home/myuser/.config/rclone/rclone.conf\n</code></pre> </li> <li> <p>If your lakeFS access key is already set in an AWS profile or environment variables, run the following command, replacing the endpoint property with your lakeFS endpoint:</p> <pre><code>cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf\n[lakefs]\ntype = s3\nprovider = Other\nendpoint = https://lakefs.example.com\nno_check_bucket = true\nEOT\n</code></pre> </li> <li> <p>Otherwise, also include your lakeFS access key pair in the Rclone configuration file:</p> <pre><code>cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf\n[lakefs]\ntype = s3\nprovider = Other\nenv_auth = false\naccess_key_id = AKIAIOSFODNN7EXAMPLE\nsecret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nendpoint = https://lakefs.example.com\nno_check_bucket = true\nEOT\n</code></pre> </li> </ul>"},{"location":"howto/copying/#option-2-use-the-rclone-interactive-config-command","title":"Option 2: Use the Rclone interactive config command","text":"<p>Run this command and follow the instructions:</p> <pre><code>rclone config\n\n\nChoose AWS S3 as your type of storage, and enter your lakeFS endpoint as your S3 endpoint.\nYou will have to choose whether you use your environment for authentication (recommended),\nor enter the lakeFS access key pair into the Rclone configuration. Select \"Edit advanced\nconfig\" and accept defaults for all values except `no_check_bucket`:\n\nIf set, don't attempt to check the bucket exists or create it.\n\nThis can be useful when trying to minimize the number of transactions\nRclone carries out, if you know the bucket exists already.\n\nThis might also be needed if the user you're using doesn't have bucket\ncreation permissions. Before v1.52.0, this would have passed silently\ndue to a bug.\n\nEnter a boolean value (true or false). Press Enter for the default (\"false\").\nno_check_bucket&gt; yes\n</code></pre>"},{"location":"howto/copying/#syncing-s3-and-lakefs","title":"Syncing S3 and lakeFS","text":"<pre><code>rclone sync mys3remote://mybucket/path/ lakefs:example-repo/main/path\n</code></pre>"},{"location":"howto/copying/#syncing-a-local-directory-and-lakefs","title":"Syncing a local directory and lakeFS","text":"<pre><code>rclone sync /home/myuser/path/ lakefs:example-repo/main/path\n</code></pre>"},{"location":"howto/export/","title":"Exporting Data","text":"<p>The export operation copies all data from a given lakeFS commit to a designated object store location.</p> <p>For instance, the contents <code>lakefs://example/main</code> might be exported on <code>s3://company-bucket/example/latest</code>. Clients entirely unaware of lakeFS could use that base URL to access latest files on <code>main</code>. Clients aware of lakeFS can continue to use the lakeFS S3 endpoint to access repository files on <code>s3://example/main</code>, as well as other versions and uncommitted versions.</p> <p>Possible use-cases:</p> <ol> <li>External consumers of data don't have access to your lakeFS installation.</li> <li>Some data pipelines in the organization are not fully migrated to lakeFS.</li> <li>You want to experiment with lakeFS as a side-by-side installation first.</li> <li>Create copies of your data lake in other regions (taking into account read pricing).</li> </ol>"},{"location":"howto/export/#exporting-data-with-spark","title":"Exporting Data With Spark","text":""},{"location":"howto/export/#using-spark-submit","title":"Using spark-submit","text":"<p>You can use the export main in three different modes:</p> <ol> <li> <p>Export all the objects from branch <code>example-branch</code> on <code>example-repo</code> repository to S3 location <code>s3://example-bucket/prefix/</code>:</p> <pre><code>.... example-repo s3://example-bucket/prefix/ --branch=example-branch\n</code></pre> </li> <li> <p>Export all the objects from a commit <code>c805e49bafb841a0875f49cd555b397340bbd9b8</code> on <code>example-repo</code> repository to S3 location <code>s3://example-bucket/prefix/</code>:</p> <pre><code>.... example-repo s3://example-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8\n</code></pre> </li> <li> <p>Export only the diff between branch <code>example-branch</code> and commit <code>c805e49bafb841a0875f49cd555b397340bbd9b8</code>    on <code>example-repo</code> repository to S3 location <code>s3://example-bucket/prefix/</code>:</p> <pre><code>.... example-repo s3://example-bucket/prefix/ --branch=example-branch --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8\n</code></pre> </li> </ol> <p>The complete <code>spark-submit</code> command would look as follows:</p> <pre><code>spark-submit --conf spark.hadoop.lakefs.api.url=https://&lt;LAKEFS_ENDPOINT&gt;/api/v1 \\\n    --conf spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY_ID&gt; \\\n    --conf spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_ACCESS_KEY&gt; \\\n    --packages io.lakefs:lakefs-spark-client_2.12:0.17.0 \\\n    --class io.treeverse.clients.Main export-app example-repo s3://example-bucket/prefix \\\n    --branch=example-branch\n</code></pre> <p>Info</p> <p>The command assumes that the Spark cluster has permissions to write to <code>s3://example-bucket/prefix</code>. Otherwise, add <code>spark.hadoop.fs.s3a.access.key</code> and <code>spark.hadoop.fs.s3a.secret.key</code> with the proper credentials.</p>"},{"location":"howto/export/#networking","title":"Networking","text":"<p>Spark export communicates with the lakeFS server.  Very large repositories may require increasing a read timeout.  If you run into timeout errors during communication from the Spark job to lakeFS consider increasing these timeouts:</p> <ul> <li>Add <code>-c spark.hadoop.lakefs.api.read.timeout_seconds=TIMEOUT_IN_SECONDS</code>   (default 10) to allow lakeFS more time to respond to requests.</li> <li>Add <code>-c   spark.hadoop.lakefs.api.connection.timeout_seconds=TIMEOUT_IN_SECONDS</code>   (default 10) to wait longer for lakeFS to accept connections.</li> </ul>"},{"location":"howto/export/#using-custom-code-notebookspark","title":"Using custom code (Notebook/Spark)","text":"<p>Set up lakeFS Spark metadata client with the endpoint and credentials as instructed in the previous page.</p> <p>The client exposes the <code>Exporter</code> object with three export options:</p> <p>Export all the objects at the HEAD of a given branch. Does not include files that were added to that branch but were not committed.</p> Scala <pre><code>exportAllFromBranch(branch: String)\n</code></pre> <p>Export ALL objects from a commit:</p> Scala <pre><code>exportAllFromCommit(commitID: String)\n</code></pre> <p>Export just the diff between a commit and the HEAD of a branch.</p> <p>This is an ideal option for continuous exports of a branch since it will change only the files that have been changed since the previous commit.</p> Scala <pre><code>exportFrom(branch: String, prevCommitID: String)\n</code></pre>"},{"location":"howto/export/#successfailure-indications","title":"Success/Failure Indications","text":"<p>When the Spark export operation ends, an additional status file will be added to the root object storage destination. If all files were exported successfully, the file path will be of the form: <code>EXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_SUCCESS</code>. For failures: the form will be<code>EXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_FAILURE</code>, and the file will include a log of the failed files operations.</p>"},{"location":"howto/export/#export-rounds-spark-success-files","title":"Export Rounds (Spark success files)","text":"<p>Some files should be exported before others, e.g., a Spark <code>_SUCCESS</code> file exported before other files under the same prefix might send the wrong indication.</p> <p>The export operation may contain several rounds within the same export. A failing round will stop the export of all the files of the next rounds.</p> <p>By default, lakeFS will use the <code>SparkFilter</code> and have 2 rounds for each export. The first round will export any non-Spark <code>_SUCCESS</code> files. Second round will export all Spark's <code>_SUCCESS</code> files. You may override the default behavior by passing a custom <code>filter</code> to the Exporter.</p>"},{"location":"howto/export/#example","title":"Example","text":"<p>First configure the <code>Exporter</code> instance:</p> Scala <pre><code>import io.treeverse.clients.{ApiClient, Exporter}\nimport org.apache.spark.sql.SparkSession\n\nval endpoint = \"http://&lt;LAKEFS_ENDPOINT&gt;/api/v1\"\nval accessKey = \"&lt;LAKEFS_ACCESS_KEY_ID&gt;\"\nval secretKey = \"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\"\n\nval repo = \"example-repo\"\n\nval spark = SparkSession.builder().appName(\"I can export\").master(\"local\").getOrCreate()\nval sc = spark.sparkContext\nsc.hadoopConfiguration.set(\"lakefs.api.url\", endpoint)\nsc.hadoopConfiguration.set(\"lakefs.api.access_key\", accessKey)\nsc.hadoopConfiguration.set(\"lakefs.api.secret_key\", secretKey)\n\n// Add any required spark context configuration for s3\nval rootLocation = \"s3://company-bucket/example/latest\"\n\nval apiClient = new ApiClient(endpoint, accessKey, secretKey)\nval exporter = new Exporter(spark, apiClient, repo, rootLocation)\n</code></pre> <p>Now you can export all objects from <code>main</code> branch to <code>s3://company-bucket/example/latest</code>:</p> Scala <pre><code>val branch = \"main\"\nexporter.exportAllFromBranch(branch)\n</code></pre> <p>Assuming a previous successful export on commit <code>f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7</code>, you can alternatively export just the difference between <code>main</code> branch and the commit:</p> Scala <pre><code>val branch = \"main\"\nval commit = \"f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7\"\nexporter.exportFrom(branch, commit)\n</code></pre>"},{"location":"howto/export/#exporting-data-with-docker","title":"Exporting Data with Docker","text":"<p>This option is recommended if you don't have Spark at your tool-set. It doesn't support distribution across machines, therefore may have a lower performance. Using this method, you can export data from lakeFS to S3 using the export options (in a similar way to the Spark export):</p> <ol> <li>Export all objects from a branch <code>example-branch</code> on <code>example-repo</code> repository to S3 location <code>s3://destination-bucket/prefix/</code>:     <pre><code>.... example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\"\n</code></pre></li> <li>Export all objects from a commit <code>c805e49bafb841a0875f49cd555b397340bbd9b8</code> on <code>example-repo</code> repository to S3 location <code>s3://destination-bucket/prefix/</code>:     <pre><code>.... example-repo s3://destination-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8\n</code></pre></li> <li>Export only the diff between branch <code>example-branch</code> and commit <code>c805e49bafb841a0875f49cd555b397340bbd9b8</code>    on <code>example-repo</code> repository to S3 location <code>s3://destination-bucket/prefix/</code>:     <pre><code>.... example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\" --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8\n</code></pre></li> </ol> <p>You will need to add the relevant environment variables. The complete <code>docker run</code> command would look like:</p> <pre><code>docker run \\\n    -e LAKEFS_ACCESS_KEY_ID=XXX -e LAKEFS_SECRET_ACCESS_KEY=YYY \\\n    -e LAKEFS_ENDPOINT=https://&lt;LAKEFS_ENDPOINT&gt;/ \\\n    -e AWS_ACCESS_KEY_ID=XXX -e AWS_SECRET_ACCESS_KEY=YYY \\\n    treeverse/lakefs-rclone-export:latest \\\n        example-repo \\\n        s3://destination-bucket/prefix/ \\\n        --branch=\"example-branch\"\n</code></pre> <p>Note</p> <p>This feature uses rclone, and specifically rclone sync. This can change the destination path, therefore the s3 destination location must be designated to lakeFS export.</p>"},{"location":"howto/import/","title":"Import Data","text":"<p>Tip</p> <p>This section describes how to import existing data into a lakeFS repository, without copying it. If you are interested in copying data into lakeFS, see Copying data to/from lakeFS._</p>"},{"location":"howto/import/#importing-data-into-lakefs","title":"Importing data into lakeFS","text":""},{"location":"howto/import/#prerequisites","title":"Prerequisites","text":"<ul> <li>Importing is permitted for users in the Supers (open-source) group or the SuperUsers (Cloud/Enterprise) group.    To learn how lakeFS Cloud and lakeFS Enterprise users can fine-tune import permissions, see Fine-grained permissions below.</li> <li>The lakeFS server must have permissions to list the objects in the source bucket.</li> <li>The source bucket must be on the same cloud provider and in the same region as your repository.</li> </ul>"},{"location":"howto/import/#using-the-lakefs-ui","title":"Using the lakeFS UI","text":"<ol> <li>In your repository's main page, click the Import button to open the import dialog.</li> <li>Under Import from, fill in the location on your object store you would like to import from.</li> <li>Fill in the import destination in lakeFS. This should be a path under the current branch.</li> <li>Add a commit message, and optionally commit metadata.</li> <li>Press Import.</li> </ol> <p>Once the import is complete, a new commit containing the imported objects will be created in the destination branch.</p> <p></p>"},{"location":"howto/import/#using-the-cli-lakectl-import","title":"Using the CLI: lakectl import","text":"<p>The lakectl import command acts the same as the UI import wizard. It commits the changes to the selected branch.</p> AWS S3 or S3 API Compatible storageAzure BlobGoogle Cloud Storage <pre><code>lakectl import \\\n--from s3://bucket/optional/prefix/ \\\n--to lakefs://my-repo/my-branch/optional/path/\n</code></pre> <pre><code>lakectl import \\\n--from https://storageAccountName.blob.core.windows.net/container/optional/prefix/ \\\n--to lakefs://my-repo/my-branch/optional/path/\n</code></pre> <pre><code>lakectl import \\\n--from gs://bucket/optional/prefix/ \\\n--to lakefs://my-repo/my-branch/optional/path/\n</code></pre> <p>Note</p> <ol> <li>Any previously existing objects under the destination prefix will be deleted.</li> <li>The import duration depends on the amount of imported objects, but will roughly be a few thousand objects per second.</li> <li>For security reasons, if you are using lakeFS on top of your local disk (<code>blockstore.type=local</code>), you need to enable the import feature explicitly. To do so, set the <code>blockstore.local.import_enabled</code> to <code>true</code> and specify the allowed import paths in <code>blockstore.local.allowed_external_prefixes</code> (see configuration reference). When using lakectl or the lakeFS UI, you can currently import only directories locally. If you need to import a single file, use the HTTP API or API Clients with <code>type=object</code> in the request body and <code>destination=&lt;full-path-to-file&gt;</code>.</li> <li>Making changes to data in the original bucket will not be reflected in lakeFS, and may cause inconsistencies.</li> </ol>"},{"location":"howto/import/#examples","title":"Examples","text":"<p>To explore practical examples and real-world use cases of importing data into lakeFS, we recommend checking out our comprehensive blog post on the subject.</p>"},{"location":"howto/import/#fine-grained-permissions","title":"Fine-grained permissions","text":"<p>Info</p> <p>Available on lakeFS Cloud and lakeFS Enterprise</p> <p>With RBAC support, The lakeFS user running the import command should have the following permissions in lakeFS: <code>fs:WriteObject</code>, <code>fs:CreateCommit</code>, <code>fs:ImportFromStorage</code> and <code>fs:ImportCancel</code>.</p> <p>As mentioned above, all of these permissions are available by default to the Supers (open-source) group or the SuperUsers (Cloud/Enterprise).</p>"},{"location":"howto/import/#provider-specific-permissions","title":"Provider-specific permissions","text":"<p>In addition, the following for provider-specific permissions may be required:</p> AWS S3 or S3 API Compatible storageAzureGoogle Cloud Storage <p>lakeFS needs access to the imported location to first list the files to import and later read the files upon users request.</p> <p>There are some use cases where the user would like to import from a destination which isn't owned by the account running lakeFS. For example, importing public datasets to experiment with lakeFS and Spark.</p> <p>lakeFS will require additional permissions to read from public buckets. For example, for S3 public buckets, the following policy needs to be attached to the lakeFS S3 service-account to allow access to public buckets, while blocking access to other owned buckets:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PubliclyAccessibleBuckets\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetBucketVersioning\",\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucketMultipartUploads\",\n                \"s3:ListBucketVersions\",\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:ListMultipartUploadParts\"\n            ],\n            \"Resource\": [\"*\"],\n            \"Condition\": {\n                \"StringNotEquals\": {\n                    \"s3:ResourceAccount\": \"&lt;YourAccountID&gt;\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>Note</p> <p>The use of the <code>adls</code> hint for ADLS Gen2 storage accounts is deprecated, please use the original source url for import.</p> <p>See [Azure deployment][deploy-azure-storage-account-creds] on limitations when using account credentials.</p> <p>No specific prerequisites</p>"},{"location":"howto/local-checkouts/","title":"Working with lakeFS Data Locally","text":"<p>lakeFS is a scalable data version control system designed to scale to billions of objects. The larger the data, the less feasible it becomes to consume it from a single machine. lakeFS addresses this challenge by enabling efficient management of large-scale data stored remotely.</p> <p>In addition to its capability to manage large datasets, lakeFS offers the flexibility to work with versioned data by exposing it as a local filesystem directory.  </p> <p>This page explains lakeFS Mount and <code>lakectl local</code>: two common ways of exposing lakeFS data locally, with different performance characteristics.  </p>"},{"location":"howto/local-checkouts/#use-cases","title":"Use cases","text":""},{"location":"howto/local-checkouts/#local-development-of-ml-models","title":"Local development of ML models","text":"<p>The development of machine learning models is a dynamic and iterative process, including experimentation with various data versions, transformations, algorithms, and hyperparameters. To optimize this iterative workflow, experiments must be conducted with speed, ease of tracking, and reproducibility in mind. Localizing the model data during development enhances the development process. It accelerates the development process by enabling interactive and offline development and reducing data access latency.</p> <p>The local availability of data is required to seamlessly integrate data version control systems and source control systems like Git. This integration is vital for achieving model reproducibility, allowing for a more efficient and collaborative model development environment.</p>"},{"location":"howto/local-checkouts/#data-locality-for-optimized-gpu-utilization","title":"Data Locality for Optimized GPU Utilization","text":"<p>Training Deep Learning models requires expensive GPUs. In the context of running such programs, the goal is to optimize GPU usage and prevent them from sitting idle. Many deep learning tasks involve accessing images, and in some cases, the same images are accessed multiple times. Localizing the data can eliminate redundant round trip times to access remote storage, resulting in cost savings.</p>"},{"location":"howto/local-checkouts/#lakefs-mount-efficiently-expose-lakefs-data-as-a-local-directory","title":"lakeFS Mount: Efficiently expose lakeFS Data as a local directory","text":"<p>Info</p> <p>Mount requires no installation, please contact us to get access.</p>"},{"location":"howto/local-checkouts/#prerequisites","title":"Prerequisites","text":"<ul> <li>A working lakeFS Server running either lakeFS Enterprise or lakeFS Cloud</li> <li>You\u2019ve installed the <code>lakectl</code> command line utility: this is the official lakeFS command line interface, on top of which lakeFS Mount is built.</li> <li>lakectl is configured properly to access your lakeFS server as detailed in the configuration instructions</li> </ul>"},{"location":"howto/local-checkouts/#mounting-a-lakefs-reference-as-a-local-directory","title":"Mounting a lakeFS reference as a local directory","text":"<p>lakeFS Mount works by exposing a virtual mountpoint on the host computer.</p> <p>This \"acts\" as a local directory, allowing applications to read write and interact with data as it is all local to the machine, while lakeFS Mount optimizes this behind the scenes by lazily fetching data as requested, caching accessed objects and efficiently managing metadata to ensure best in class performance. Read more about how lakeFS Mount optimizes performance</p> <p>Mounting a reference is a single command:</p> <pre><code>everest mount lakefs://example-repo/example-branch/path/to/data/ ./my_local_dir\n</code></pre> <p>Once executed, the <code>my_local_dir</code> directory should appear to have the contents of the remote path we provided. We can verify this:</p> <pre><code>ls -l ./my_local_dir/\n</code></pre> <p>Which should return the listing of the mounted path.</p> <p>Tip</p> <p>lakeFS Mount allows quite a bit of tuning to ensure optimal performance. Read more about how lakeFS Mount works and how to configure it.</p>"},{"location":"howto/local-checkouts/#reading-from-a-mount","title":"Reading from a mount","text":"<p>Reading from a lakeFS Mount requires no special tools, integrations or SDKs! Simply point your code to the directory and read from it as if it was in fact local:</p> <pre><code>#!/usr/bin/env python\nimport glob\n\nfor image_path in glob.glob('./my_local_dir/*.png'):\n    with open(image_path, 'rb') as f:\n        process(f)\n</code></pre>"},{"location":"howto/local-checkouts/#unmounting","title":"Unmounting","text":"<p>When done, simply run:</p> <pre><code>everest umount ./my_local_dir\n</code></pre> <p>This will unmount the lakeFS Mount, cleaning up background tasks</p>"},{"location":"howto/local-checkouts/#lakectl-local-sync-lakefs-data-with-a-local-directory","title":"lakectl local: Sync lakeFS data with a local directory","text":"<p>The local command of lakeFS' CLI lakectl enables working with lakeFS data locally by copying the data onto the host machine. It allows syncing local directories with remote lakeFS locations, and to seamlessly integrate lakeFS with Git.</p> <p>Here are the available lakectl local commands:</p> Command What it does Notes init Connects between a local directory and a lakeFS remote URI to enable data sync To undo a directory init, delete the .lakefs_ref.yaml file created in the initialized directory clone Clones lakeFS data from a path into an empty local directory and initializes the directory A directory can only track a single lakeFS remote location. i.e., you cannot clone data into an already initialized directory list Lists directories that are synced with lakeFS It is recommended to follow any init or clone command with a list command to verify its success status Shows remote and local changes to the directory and the remote location it tracks commit Commits changes from local directory to the lakeFS branch it tracks Uncommitted changes to directories connected to lakeFS remote locations will not reflect in lakeFS until after doing lakectl local commit. pull Fetches latest changes from a lakeFS remote location into a connected local directory checkout Syncs a local directory with the state of a lakeFS ref <p>Warning</p> <p>The data size you work with locally should be reasonable for smooth operation on a local machine which is typically no larger than 15 GB.  </p>"},{"location":"howto/local-checkouts/#configuration","title":"Configuration","text":"<p>The <code>lakectl local</code> commands can be configured through the lakectl configuration file to handle special cases like symbolic links. The following configuration options are available under the <code>local</code> section:</p> Configuration Option Description Default <code>skip_non_regular_files</code> By default, lakectl local fails if a local directory contains non regular files. When set to <code>true</code>, lakectl will ignore them instead of failing. <code>false</code> <code>symlink_support</code> Controls whether symlinks are supported and processed by lakectl local commands. <code>false</code> <p>Example configuration:</p> <pre><code>local:\n  symlink_support: true\n</code></pre> <p>When <code>symlink_support</code> is enabled, <code>lakectl local</code> commands will store and restore the state of the symlinks by creating special metadata objects that specify the symlink target. Metadata objects representing symlinks are created during data uploads. In contrast, during data downloads, <code>lakectl</code> will recreate the symbolic links in the local directory based on these metadata objects. This allows for a seamless integration of symlinks within your data management workflow. However, if the feature is turned off, any previously created symlinks will be treated as empty objects, meaning their targets will not be preserved or recreated during sync operations. It is crucial to enable this feature if your workflow relies on symbolic links to ensure data integrity and consistency.</p> <p>Note</p> <p>When <code>skip_non_regular_files</code> is enabled, symbolic links in your local directory will be ignored during sync operations. If you need to work with symbolic links, consider using <code>symlink_support</code> instead, though this feature may have different behavior and limitations.</p> <p>Warning</p> <p>Using symbolic links can be potentially dangerous as they can point to any file on your system. When <code>symlink_support</code> is enabled, lakeFS will not download data to a target which is a symlink, to prevent any unexpected behavior.</p>"},{"location":"howto/local-checkouts/#example-using-lakectl-local-in-tandem-with-git","title":"Example: Using lakectl local in tandem with Git","text":"<p>We are going to develop an ML model that predicts whether an image is an Alpaca or not. Our goal is to improve the input for the model. The code for the model is versioned by Git while the model dataset is versioned by lakeFS. We will be using lakectl local to tie code versions to data versions to achieve model reproducibility.  </p>"},{"location":"howto/local-checkouts/#setup","title":"Setup","text":"<p>To get start with, we have initialized a Git repo called <code>is_alpaca</code> that includes the model code: </p> <p>We also created a lakeFS repository and uploaded the is_alpaca train dataset by Kaggel into it: </p>"},{"location":"howto/local-checkouts/#create-an-isolated-environment-for-experiments","title":"Create an Isolated Environment for Experiments","text":"<p>Our goal is to improve the model predictions. To meet our goal, we will experiment with editing the training dataset. We will run our experiments in isolation to not change anything until after we are certain the data is improved and ready.</p> <p>Let's create a new lakeFS branch called <code>experiment-1</code>. Our is_alpaca dataset is accessible on that branch, and we will interact with the data from that branch only. </p> <p>On the code side, we will create a Git branch also called <code>experiment-1</code> to not pollute our main branch with a dataset which is under tuning.</p>"},{"location":"howto/local-checkouts/#clone-lakefs-data-into-a-local-git-repository","title":"Clone lakeFS Data into a Local Git Repository","text":"<p>Inspecting the <code>train.py</code> script, we can see that it expects an input on the <code>input</code> directory.</p> <pre><code>#!/usr/bin/env python\nimport tensorflow as tf\n\ninput_location = './input'\nmodel_location = './models/is_alpaca.h5'\n\ndef get_ds(subset):\n    return tf.keras.utils.image_dataset_from_directory(\n        input_location, validation_split=0.2, subset=subset,\n        seed=123, image_size=(244, 244), batch_size=32)\n\ntrain_ds = get_ds(\"training\")\nval_ds = get_ds(\"validation\")\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Rescaling(1./255),\n    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(2)])\n\n# Fit and save\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\nmodel.fit(train_ds, validation_data=val_ds, epochs=3)\nmodel.save(model_location)\n</code></pre> <p>This means that to be able to locally develop our model and experiment with it we need to have the is_alpaca dataset managed by lakeFS available locally on that path. To do that, we will use the <code>lakectl local clone</code> command from our local Git repository root:</p> <pre><code>lakectl local clone lakefs://is-alpaca/experiment-1/dataset/train/ input\n</code></pre> <p>This command will do a diff between out local input directory (that did not exist until now) and the provided lakeFS path and identify that there are files to be downloaded from lakeFS.</p> <pre><code>Successfully cloned lakefs://is-alpaca/experiment-1/dataset/train/ to ~/ml_models/is_alpaca/input\n\nClone Summary:\n\nDownloaded: 250\nUploaded: 0\nRemoved: 0\n</code></pre> <p>Running <code>lakectl local list</code> from our Git repository root will show that the <code>input</code> directory is now in sync with a lakeFS prefix (Remote URI), and what lakeFS version of the data (Synced Commit) the is it tracking:</p> <pre><code> is_alpaca % lakectl local list                 \n+-----------+------------------------------------------------+------------------------------------------------------------------+\n| DIRECTORY | REMOTE URI                                     | SYNCED COMMIT                                                    |\n+-----------+------------------------------------------------+------------------------------------------------------------------+\n| input     | lakefs://is-alpaca/experiment-1/dataset/train/ | 589f87704418c6bac80c5a6fc1b52c245af347b9ad1ea8d06597e4437fae4ca3 |\n+-----------+------------------------------------------------+------------------------------------------------------------------+\n</code></pre>"},{"location":"howto/local-checkouts/#tie-code-version-and-data-version","title":"Tie Code Version and Data Version","text":"<p>Now let's tell Git to stage the dataset we've added and inspect our Git branch status:</p> <pre><code>is_alpaca % git add input/\nis_alpaca % git status \nOn branch experiment-1\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n new file:   input/.lakefs_ref.yaml\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n modified:   .gitignore\n</code></pre> <p>We can see that the <code>.gitignore</code> file changed, and that the files we cloned from lakeFS into the <code>input</code> directory are not tracked by git. This is intentional - remember that lakeFS is the one managing the data. But wait, what is this special <code>input/.lakefs_ref.yaml</code> file that Git does track?  </p> <pre><code>is_alpaca % cat input/.lakefs_ref.yaml\n\nsrc: lakefs://is-alpaca/experiment-1/dataset/train/s\nat_head: 589f87704418c6bac80c5a6fc1b52c245af347b9ad1ea8d06597e4437fae4ca3\n</code></pre> <p>This file includes the lakeFS version of the data that the Git repository is currently pointing to.</p> <p>Let's commit the changes to Git with:</p> <pre><code>git commit -m \"added is_alpaca dataset\" \n</code></pre> <p>By committing to Git, we tie the current code version of the model to the dataset version in lakeFS as it appears in <code>input/.lakefs_ref.yaml</code>.</p>"},{"location":"howto/local-checkouts/#experiment-and-version-results","title":"Experiment and Version Results","text":"<p>We ran the train script on the cloned input, and it generated a model. Now, let's use the model to predict whether an axolotl is an alpaca.</p> <p>A reminder - this is how an axolotl looks like - not like an alpaca!</p> <p></p> <p>Here are the (surprising) results:</p> <pre><code>is_alpaca % ./predict.py ~/axolotl1.jpeg\n{'alpaca': 0.32112, 'not alpaca': 0.07260383}\n</code></pre> <p>We expected the model to provide a more concise prediction, so let's try to improve it. To do that, we will add additional images of axolotls to the model input directory:</p> <pre><code>is_alpaca % cp ~/axolotls_images/* input/not_alpaca\n</code></pre> <p>To inspect what changes we made to out dataset we will use lakectl local status.</p> <pre><code>is_alpaca % lakectl local status input \ndiff 'local:///ml_models/is_alpaca/input' &lt;--&gt; 'lakefs://is-alpaca/589f87704418c6bac80c5a6fc1b52c245af347b9ad1ea8d06597e4437fae4ca3/dataset/train/'...\ndiff 'lakefs://is-alpaca/589f87704418c6bac80c5a6fc1b52c245af347b9ad1ea8d06597e4437fae4ca3/dataset/train/' &lt;--&gt; 'lakefs://is-alpaca/experiment-1/dataset/train/'...\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 SOURCE \u2551 CHANGE \u2551 PATH                       \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 local  \u2551 added  \u2551 not_alpaca/axolotl2.jpeg   \u2551\n\u2551 local  \u2551 added  \u2551 not_alpaca/axolotl3.png    \u2551\n\u2551 local  \u2551 added  \u2551 not_alpaca/axolotl4.jpeg   \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre> <p>At this point, the dataset changes are not yet tracked by lakeFS. We will validate that by looking at the uncommitted changes area of our experiment branch and verifying it is empty.</p> <p>To commit these changes to lakeFS we will use lakectl local commit:</p> <pre><code>is_alpaca % lakectl local commit input -m \"add images of axolotls to the training dataset\"\n\nGetting branch: experiment-1\n\ndiff 'local:///ml_models/is_alpaca/input' &lt;--&gt; 'lakefs://is-alpaca/589f87704418c6bac80c5a6fc1b52c245af347b9ad1ea8d06597e4437fae4ca3/dataset/train/'...\nupload not_alpaca/axolotl3.png              ... done! [5.04KB in 679ms]\nupload not_alpaca/axolotl2.jpeg             ... done! [38.31KB in 685ms]\nupload not_alpaca/axolotl4.jpeg             ... done! [7.70KB in 718ms]\n\nSync Summary:\n\nDownloaded: 0\nUploaded: 3\nRemoved: 0\n\nFinished syncing changes. Perform commit on branch...\nCommit for branch \"experiment-1\" completed.\n\nID: 0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6\nMessage: add images of axolotls to the training dataset\nTimestamp: 2024-02-08 17:41:20 +0200 IST\nParents: 589f87704418c6bac80c5a6fc1b52c245af347b9ad1ea8d06597e4437fae4ca3\n</code></pre> <p>Looking at the lakeFS UI we can see that the lakeFS commit includes metadata that tells us what was the code version of the linked Git repository at the time of the commit. </p> <p>Inspecting the Git repository, we can see that the input/.lakefs_ref.yaml is pointing to the latest lakeFS commit <code>0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6</code>.  </p> <p>We will now re-train our model with the modified dataset and give a try to predict whether an axolotl is an alpaca:</p> <pre><code>is_alpaca % ./predict.py ~/axolotl1.jpeg\n{'alpaca': 0.12443, 'not alpaca': 0.47260383}\n</code></pre> <p>Results are indeed more accurate.</p>"},{"location":"howto/local-checkouts/#sync-a-local-directory-with-lakefs","title":"Sync a Local Directory with lakeFS","text":"<p>Now that we think that the latest version of our model generates reliable predictions, let's validate it against a test dataset rather than against a single picture. We will use the test dataset provided by Kaggel. Let's create a local <code>testDataset</code> directory in our git repository and populate it with the test dataset.</p> <p>Now, we will use  lakectl local init to sync the <code>testDataset</code> directory with our lakeFS repository:</p> <pre><code>is_alpaca % lakectl local init lakefs://is-alpaca/main/dataset/test/ testDataset \nLocation added to /is_alpaca/.gitignore\nSuccessfully linked local directory '/is_alpaca/testDataset' with remote 'lakefs://is-alpaca/main/dataset/test/'\n</code></pre> <p>And validate that the directory was linked successfully:</p> <pre><code>is_alpaca % lakectl local list                                                           \n+-------------+-------------------------------------------------+------------------------------------------------------------------+\n| DIRECTORY   | REMOTE URI                                      | SYNCED COMMIT                                                    |\n+-------------+-------------------------------------------------+------------------------------------------------------------------+\n| input       | lakefs://is-alpaca/main/dataset/train/          | 0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6 |\n| testDataset | lakefs://is-alpaca/main/dataset/test/           | 0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6 |\n+-------------+-------------------------------------------------+------------------------------------------------------------------+\n</code></pre> <p>Now we will tell Git to track the <code>testDataset</code> directory with <code>git add testDataset</code>, and as we saw earlier Git will only track the <code>testDataset/.lakefs_ref.yaml</code> for that directory rather than its content.  </p> <p>To see the difference between our local <code>testDataset</code> directory and its lakeFS location <code>lakefs://is-alpaca/main/dataset/test/</code> we will use lakectl local status:</p> <pre><code>is_alpaca % lakectl local status testDataset \n\ndiff 'local:///ml_models/is_alpaca/testDataset' &lt;--&gt; 'lakefs://is-alpaca/0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6/dataset/test/'...\ndiff 'lakefs://is-alpaca/0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6/dataset/test/' &lt;--&gt; 'lakefs://is-alpaca/main/dataset/test/'...\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 SOURCE \u2551 CHANGE \u2551 PATH                           \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 local  \u2551 added  \u2551 alpaca/alpaca (1).jpg          \u2551\n\u2551 local  \u2551 added  \u2551 alpaca/alpaca (10).jpg         \u2551\n\u2551    .         .                  .                \u2551 \n\u2551    .         .                  .                \u2551\n\u2551    .         .                  .                \u2551\n\u2551 local  \u2551 added  \u2551 not_alpaca/not_alpaca (9).jpg  \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre> <p>We can see that multiple files were locally added to the synced directory.  </p> <p>To apply these changes to lakeFS we will commit them:</p> <pre><code>is_alpaca % lakectl local commit testDataset -m \"add is_alpaca test dataset to lakeFS\" \n\nGetting branch: experiment-1\n\ndiff 'local:///ml_models/is_alpaca/testDataset' &lt;--&gt; 'lakefs://is-alpaca/0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6/dataset/test/'...\nupload alpaca/alpaca (23).jpg            ... done! [113.81KB in 1.241s]\nupload alpaca/alpaca (26).jpg            ... done! [102.74KB in 1.4s]\n          .                                             .\n          .                                             .\nupload not_alpaca/not_alpaca (42).jpg    ... done! [886.93KB in 14.336s]\n\nSync Summary:\n\nDownloaded: 0\nUploaded: 77\nRemoved: 0\n\nFinished syncing changes. Perform commit on branch...\nCommit for branch \"experiment-1\" completed.\n\nID: c8be7f4f5c13dd2e489ae85e6f747230bfde8e50f9cd9b6af20b2baebfb576cf\nMessage: add is_alpaca test dataset to lakeFS\nTimestamp: 2024-02-10 12:31:53 +0200 IST\nParents: 0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6\n</code></pre> <p>Looking at the lakFS UI we see that our test data is now available at lakeFS: </p> <p>Finally, we will Git commit the local changes to link between the Git and lakeFS repositories state.</p> <p>Tip</p> <p>While syncing a local directory with a lakeF prefix, it is recommended to first commit the data to lakeFS and then do a Git commit that will include the changes done to the <code>.lakefs_ref.yaml</code> for the synced directory. Reasoning is that only after committing the data to lakeFS, the <code>.lakefs_ref.yaml</code> file points to a lakeFS commit that includes the added content from the directory.</p>"},{"location":"howto/local-checkouts/#reproduce-model-results","title":"Reproduce Model Results","text":"<p>What if we wanted to re-run the model that predicted that an axolotl is more likely to be an alpaca? This question translates into the question: \"How do I roll back my code and data to the time before we optimized the train dataset?\" Which translates to: \"What was the Git commit ID at this point?\"</p> <p>Searching our Git log we find this commit:</p> <pre><code>commit 5403ec29903942b692aabef404598b8dd3577f8a\n\n    added is_alpaca dataset\n</code></pre> <p>So, all we have to do now is <code>git checkout 5403ec29903942b692aabef404598b8dd3577f8a</code> and we are good to reproduce the model results!</p> <p>Checkout our article about ML Data Version Control and Reproducibility at Scale to get another example for how lakeFS and Git work seamlessly together.</p>"},{"location":"howto/migrate-away/","title":"Migrating away from lakeFS","text":""},{"location":"howto/migrate-away/#copying-data-from-a-lakefs-repository-to-an-s3-bucket","title":"Copying data from a lakeFS repository to an S3 bucket","text":"<p>The simplest way to migrate away from lakeFS is by copying data from a lakeFS repository to an S3 bucket (or any other object store).</p> <p>For smaller repositories, you can do this by using the AWS CLI or Rclone. For larger repositories, running distcp with lakeFS as the source is also an option.</p>"},{"location":"howto/mirroring/","title":"Transactional Mirroring","text":"<p>Info</p> <p>Transactional Mirroring is only available for lakeFS Cloud.</p>"},{"location":"howto/mirroring/#what-is-lakefs-transactional-mirroring","title":"What is lakeFS Transactional Mirroring?","text":"<p>Transactional Mirroring in lakeFS allows replicating a lakeFS repository (\"source\") into read-only copies (\"mirror\") in different locations.</p> <p>Unlike conventional mirroring, data isn't simply copied between regions - lakeFS Cloud tracks the state of each commit, advancing the commit log on the mirror only once a commit has been fully replicated and all data is available.</p> <p></p>"},{"location":"howto/mirroring/#uses-cases","title":"Uses cases","text":""},{"location":"howto/mirroring/#disaster-recovery","title":"Disaster recovery","text":"<p>Typically, object stores provide a replication/batch copy API to allow for disaster recovery: as new objects are written, they are asynchronously copied to other geo locations. </p> <p>In the case of regional failure, users can rely on the other geo-locations which should contain relatively-up-to-date state.</p> <p>The problem is reasoning about what managed to arrive by the time of disaster and what hasn't:</p> <ul> <li>have all the necessary files for a given dataset arrived?</li> <li>in cases there are dependencies between datasets, are all dependencies also up to date?</li> <li>what is currently in-flight or haven't even started replicating yet?</li> </ul> <p>Reasoning about these is non-trivial, especially in the face of a regional disaster, however ensuring business continuity might require that we have these answers.</p> <p>Using lakeFS Transactional Mirroring makes it much easier to answer: we are guaranteed that the latest commit that exists in the replica is in a consistent state and is fully usable, even if it isn't the absolute latest commit - it still reflects a known, consistent, point in time.</p>"},{"location":"howto/mirroring/#data-locality","title":"Data Locality","text":"<p>For certain workloads, it might be cheaper to have data available in multiple regions: Expensive hardware such as GPUs might fluctuate in price, so we'd want to pick the region that currently offers the best pricing. The difference could easily offset to cost of the replicated data.</p> <p>The challenge is reproducibility - Say we have an ML training job that reads image files from a path in the object store. Which files existed at the time of training?</p> <p>If data is constantly flowing between regions, this might be harder to answer than we think. And even if we know - how can we recreate that exact state if we want to run the process again (for example, to rebuild that model for troubleshooting).</p> <p>Using consistent commits solves this problem - with lakeFS Transactional Mirroring, it is guaranteed that a commit ID, regardless of location, will always contain the exact same data.</p> <p>We can train our model in region A, and a month later feed the same commit ID into another region - and get back the same results.</p>"},{"location":"howto/mirroring/#setting-up-transactional-mirroring","title":"Setting up Transactional Mirroring","text":""},{"location":"howto/mirroring/#configuring-bucket-replication-on-s3","title":"Configuring bucket replication on S3","text":"<p>The objects within the repository are copied using your cloud provider's object store replication mechanism. For AWS S3, please refer to the AWS S3 replication documentation to make sure your lakeFS repository's storage namespace (source) is replicated to the region you'd like your mirror to be located on (target).</p> <p>After setting the replication rule, new objects will be replicated to the destination bucket. </p> <p>In order to replicate the existing objects, we'd need to manually copy them - however, we can use S3 batch jobs to do this.</p>"},{"location":"howto/mirroring/#creating-a-lakefs-user-with-a-replicator-policy","title":"Creating a lakeFS user with a \"replicator\" policy","text":"<p>On our source lakeFS installation, under Administration create a new user that will be used by the replication subsystem. The user should have the following RBAC policy attached:</p> <pre><code>{\n   \"id\": \"ReplicationPolicy\",\n   \"statement\": [\n      {\n         \"action\": [\n            \"fs:ReadRepository\",\n            \"fs:CreateRepository\",\n            \"fs:UpdateRepository\",\n            \"fs:DeleteRepository\",\n            \"fs:ListRepositories\",\n            \"fs:AttachStorageNamespace\",\n            \"fs:ReadObject\",\n            \"fs:WriteObject\",\n            \"fs:DeleteObject\",\n            \"fs:ListObjects\",\n            \"fs:CreateCommit\",\n            \"fs:ReadCommit\",\n            \"fs:ListCommits\",\n            \"fs:CreateBranch\",\n            \"fs:DeleteBranch\",\n            \"fs:RevertBranch\",\n            \"fs:ReadBranch\",\n            \"fs:ListBranches\"\n         ],\n         \"effect\": \"allow\",\n         \"resource\": \"*\"\n      }\n   ]\n}\n</code></pre> <p>Alternatively, we can create a policy with a narrower scope, only for a specific repository and/or mirror:</p> <pre><code>{\n   \"id\": \"ReplicationPolicy\",\n   \"statement\": [\n      {\n         \"action\": [\n            \"fs:ListRepositories\"\n         ],\n         \"effect\": \"allow\",\n         \"resource\": \"*\"\n      },\n      {\n         \"action\": [\n            \"fs:ReadRepository\",\n            \"fs:ReadObject\",\n            \"fs:ListObjects\",\n            \"fs:ReadCommit\",\n            \"fs:ListCommits\",\n            \"fs:ReadBranch\",\n            \"fs:ListBranches\"\n         ],\n         \"effect\": \"allow\",\n         \"resource\": \"arn:lakefs:fs:::repository/{sourceRepositoryId}\"\n      },\n      {\n         \"action\": [\n            \"fs:ReadRepository\",\n            \"fs:CreateRepository\",\n            \"fs:UpdateRepository\",\n            \"fs:DeleteRepository\",\n            \"fs:AttachStorageNamespace\",\n            \"fs:ReadObject\",\n            \"fs:WriteObject\",\n            \"fs:DeleteObject\",\n            \"fs:ListObjects\",\n            \"fs:CreateCommit\",\n            \"fs:ReadCommit\",\n            \"fs:ListCommits\",\n            \"fs:CreateBranch\",\n            \"fs:DeleteBranch\",\n            \"fs:RevertBranch\",\n            \"fs:ReadBranch\",\n            \"fs:ListBranches\"\n         ],\n         \"effect\": \"allow\",\n         \"resource\": \"arn:lakefs:fs:::repository/{mirrorId}\"\n      },\n      {\n         \"action\": [\n            \"fs:AttachStorageNamespace\"\n         ],\n         \"effect\": \"allow\",\n         \"resource\": \"arn:lakefs:fs:::namespace/{DestinationStorageNamespace}\"\n      }\n   ]\n}\n</code></pre> <p>Once a user has been created and the replication policy attached to it, create an access key and secret to be used by the mirroring process.</p>"},{"location":"howto/mirroring/#authorizing-the-lakefs-mirror-process-to-use-the-replication-user","title":"Authorizing the lakeFS Mirror process to use the replication user","text":"<p>Please contact Treeverse customer success to connect the newly created user with the mirroring process  </p>"},{"location":"howto/mirroring/#configuring-repository-replication","title":"Configuring repository replication","text":"<p>Replication has a stand-alone HTTP API. In this example, we'll use cURL, but feel free to use any HTTP client or library:</p> <p><pre><code>curl --location 'https://&lt;ORGANIZATION_ID&gt;.&lt;SOURCE_REGION&gt;.lakefscloud.io/service/replication/v1/repositories/&lt;SOURCE_REPO&gt;/mirrors' \\\n--header 'Content-Type: application/json' \\\n-u &lt;ACCESS_KEY_ID&gt;:&lt;SECRET_ACCESS_KEY&gt; \\\n-X POST \\\n--data '{\n    \"name\": \"&lt;MIRROR_NAME&gt;\",\n    \"region\": \"&lt;MIRROR_REGION&gt;\",\n    \"storage_namespace\": \"&lt;MIRROR_STORAGE_NAMESPACE&gt;\"\n}'\n</code></pre> Using the following parameters:</p> <ul> <li><code>ORGANIZATION_ID</code> - The ID as it appears in the URL of your lakeFS installation (e.g. <code>https://my-org.us-east-1.lakefscloud.io/</code>)</li> <li><code>SOURCE_REGION</code> - The region where our source repository is hosted</li> <li><code>SOURCE_REPO</code> - Name of the repository acting as our replication source. It should exist</li> <li><code>ACCESS_KEY_ID</code> &amp; <code>SECRET_ACCESS_KEY</code> - Credentials for your lakeFS user (make sure you have the necessary RBAC permissions as listed below)</li> <li><code>MIRROR_NAME</code> - Name used for the read-only mirror to be created on the destination region</li> <li><code>MIRROR_STORAGE_NAMESPACE</code> - Location acting as the replication target for the storage namespace of our source repository</li> </ul>"},{"location":"howto/mirroring/#transactional-mirroring-and-garbage-collection","title":"Transactional Mirroring and Garbage Collection","text":"<p>Garbage collection won't run on mirrored repositories.  Deletions from garbage collection should be replicated from the source:</p> <ol> <li>Enable DELETED marker replication on the source bucket.</li> <li>Create a lifecycle policy on the destination bucket to delete the objects with the DELETED marker.</li> </ol>"},{"location":"howto/mirroring/#rbac","title":"RBAC","text":"<p>These are the required RBAC permissions for working with the new cross-region replication feature:</p> <p>Creating a Mirror:</p> Action ARN <code>fs:CreateRepository</code> <code>arn:lakefs:fs:::repository/{mirrorId}</code> <code>fs:MirrorRepository</code> <code>arn:lakefs:fs:::repository/{sourceRepositoryId}</code> <code>fs:AttachStorageNamespace</code> <code>arn:lakefs:fs:::namespace/{storageNamespace}</code> <p>Editing Mirrored Branches:</p> Action ARN <code>fs:MirrorRepository</code> <code>arn:lakefs:fs:::repository/{sourceRepositoryId}</code> <p>Deleting a Mirror:</p> Action ARN <code>fs:DeleteRepository</code> <code>arn:lakefs:fs:::repository/{mirrorId}</code> <p>Listing/Getting Mirrors for a Repository:</p> Action ARN <code>fs:ListRepositories</code> <code>*</code>"},{"location":"howto/mirroring/#other-replication-operations","title":"Other replication operations","text":""},{"location":"howto/mirroring/#listing-all-mirrors-for-a-repository","title":"Listing all mirrors for a repository","text":"<pre><code>curl --location 'https://&lt;ORGANIZATION_ID&gt;.&lt;SOURCE_REGION&gt;.lakefscloud.io/service/replication/v1/repositories/&lt;SOURCE_REPO&gt;/mirrors' \\\n-u &lt;ACCESS_KEY_ID&gt;:&lt;SECRET_ACCESS_KEY&gt; -s\n</code></pre>"},{"location":"howto/mirroring/#getting-a-specific-mirror","title":"Getting a specific mirror","text":"<pre><code>url --location 'https://&lt;ORGANIZATION_ID&gt;.&lt;SOURCE_REGION&gt;.lakefscloud.io/service/replication/v1/repositories/&lt;SOURCE_REPO&gt;/mirrors/&lt;MIRROR_ID&gt;' \\\n-u &lt;ACCESS_KEY_ID&gt;:&lt;SECRET_ACCESS_KEY&gt; -s\n</code></pre>"},{"location":"howto/mirroring/#deleting-a-specific-mirror","title":"Deleting a specific mirror","text":"<pre><code>curl --location --request DELETE 'https://&lt;ORGANIZATION_ID&gt;.&lt;SOURCE_REGION&gt;.lakefscloud.io/service/replication/v1/repositories/&lt;SOURCE_REPO&gt;/mirrors/&lt;MIRROR_ID&gt;' \\\n-u &lt;ACCESS_KEY_ID&gt;:&lt;SECRET_ACCESS_KEY&gt;\n</code></pre>"},{"location":"howto/mirroring/#limitations","title":"Limitations","text":"<ol> <li>Transactional Mirroring is currently only supported on AWS S3 and lakeFS Cloud for AWS</li> <li>Read-only mirrors cannot be written to. Transactional Mirroring is one-way, from source to destination(s)</li> <li>Currently, only branches are mirrored. Tags and arbitrary commits that do not belong to any branch are not replicated</li> <li>lakeFS Hooks will only run on the source repository, not its replicas</li> <li>Replication is still asynchronous: reading from a branch will always return a valid commit that the source has pointed to, but it is not guaranteed to be the latest commit the source branch is pointing to.</li> </ol>"},{"location":"howto/multiple-storage-backends/","title":"Multiple Storage Backends","text":"<p>Info</p> <p>Multi-storage backend support is only available to licensed lakeFS Enterprise customers. Contact us to get started!</p>"},{"location":"howto/multiple-storage-backends/#what-is-multiple-storage-backend-support","title":"What is Multiple storage Backend Support?","text":"<p>lakeFS multiple storage backend support enables seamless data management across multiple storage systems \u2014 on-premises, across public clouds, or hybrid environments. This capability makes lakeFS a unified data management platform for all organizational data assets, which is especially critical in AI/ML environments that rely on diverse datasets stored in multiple locations.</p> <p>With a multiple-store setup, lakeFS can connect to and manage any combination of supported storage systems, including:</p> <ul> <li>AWS S3</li> <li>Azure Blob</li> <li>Google Cloud Storage</li> <li>other S3-compatible storage</li> <li>local storage</li> </ul> <p>Note</p> <p>Multi-storage backends support is available from version 1.51.0 of lakeFS Enterprise.</p>"},{"location":"howto/multiple-storage-backends/#use-cases","title":"Use Cases","text":"<ol> <li>Distributed Data Management:<ul> <li>Eliminate data silos and enable seamless cross-cloud collaboration.</li> <li>Maintain version control across different storage providers for consistency and reproducibility.</li> <li>Ideal for AI/ML environments where datasets are distributed across multiple storage locations.</li> </ul> </li> <li>Unified Data Access:<ul> <li>Access data across multiple storage backends using a single, consistent URI format.</li> </ul> </li> <li>Centralized Access Control &amp; Governance:<ul> <li>Access permissions and policies can be centrally managed across all connected storage systems using lakeFS RBAC.</li> <li>Compliance and security controls remain consistent, regardless of where the data is stored.</li> </ul> </li> </ol>"},{"location":"howto/multiple-storage-backends/#configuration","title":"Configuration","text":"<p>To configure your lakeFS server to connect to multiple storage backends, define them under the <code>blockstores</code> section in your server configurations. The <code>blockstores.stores</code> field is an array of storage backends, each with its own configuration.</p> <p>For a complete list of available options, refer to the server configuration reference.</p> <p>Note</p> <p>If you're upgrading from a single-store lakeFS setup, refer to the upgrade guidelines to ensure a smooth transition.</p>"},{"location":"howto/multiple-storage-backends/#example-configurations","title":"Example Configurations","text":"On-PremMulti-CloudHybrid <p>This example setup configures lakeFS to manage data across two separate MinIO instances:</p> <p>Example</p> <pre><code>blockstores:\n  signing:\n    secret_key: \"some-secret\"\n  stores:\n    - id: \"minio-prod\"\n      description: \"Primary on-prem MinIO storage for production data\"\n      type: \"s3\"\n      s3:\n        force_path_style: true\n        endpoint: 'http://minio-prod.local'\n        discover_bucket_region: false\n        credentials:\n          access_key_id: \"prod_access_key\"\n          secret_access_key: \"prod_secret_key\"\n    - id: \"minio-backup\"\n      description: \"Backup MinIO storage for disaster recovery\"\n      type: \"s3\"\n      s3:\n        force_path_style: true\n        endpoint: 'http://minio-backup.local'\n        discover_bucket_region: false\n        credentials:\n          access_key_id: \"backup_access_key\"\n          secret_access_key: \"backup_secret_key\"\n</code></pre> <p>This example setup configures lakeFS to manage data across two public cloud providers: AWS and Azure:</p> <p>Example</p> <pre><code>blockstores:\n  signing:\n    secret_key: \"some-secret\"\n  stores:\n    - id: \"s3-prod\"\n      description: \"AWS S3 storage for production data\"\n      type: \"s3\"\n      s3:\n        region: \"us-east-1\"\n    - id: \"azure-analytics\"\n      description: \"Azure Blob storage for analytics data\"\n      type: \"azure\"\n      azure:\n        storage_account: \"analytics-account\"\n        storage_access_key: \"EXAMPLE45551FSAsVVCXCF\"\n</code></pre> <p>This hybrid setup allows lakeFS to manage data across both cloud and on-prem storages.</p> <p>Example</p> <pre><code>blockstores:\n  signing:\n    secret_key: \"some-secret\"\n  stores:\n    - id: \"s3-archive\"\n      description: \"AWS S3 storage for long-term archival\"\n      type: \"s3\"\n      s3:\n        region: \"us-west-2\"\n    - id: \"minio-fast-access\"\n      description: \"On-prem MinIO for high-performance workloads\"\n      type: \"s3\"\n      s3:\n        force_path_style: true\n        endpoint: 'http://minio.local'\n        discover_bucket_region: false\n        credentials:\n          access_key_id: \"minio_access_key\"\n          secret_access_key: \"minio_secret_key\"\n</code></pre>"},{"location":"howto/multiple-storage-backends/#key-considerations","title":"Key Considerations","text":"<ul> <li>Unique Blockstore IDs: Each storage must have a unique id.</li> <li>Persistence of Blockstore IDs: Once defined, an id must not change.</li> <li>S3 Authentication Handling:</li> <li>All standard S3 authentication methods are supported.</li> <li>Every blockstore needs to be authenticated.  So make sure to configure a profile or static credentials for all storages of type <code>s3</code>.       S3 storage will use the credentials chain by default, so you might be able to use that for one storage.</li> </ul> <p>Warning</p> <p>Changing a storage ID is not supported and may result in unexpected behavior. Ensure IDs remain consistent once configured.</p>"},{"location":"howto/multiple-storage-backends/#upgrading-from-a-single-storage-backend-to-multiple-storage-backends","title":"Upgrading from a single storage backend to Multiple Storage backends","text":"<p>When upgrading from a single storage backend to a multi-storage setup, follow these guidelines:</p> <ul> <li>Use the new <code>blockstores</code> structure, replacing the existing <code>blockstore</code> configuration. Note that <code>blockstore</code> and <code>blockstores</code>   configurations are mutually exclusive - lakeFS does not support both simultaneously.</li> <li>Define all previously available single-blockstore settings under their respective storage backends.</li> <li>The <code>signing.secret_key</code> is a required setting global to all connected stores.</li> <li>Set <code>backward_compatible: true</code> for the existing storage backend to ensure:</li> <li>Existing repositories continue to use the original storage backend.</li> <li>Newly created repositories default to this backend unless explicitly assigned a different one, to ensure a non-breaking upgrade process.</li> <li>This setting is mandatory \u2014 lakeFS will not function if it is unset.</li> <li>Do not remove this setting as long as you need to support repositories created before the upgrade.     If removed, lakeFS will fail to start because it will treat existing repositories as disconnected from any configured storage.</li> </ul>"},{"location":"howto/multiple-storage-backends/#adding-or-removing-a-storage-backend","title":"Adding or Removing a Storage Backend","text":"<p>To add a storage backend, update the server configuration with the new storage entry and restart the server.</p> <p>To remove a storage backend:</p> <ul> <li>Delete all repositories associated with the storage backend. (definition only)</li> <li>Remove the storage entry from the configuration.</li> <li>Restart the server.</li> </ul> <p>Warning</p> <p>lakeFS will fail to start if there are repositories defined on a removed storage. Ensure all necessary cleanup is completed before removing a storage backend.</p>"},{"location":"howto/multiple-storage-backends/#listing-connected-storage-backends","title":"Listing Connected Storage Backends","text":"<p>The Get Config API endpoint returns a list of storage configurations. In multi-storage setups, this is the recommended method to list connected storage backends and view their details.</p>"},{"location":"howto/multiple-storage-backends/#troubleshooting","title":"Troubleshooting","text":"Issue Cause Solution Blockstore ID conflicts Duplicate <code>id</code> values in <code>stores</code> Ensure each storage backend has a unique ID Missing <code>backward_compatible</code> Upgrade from single to multi-storage without setting the flag Add <code>backward_compatible: true</code> for the existing storage Unsupported configurations in OSS or unlicensed Enterprise accounts Using multi-storage features in an unsupported setup Contact us to start using the feature"},{"location":"howto/multiple-storage-backends/#migrating-from-multiple-storage-backend-to-single-storage-backend","title":"Migrating from Multiple Storage Backend to Single Storage Backend","text":"<p>Once you upgrade to a multi-storage setup, you cannot simply revert back by changing the configuration from <code>blockstores</code> to <code>blockstore</code>. The internal repository metadata format changes to support multiple storage backends, and is not backward compatible with the single storage format. If you need to consolidate your data and revert from a multi-storage setup to a single storage backend, you'll need to perform a full migration by following these steps:</p>"},{"location":"howto/multiple-storage-backends/#overview","title":"Overview","text":"<p>The migration process involves:</p> <ol> <li>Dumping repository references from the multi-storage setup</li> <li>Deleting repositories in the multi-storage environment</li> <li>Configuring lakeFS with a single storage backend</li> <li>[Optional]: Copying repository data to the new single storage location</li> <li>Restoring repositories to the single storage environment</li> </ol>"},{"location":"howto/multiple-storage-backends/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>Use the <code>lakefs-refs.py</code> script, instruction on how to aquire found in Backup and Restore.</p> <ol> <li> <p>Dump Repository References     To dump the repository metadata:</p> <pre><code># Dump a single repository\npython lakefs-refs.py dump my-repository\n\n# Or dump all repositories\npython lakefs-refs.py dump --all\n</code></pre> <p>This will create manifest files for each repository.</p> <p>Optionally, you can use the <code>--rm</code> flag to automatically delete repositories after successful dump:</p> <pre><code># Dump and delete a single repository\npython lakefs-refs.py dump my-repository --rm\n\n# Or dump and delete all repositories\npython lakefs-refs.py dump --all --rm\n</code></pre> </li> <li> <p>Delete Source Repositories (if not using --rm flag)</p> <p>If you didn't use the <code>--rm</code> flag in step 1, you'll need to delete the repositories manually. Note that deleting a repository only removes the repository record from lakeFS - it does not delete the actual data files or metadata from your storage.</p> <p>You can delete repositories through:</p> <p>a. The lakeFS UI b. Using lakectl (ex: <code>lakectl repo delete lakefs://my-repository</code>)</p> </li> <li> <p>Configure lakeFS Single Storage</p> <ul> <li>Update your lakeFS configuration to use a single storage backend (using the <code>blockstore</code> section instead of <code>blockstores</code>).</li> <li>Start or restart lakeFS after applying the new configuration</li> </ul> Example of single storage backend configuration <pre><code>...\nblockstore:\n  type: s3\n  s3:\n    region: us-east-1\n...\n</code></pre> </li> <li> <p>Copy Repository Data (if needed)</p> <p>If the repositories you want to restore were created on a storage system that lakeFS is no longer connected to:</p> <p>a. Copy data from the old storage locations to the new one:</p> <pre><code># Example for S3\naws s3 sync s3://old-bucket/path/to/storge-namespace s3://new-bucket/path/to/storage-namespace\n\n# Alternative: Using rclone for cross-provider transfers\n# rclone supports various storage providers (S3, Azure, GCS, etc.)\nrclone sync azure:old-container/path/to/storage-namespace aws:new-bucket/path/to/storage-namespace\n</code></pre> <p>b. Update the manifest file, created on step 1 (dump repository references) with the new storage namespace:</p> <pre><code>{\n    \"repository\": {\n        \"name\": \"my-repository\",\n        \"storage_namespace\": \"s3://new-bucket/path/to/repo\", // ... update here ...\n        \"default_branch\": \"main\",\n        \"storage_id\": \"storage-1\"\n    },\n    \"refs\": {\n        // ... existing refs data ...\n    }\n}\n</code></pre> </li> <li> <p>Restore Repositories     !!! note         If you copied the data to a new location in step 4, make sure to update the storage namespace in the manifest files before restoring.</p> <p>Use the <code>--ignore-storage-id</code> flag to ensure repositories are created without storage IDs in the single-storage environment:</p> <pre><code># Restore a single repository\npython lakefs-refs.py restore my-repository_manifest.json --ignore-storage-id\n\n# Or restore multiple repositories\npython lakefs-refs.py restore repo1_manifest.json repo2_manifest.json --ignore-storage-id\n</code></pre> </li> </ol> <p>Important Notes</p> <ul> <li>Keep the manifest files safe as they contain repository metadata</li> <li>If using different storage backends, ensure proper access permissions to copy the data</li> <li>The <code>--commit</code> flag can be used if you want to ensure all changes are committed before dumping</li> <li>Make sure the new storage backend has sufficient space for all repository data</li> <li>A lakeFS instance configured with a single storage type will not start if repositories created on multiple storage setup still exist</li> </ul>"},{"location":"howto/multiple-storage-backends/#working-with-repositories","title":"Working with Repositories","text":"<p>After setting up lakeFS Enterprise to connect with multiple storage backends, this section explains how to use these connected storages when working with lakeFS.</p> <p>With multiple storage backends configured, lakeFS repositories are now linked to a specific storage. Together with the repository's storage namespace, this defines the exact location in the underlying storage where the repository's data is stored.</p> <p>The choice of storage backend impacts the following lakeFS operations:</p>"},{"location":"howto/multiple-storage-backends/#creating-a-repository","title":"Creating a Repository","text":"<p>In a multi-storage setup, users must specify a storage ID when creating a repository. This can be done using the following methods:</p> UICLIAPIHigh-Level Python SDK <p>Select a storage backend from the dropdown menu. </p> <p>Use the <code>--storage-id</code> flag with the repo create command:</p> <pre><code>lakectl repo create lakefs://my-repo s3://my-bucket --storage-id my-storage\n</code></pre> <p>Note</p> <p>The <code>--storage-id</code> flag is currently hidden in the CLI.</p> <p>Use the <code>storage_id</code> parameter in the Create Repository endpoint.</p> <p>Starting from version 0.9.0 of the High-level Python SDK, you can use <code>kwargs</code> to pass <code>storage_id</code> dynamically when calling the create repository method:</p> <pre><code>import lakefs\n\nrepo = lakefs.Repository(\"example-repo\").create(\n    storage_namespace=\"s3://storage-bucket/repos/example-repo\", \n    storage_id=\"my-storage-id\"\n)\n</code></pre> <p>Important Notes</p> <p>*In multi-storage setups where a storage backend is marked as <code>backward_compatible: true</code>, repository creation requests without a storage ID will default to this storage. * If no storage backend is marked as <code>backward_compatible</code>, repository creation requests without a storage ID will fail. * Each repository is linked to a single backend and stores data within a single storage namespace on that backend.</p>"},{"location":"howto/multiple-storage-backends/#viewing-repository-details","title":"Viewing Repository Details","text":"<p>To check which storage backend is associated with a repository:</p> UIAPI <p>The storage ID is displayed under \"Storage\" in the repository settings page. </p> <p>Use the List Repositories endpoint. Its response includes the storage ID.</p>"},{"location":"howto/multiple-storage-backends/#importing-data-into-a-repository","title":"Importing Data into a Repository","text":"<p>Importing data into a repository is supported when the credentials used for the repository's backing blockstore allow read and list access to the storage location.</p>"},{"location":"howto/multiple-storage-backends/#limitations","title":"Limitations","text":""},{"location":"howto/multiple-storage-backends/#supported-storages","title":"Supported storages","text":"<p>Multi-storage backend support has been validated on:</p> <ul> <li>Self-managed S3-compatible object storage (MinIO)</li> <li>Amazon S3</li> <li>Local storage</li> </ul> <p>Warning</p> <p>Other storage backends may work but have not been officially tested. If you're interested in exploring additional configurations, please reach contact us.</p>"},{"location":"howto/multiple-storage-backends/#unsupported-clients","title":"Unsupported clients","text":"<p>The following clients do not currently support working with multiple storage backends. However, we are actively working to bridge this gap:</p> <ul> <li>Spark-based GC</li> <li>Spark client</li> <li>lakeFS Hadoop FileSystem</li> <li>Everest</li> </ul>"},{"location":"howto/private-link/","title":"Private Link","text":"<p>Info</p> <p>PrivateLink is only applicable to lakeFS Cloud</p> <p>Private Link enables lakeFS Cloud to interact with your infrastructure using private networking.</p>"},{"location":"howto/private-link/#supported-vendors","title":"Supported Vendors","text":"<p>At the moment, we support Private-Link with AWS and Azure. If you are looking for Private Link for GCP please contact us.</p> AWSAzure <p>Azure Private Link enables secure access to Azure services from a private endpoint within your virtual network. By using Azure Private Link with lakeFS, you can securely access lakeFS services without exposing traffic to the public internet. In this manual, we will guide you through the steps to enable Azure Private Link to your lakeFS instance.</p>"},{"location":"howto/private-link/#access-methods","title":"Access Methods","text":"<p>There are two types of Private Link implementation:</p> <ul> <li> <p>Front-End Access refers to API and UI access. Use this option if you'd like your lakeFS application to be exposed only to your infrastructure and not to the whole internet.</p> </li> <li> <p>Back-End Access refers to the network communication between the lakeFS clusters we host, and your infrastructure. Use this option if you'd like lakeFS to communicate with your servers privately and not over the internet.</p> </li> </ul> <p>The two types of access are not mutually exclusive nor are they dependent on each other.</p>"},{"location":"howto/private-link/#setting-up-private-link","title":"Setting up Private Link","text":""},{"location":"howto/private-link/#front-end-access","title":"Front-End Access","text":"<p>Prerequisites:</p> <ul> <li>Administrator access to your AWS account</li> <li>In order for us to communicate with your account privately, we'll need to create a service endpoint on our end first.</li> </ul> <p>Steps:</p> <ol> <li>Login to your AWS account</li> <li>Go to AWS VPC Service</li> <li>Filter the relevant VPC &amp; Navigate to Endpoints</li> <li>Click Create endpoint</li> <li>Fill in the following:<ul> <li>Name: lakefs-cloud</li> <li>Service category: Other endpoint services</li> <li>Service name: input from Treeverse team (see prerequisites)</li> <li>Click Verify service</li> <li>Pick the VPC you'd like to expose this service to.</li> <li>Click Create endpoint</li> </ul> </li> </ol> <p>Now you can access your infrastructure privately using the endpoint DNS name. If you would like to change the DNS name to a friendly one please contact support@treeverse.io.</p>"},{"location":"howto/private-link/#back-end-access","title":"Back-End Access","text":"<p>Prerequisites:</p> <ul> <li>Administrator access to your AWS account</li> </ul> <p>Steps:</p> <ol> <li>Login to your AWS account</li> <li>Go to AWS VPC Service</li> <li>Filter the relevant VPC &amp; Navigate to Endpoints</li> <li>Click endpoint service</li> <li>Fill in the following:<ul> <li>Name: lakefs-cloud</li> <li>Load Balancer Type: Network</li> <li>Available load balancers: pick the load balancer you'd like lakefs-cloud to send events to.</li> <li>Click Create</li> </ul> </li> <li>Pick the newly created Endpoint Service from within the Endpoint Services page.</li> <li>Navigate to the Allow principals tab.</li> <li>Click Allow principals</li> <li>Fill in the following ARN: <code>arn:aws:iam::924819537486:root</code></li> <li>Click Allow principals</li> </ol> <p>That's it on your end! Now, we'll need the service name you've just created in order to associate it with our infrastructure, once we do, we'll be ready to use the back-end access privately.</p>"},{"location":"howto/private-link/#register-your-azure-subscription","title":"Register your Azure subscription","text":"<p>To automatically approve private endpoint connections to the lakeFS network, please provide us with your subscription. If required, you can register multiple subscriptions.</p>"},{"location":"howto/private-link/#create-an-azure-private-link-connection-to-lakefs-cloud","title":"Create an Azure Private Link connection to lakeFS Cloud","text":"<p>Once your subscription is in our trusted subscriptions navigate to the Azure portal and do the following steps:</p> <ol> <li>Navigate to the private endpoint</li> <li>Click Create</li> <li>On the first step (basics):<ul> <li>Select your subscription</li> <li>Specify the desired resource group used to access lakeFS</li> <li>Provide a name for your private endpoint instance</li> <li>Specify the region of your lakeFS instance</li> </ul> </li> <li>On the second step (Resource)<ul> <li>In connection method select <code>connect to an Azure resource by resource ID or alias</code></li> <li>Insert the alias provided by us into the Resource ID or alias</li> <li>No need to add a request message</li> </ul> </li> <li>Continue with the steps and run Review + Create</li> </ol>"},{"location":"howto/private-link/#create-a-dns-entry-for-your-private-endpoint","title":"Create a DNS entry for your private endpoint","text":"<p>Update your DNS server to resolve your account URL (which will be provided by us) to the Private Link IP address. You can add the DNS entry to your on-premises DNS server or private DNS on your VNet, to access lakeFS services.</p>"},{"location":"howto/protect-branches/","title":"Branch Protection Rules","text":"<p>Define branch protection rules to prevent direct changes and commits to specific branches. Only merges are allowed into protected branches. Together with the power of pre-merge hooks, you can run validations on your data before it reaches your important branches and is exposed to consumers.</p> <p>You can create rules for a specific branch or any branch that matches a name pattern you specify with glob syntax (supporting <code>?</code> and <code>*</code> wildcards).</p>"},{"location":"howto/protect-branches/#how-it-works","title":"How it works","text":"<p>When at least one protection rule applies to a branch, the branch is protected. The following operations will fail on protected branches:</p> <ol> <li>Object write operations: upload and delete objects.</li> <li>Branch operations: commit and reset uncommitted changes.</li> </ol> <p>To operate on a protected branch, merge commits from other branches into it. Use pre-merge hooks to validate the changes before they are merged.</p> <p>Note</p> <p>Reverting a previous commit using <code>lakectl branch revert</code> is allowed on a protected branch.</p>"},{"location":"howto/protect-branches/#managing-branch-protection-rules","title":"Managing branch protection rules","text":"<p>This section explains how to use the lakeFS UI to manage rules. You can also use the command line and API.</p>"},{"location":"howto/protect-branches/#reaching-the-branch-protection-rules-page","title":"Reaching the branch protection rules page","text":"<ol> <li>On lakeFS, navigate to the main page of the repository.</li> <li>Click on the Settings tab.</li> <li>In the left menu, click Branches.</li> </ol>"},{"location":"howto/protect-branches/#adding-a-rule","title":"Adding a rule","text":"<p>To add a new rule, click the Add button. In the dialog, enter the branch name pattern and then click Create.</p> <p></p>"},{"location":"howto/protect-branches/#deleting-a-rule","title":"Deleting a rule","text":"<p>To delete a rule, click the Delete button next to it.</p> <p></p>"},{"location":"howto/pull-requests/","title":"Pull Requests","text":"<p>A pull request is a proposal to merge a set of changes from one branch into another. In a pull request, collaborators can review the proposed set of changes before they integrate the changes. Pull requests display the differences, or diffs, between the content in the source branch and the content in the target branch.</p>"},{"location":"howto/pull-requests/#open-a-pull-request","title":"Open a Pull Request","text":"<p>Create a branch, and make all the necessary changes in that branch. When your changes are ready for review, head over to the Pull Requests tab in your repository. Choose your source branch and target branch, add a title and description (optional, and markdown is supported).</p> <p></p> <p>When ready, click Create Pull Request. You will be redirected to the newly created pull request page.</p>"},{"location":"howto/pull-requests/#review-changes","title":"Review Changes","text":"<p>Run validation checks or automated data quality tests to ensure that the changes meet your standards.</p> <p></p> <p>Every Pull Request is assigned a unique ID. You can share the Pull Request's URL with others to review the change.</p> <p>As with any lakeFS reference, reviewers can take the source branch, query, test and modify it as necessary prior to merging.</p>"},{"location":"howto/pull-requests/#merge-or-close","title":"Merge or Close","text":"<p>Once the review is complete and all checks have passed, click the Merge pull request button to merge the changes into the target branch.</p> <p></p> <p>The data is now updated in a controlled and transparent manner.</p> <p>If the changes are not ready to be merged, you can close the pull request without merging the changes, by clicking the Close pull request button.</p>"},{"location":"howto/pull-requests/#view-pull-requests","title":"View Pull Requests","text":"<p>You can view all open and closed pull requests in the Pull Requests tab in your repository. The tabs (Open, Closed) allow you to filter the list of pull requests according to their status.</p>"},{"location":"howto/scim/","title":"System for Cross-domain Identity Management (SCIM)","text":"<p>Info</p> <p>SCIM support is available on lakeFS Cloud and lakeFS Enterprise</p> <p>lakeFS Cloud includes an SCIM v2.0 compliant server, which can integrate with SCIM clients (IdPs) to automate provisioning/de-provisioning of users and groups.  </p>"},{"location":"howto/scim/#officially-supported-clients-idps-capabilities-and-limitations","title":"Officially Supported Clients (IdPs), Capabilities, and Limitations","text":""},{"location":"howto/scim/#supported-clients-idps","title":"Supported Clients (IdPs)","text":"<p>Currently, the lakeFS Cloud SCIM server has been tested and validated with Entra ID (a.k.a Azure AD). However, with SCIM v2.0 being an accepted standard, any SCIM-compliant IdP should be able to integrate with lakeFS Cloud.</p>"},{"location":"howto/scim/#capabilities","title":"Capabilities","text":""},{"location":"howto/scim/#user-provisioning","title":"User Provisioning","text":"<ul> <li>Create users: Users and members of groups assigned to the application will be provisioned in lakeFS Cloud</li> <li>Update user attributes: Changes to supported user attributes are synced to lakeFS Cloud</li> <li>Deactivate users: Deactivating a user or removing their assignment to the application will disable them in lakeFS Cloud</li> <li>User adoption: Users that are already found in lakeFS Cloud will be \"adopted\" by the IdP and not re-created</li> </ul>"},{"location":"howto/scim/#group-provisioning","title":"Group Provisioning","text":"<ul> <li>Create groups: Groups assigned to the application are created in lakeFS Cloud and any group user members are created and added to the group in lakeFS Cloud</li> <li>Update group name: When a synced group is renamed in the IdP, it will be renamed in lakeFS Cloud</li> <li>Add/remove members: When members are added/removed from an assigned group, they will be added/removed from the group in lakeFS Cloud</li> <li>Group adoption: Groups that already exist in lakeFS Cloud will be \"adopted\" by the IdP and not re-created</li> </ul>"},{"location":"howto/scim/#user-attributes-and-consent","title":"User Attributes and Consent","text":"<p>The lakeFS Cloud SCIM server requires the minimum set of user attributes required for provisioning. The required attributes are a sub-set of the basic user profile, which is exchanged during federated authentication/SSO login. User consent is requested by the IdP upon first login to lakeFS Cloud.</p>"},{"location":"howto/scim/#known-limitations","title":"Known Limitations","text":"<ul> <li>User and group policies can only be managed in lakeFS   This means groups and users newly created via SCIM only have basic read permissions. The lakeFS UI or API must be used to attach policies to those users and groups. However, if a user is created and added to an existing group with an attached policy, that user will receive the permissions allowed by the policy attached to the group.</li> <li>Only direct group memberships are provisioned via SCIM   Both Okta and Entra ID only support syncing direct group membership via SCIM. This means that if you assign a group to the application, only its user members will be provisioned via SCIM. SCIM provisioning will not cascade to member groups and their members, and so forth.</li> </ul>"},{"location":"howto/scim/#enabling-scim-in-lakefs-cloud","title":"Enabling SCIM in lakeFS Cloud","text":"<p>To enable SCIM support in lakeFS Cloud, you need to log into the cloud admin. In the cloud admin, SCIM settings are under Access &gt; Settings. SCIM is not enabled by default, so to enable SCIM for the organization, click the Setup Provisioning Button.</p> <p></p> <p>Clicking the button will enable SCIM for the organization and provide the details you'll need to set up your IdP to work with lakeFS Cloud SCIM.</p> <p></p> <p>To set up your IdP, you'll need the lakeFS Cloud SCIM provisioning endpoint and you'll also need to generate an integration token. When creating a new integration token, you can optionally provide a description for future reference.</p> <p>Info</p> <p>The token value is only presented once, right after creation. Make sure to copy the token, as its value isn't stored and cannot be retrieved after the initial creation.</p>"},{"location":"howto/scim/#setting-up-scim-provisioning-in-entra-id-aka-azure-ad","title":"Setting Up SCIM Provisioning in Entra ID (a.k.a Azure AD)","text":"<p>Note</p> <p>This guide assumes you've already set up an Entra ID enterprise application for federated authentication to lakeFS Cloud.</p> <p>In the Entra ID admin dashboard, go to Enterprise Applications and choose the lakeFS Cloud enterprise application from the list. Then click Provisioning in the sidebar and then Get Started.</p> <ol> <li>In the provisioning settings set mode to Automatic</li> <li>In Tenant URL enter the URL from the lakeFS Cloud provisioning settings. You will need to append <code>?aadOptscim062020</code> to the end of the URL to ensure proper integration with Entra ID.</li> <li>In Secret Token paste the token you copied in the previous step. If you haven't created a token yet, you may do so now</li> <li>Click Test Connection</li> <li>If the test fails, please ensure you've entered the correct SCIM endpoint URL from lakeFS Cloud and copied the token correctly. Otherwise, click \"Save\" at the top of the settings panel</li> <li>Configure provisioning attribute mappings(this determines which attributes are sent to the lakeFS SCIM endpoint)</li> </ol>"},{"location":"howto/scim/#required-attributes","title":"Required Attributes","text":"<p>The LakeFS SCIM implementation has a number of attributes that it expects to see in requests. Missing, incorrect, or extraneous attributes will generally result in a 400 error code.</p>"},{"location":"howto/scim/#user-resource-attributes","title":"User Resource Attributes","text":"<ul> <li>username: Unique identifier for the User, typically used by the user to directly authenticate to the service provider. Each User MUST include a non-empty userName value. This identifier MUST be unique across the service provider's entire set of Users. REQUIRED.</li> <li>externalId: A String that is an identifier for the resource as defined by the provisioning client. REQUIRED.</li> <li>emails: Email addresses for the user. The value SHOULD be canonicalized by the service provider, e.g., 'bjensen@example.com' instead of 'bjensen@EXAMPLE.COM'. Canonical type values of 'work', 'home', and 'other'. One should be marked as primary. REQUIRED.</li> <li>active: A Boolean value indicating the User's administrative status.</li> </ul>"},{"location":"howto/scim/#group-resource-attributes","title":"Group Resource Attributes","text":"<ul> <li>displayName: A human-readable name for the Group. REQUIRED.</li> <li>externalId: A String that is an identifier for the resource as defined by the provisioning client. REQUIRED.</li> <li>members: A list of members of the Group.</li> </ul> <p>Info</p> <p>lakeFS Cloud is designed to work with the default attribute mapping for users and groups provided by Entra ID. If your organization has customized the user and/or group entities in Entra ID, you might want to set mappings in accordance with those. You can find details of how this is done in the Entra ID documentation.  </p> <p>Incorrectly modifying these mappings can break provisioning functionality, so it's advised to do so cautiously and only when necessary.</p>"},{"location":"howto/sizing-guide/","title":"Sizing guide","text":"<p>Info</p> <p>For a scalable managed lakeFS service with guaranteed SLAs, try lakeFS Cloud</p>"},{"location":"howto/sizing-guide/#system-requirements","title":"System Requirements","text":""},{"location":"howto/sizing-guide/#operating-systems-and-isa","title":"Operating Systems and ISA","text":"<p>lakeFS can run on MacOS and Linux. Windows binaries are available but not rigorously tested -  we don't recommend deploying lakeFS to production on Windows. x86_64 and arm64 architectures are supported for both MacOS and Linux.</p>"},{"location":"howto/sizing-guide/#memory-and-cpu-requirements","title":"Memory and CPU requirements","text":"<p>lakeFS servers require a minimum of 512mb of RAM and 1 CPU core.  For high throughput, additional CPUs help scale requests across different cores.  \"Expensive\" operations such as large diff or commit operations can take advantage of multiple cores. </p>"},{"location":"howto/sizing-guide/#network","title":"Network","text":"<p>If using the data APIs such as the S3 Gateway,  lakeFS will require enough network bandwidth to support the planned concurrent network upload/download operations. For most cloud providers, more powerful machines (i.e., more expensive and usually containing more CPU cores) also provide increased network bandwidth.</p> <p>If using only the metadata APIs (for example, only using the Hadoop/Spark clients), network bandwidth is minimal,  at roughly 1Kb per request.</p>"},{"location":"howto/sizing-guide/#disk","title":"Disk","text":"<p>lakeFS greatly benefits from fast local disks.  A lakeFS instance doesn't require any strong durability guarantees from the underlying storage,  as the disk is only ever used as a local caching layer for lakeFS metadata and not for long-term storage. lakeFS is designed to work with ephemeral disks -  these are usually based on NVMe and are tied to the machine's lifecycle.  Using ephemeral disks lakeFS can provide a very high throughput/cost ratio,  probably the best that could be achieved on a public cloud, so we recommend those.</p> <p>A local cache of at least 512 MiB should be provided.  For large installations (managing &gt;100 concurrently active branches, with &gt;100M objects per commit), we recommend allocating at least 10 GiB - since it's a caching layer over a relatively slow storage (the object store),  see Important metrics below to understand how to size this: it should be big enough to hold all commit metadata for actively referenced commits.</p>"},{"location":"howto/sizing-guide/#lakefs-kv-store","title":"lakeFS KV Store","text":"<p>lakeFS uses a key-value database to manage branch references, authentication and authorization information  and to keep track of currently uncommitted data across branches. Please refer to the relevant driver tab for best practices, requirements and benchmarks.</p>"},{"location":"howto/sizing-guide/#storage","title":"Storage","text":"<p>The dataset stored in the metadata store is relatively modest as most metadata is pushed down into the object store.  Required storage is mostly a factor of the amount of uncommitted writes across all branches at any given point in time:  in the range of 150 MiB per every 100,000 uncommitted writes. </p> <p>We recommend starting at 10 GiB for a production deployment, as it will likely be more than enough.</p> PostgreSQLDynamoDB <p>RAM Since the data size is small, it's recommended to provide enough memory to hold the vast majority of that data in RAM. Cloud providers will save you the need to tune this parameter - it will be set to a fixed percentage the chosen instance's available RAM (25% on AWS RDS, 30% on Google Cloud SQL). It is recommended that you check with your selected cloud provider for configuration and provisioning information for you database. For self-managed database instances follow these best practices</p> <p>Ideally, configure the shared_buffers of your PostgreSQL instances to be large enough to contain the currently active dataset. Pick a database instance with enough RAM to accommodate this buffer size at roughly x4 the size given for <code>shared_buffers</code>. For example, if an installation has ~500,000 uncommitted writes at any given time, it would require about 750 MiB of <code>shared_buffers</code> that would require about 3 GiB of RAM.</p> <p>CPU PostgreSQL CPU cores help scale concurrent requests. 1 CPU core for every 5,000 requests/second is ideal.</p> <p>lakeFS will create a DynamoDB table for you, defaults to on-demand capacity setting. No need to specify how much read and write throughput you expect your application to perform, as DynamoDB instantly accommodates your workloads as they ramp up or down.</p> <p>You can customize the table settings to provisioned capacity which allows you to manage and optimize your costs by allocating read/write capacity in advance (see Benchmarks)</p> <p>Notes</p> <ul> <li>Using DynamoDB on-demand capacity might generate unwanted costs if the table is abused, if you'd like to cap your costs, make sure to change the table to use provisioned capacity instead.  </li> <li>lakeFS doesn't manage the DynamoDB's table lifecycle, we've included the table creation in order to help evaluating the system with minimal effort, any change to the table beyond the table creation - will need to be handled manually or by 3rd party tools.</li> </ul> <p>RAM Managed by AWS.</p> <p>CPU Managed by AWS.</p>"},{"location":"howto/sizing-guide/#scaling-factors","title":"Scaling factors","text":"<p>Scaling lakeFS, like most data systems, moves across two axes: throughput of requests (amount per given timeframe) and latency (time to complete a single request).</p>"},{"location":"howto/sizing-guide/#understanding-latency-and-throughput-considerations","title":"Understanding latency and throughput considerations","text":"<p>Most lakeFS operations are designed to be very low in latency. Assuming a well-tuned local disk cache (see Storage above), most critical path operations (writing objects, requesting objects, deleting objects) are designed to complete in &lt;25ms at p90. Listing objects obviously requires accessing more data, but should always be on-par with what the underlying object store can provide, and in most cases, it's actually faster. At the worst case, for directory listing with 1,000 common prefixes returned, expect a latency of 75ms at p90.</p> <p>Managing branches (creating them, listing them and deleting them) are all constant-time operations, generally taking &lt;30ms at p90.</p> <p>Committing and merging can take longer, as they are proportional to the amount of changes introduced. This is what makes lakeFS optimal for large Data Lakes - the amount of changes introduced per commit usually stays relatively stable while the entire data set usually grows over time. This means lakeFS will provide predictable performance: committing 100 changes will take roughly the same amount of time whether the resulting commit contains 500 or 500 million objects.</p> <p>See Data Model for more information.</p> <p>Scaling throughput depends very much on the amount of CPU cores available to lakeFS. In many cases, it's easier to scale lakeFS across a fleet of smaller cloud instances (or containers) than scale up with machines that have many cores. In fact, lakeFS works well in both cases. Most critical path operations scale very well across machines.</p>"},{"location":"howto/sizing-guide/#benchmarks","title":"Benchmarks","text":"PostgresSQLDynamoDB <p>All benchmarks below were measured using 2 x c5ad.4xlarge instances on AWS us-east-1. Similar results can be achieved on Google Cloud using a <code>c2-standard-16</code> machine type, with an attached local SSD. On Azure, you can use a <code>Standard_F16s_v2</code> virtual machine.</p> <p>The PostgreSQL instance that was used is a db.m6g.2xlarge (8 vCPUs, 32 GB RAM). Equivalent machines on Google Cloud or Azure should yield similar results.</p> <p>The example repository we tested against contains the metadata of a large lakeFS installation, where each commit contains ~180,000,000 objects (representing ~7.5 Petabytes of data).</p> <p>All tests are reproducible using the lakectl abuse command, so use it to properly size and tune your setup. All tests are accompanied by the relevant <code>lakectl abuse</code> command that generated them.</p> <p>All benchmarks below were measured using m5.xlarge instance on AWS us-east-1.</p> <p>The DynamoDB table that was used was provisioned with 500/1000 read/write capacity.</p> <p>The example repository we tested against contains the metadata of a large lakeFS installation, where each commit contains ~100,000,000 objects (representing ~3.5 Petabytes of data).</p> <p>All tests are reproducible using the lakectl abuse command, so use it to properly size and tune your setup. All tests are accompanied by the relevant lakectl abuse command that generated them.</p>"},{"location":"howto/sizing-guide/#random-reads","title":"Random reads","text":"<p>This test generates random read requests to lakeFS, in a given commit. Paths are requested randomly from a file containing a set of preconfigured (and existing) paths.</p> <p>command executed:</p> <pre><code>lakectl abuse random-read \\\n    --from-file randomly_selected_paths.txt \\\n    --amount 500000 \\\n    --parallelism 128 \\\n    lakefs://example-repo/&lt;commit hash&gt;\n</code></pre> <p>Note lakeFS version &lt;= v0.33.1 uses '@' (instead of '/') as separator between repository and commit hash.</p> <p>Result Histogram (raw):</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   37945\n7   179727\n10  296964\n15  399682\n25  477502\n50  499625\n75  499998\n100 499998\n250 500000\n350 500000\n500 500000\n750 500000\n1000    500000\n5000    500000\nmin 3\nmax 222\ntotal   500000\n</code></pre> <p>So 50% of all requests took &lt;10ms, while 99.9% of them took &lt;50ms</p> <p>throughput:</p> <p>Average throughput during the experiment was 10851.69 requests/second</p>"},{"location":"howto/sizing-guide/#random-writes","title":"Random Writes","text":"<p>This test generates random write requests to a given lakeFS branch. All the paths are pre-generated and don't overwrite each other (as overwrites are relatively rare in a Data Lake setup).</p> <p>command executed:</p> <pre><code>lakectl abuse random-write \\\n    --amount 500000 \\\n    --parallelism 64 \\\n    lakefs://example-repo/main\n</code></pre> <p>Note lakeFS version &lt;= v0.33.1 uses '@' (instead of '/') as separator between repository and branch.</p> <p>Result Histogram (raw):</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   30715\n7   219647\n10  455807\n15  498144\n25  499535\n50  499742\n75  499784\n100 499802\n250 500000\n350 500000\n500 500000\n750 500000\n1000    500000\n5000    500000\nmin 3\nmax 233\ntotal   500000\n</code></pre> <p>So, 50% of all requests took &lt;10ms, while 99.9% of them took &lt;25ms.</p> <p>throughput:</p> <p>The average throughput during the experiment was 7595.46 requests/second.</p>"},{"location":"howto/sizing-guide/#branch-creation","title":"Branch creation","text":"<p>This test creates branches from a given reference.</p> <p>command executed:</p> <pre><code>lakectl abuse create-branches \\\n    --amount 500000 \\\n    --branch-prefix \"benchmark-\" \\\n    --parallelism 256 \\\n    lakefs://example-repo/&lt;commit hash&gt;\n</code></pre> <p>Note lakeFS version &lt;= v0.33.1 uses '@' (instead of '/') as separator between repository and commit hash.</p> <p>Result Histogram (raw):</p> <pre><code>Histogram (ms):\n1   0\n2   1\n5   5901\n7   39835\n10  135863\n15  270201\n25  399895\n50  484932\n75  497180\n100 499303\n250 499996\n350 500000\n500 500000\n750 500000\n1000    500000\n5000    500000\nmin 2\nmax 304\ntotal   500000\n</code></pre> <p>So, 50% of all requests took &lt;15ms, while 99.9% of them took &lt;100ms.</p> <p>throughput:</p> <p>The average throughput during the experiment was 7069.03 requests/second.</p>"},{"location":"howto/sizing-guide/#random-reads_1","title":"Random reads","text":"<p>This test generates random read requests to lakeFS, in a given commit. Paths are requested randomly from a file containing a set of preconfigured (and existing) paths.</p> <p>command executed:</p> <pre><code>lakectl abuse random-read \\\n    --from-file randomly_selected_paths.txt \\\n    --amount 500000 \\\n    --parallelism 128 \\\n    lakefs://example-repo/&lt;commit hash&gt;\n</code></pre> <p>Result Histogram (raw): Provisioned read capacity units = 1000 Provisioned write capacity units = 1000</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   0\n7   0\n10  0\n15  0\n25  122\n50  47364\n75  344489\n100 460404\n250 497912\n350 498016\n500 498045\n750 498111\n1000 498176\n5000 499478\nmin 18\nmax 52272\ntotal 500000\n</code></pre> <p>Result Histogram (raw): Provisioned read capacity units = 500 Provisioned write capacity units = 500</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   0\n7   0\n10  0\n15  1\n25  2672\n50  239661\n75  420171\n100 470146\n250 486603\n350 486715\n500 486789\n750 487443\n1000    488113\n5000    493201\nmin 14\nmax 648085\ntotal   499998\n</code></pre>"},{"location":"howto/sizing-guide/#random-writes_1","title":"Random Writes","text":"<p>This test generates random write requests to a given lakeFS branch. All the paths are pre-generated and don't overwrite each other (as overwrites are relatively rare in a Data Lake setup).</p> <p>command executed:</p> <pre><code>lakectl abuse random-write \\\n    --amount 500000 \\\n    --parallelism 64 \\\n    lakefs://example-repo/main\n</code></pre> <p>Result Histogram (raw): Provisioned read capacity units = 1000 Provisioned write capacity units = 1000</p> <p><pre><code>Histogram (ms):\n1   0\n2   0\n5   0\n7   0\n10  0\n15  0\n25  24\n50  239852\n75  458504\n100 485225\n250 493687\n350 493872\n500 493960\n750 496239\n1000    499194\n5000    500000\nmin 23\nmax 4437\ntotal   500000\n</code></pre> Result Histogram (raw): Provisioned read capacity units = 500 Provisioned write capacity units = 500</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   0\n7   0\n10  0\n15  0\n25  174\n50  266460\n75  462641\n100 484486\n250 490633\n350 490856\n500 490984\n750 492973\n1000 495605\n5000 498920\nmin 21\nmax 50157\ntotal 500000\n</code></pre>"},{"location":"howto/sizing-guide/#branch-creation_1","title":"Branch creation","text":"<p>This test creates branches from a given reference.</p> <p>command executed:</p> <pre><code>lakectl abuse create-branches \\\n    --amount 500000 \\\n    --branch-prefix \"benchmark-\" \\\n    --parallelism 256 \\\n    lakefs://example-repo/&lt;commit hash&gt;\n</code></pre> <p>Result Histogram (raw): Provisioned read capacity units = 1000 Provisioned write capacity units = 1000</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   0\n7   0\n10  0\n15  0\n25  0\n50  628\n75  26153\n100 58099\n250 216160\n350 307078\n500 406165\n750 422898\n1000    431332\n5000    475848\nmin 41\nmax 430725\ntotal   490054\n</code></pre> <p>Result Histogram (raw): Provisioned read capacity units = 500 Provisioned write capacity units = 500</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   0\n7   0\n10  0\n15  0\n25  0\n50  3132\n75  155570\n100 292745\n250 384224\n350 397258\n500 431141\n750 441360\n1000 445597\n5000 469538\nmin 39\nmax 760626\ntotal 497520\n</code></pre>"},{"location":"howto/sizing-guide/#important-metrics","title":"Important metrics","text":"<p>lakeFS exposes metrics using the Prometheus protocol.  Every lakeFS instance exposes a <code>/metrics</code> endpoint that could be used to extract them. </p> <p>Here are a few notable metrics to keep track of when sizing lakeFS:</p> <p><code>api_requests_total</code> - Tracks throughput of API requests over time.</p> <p><code>api_request_duration_seconds</code> - Histogram of latency per operation type.</p> <p><code>gateway_request_duration_seconds</code> - Histogram of latency per S3 Gateway operation.</p> PostgreSQLDynamoDB <p><code>dynamo_request_duration_seconds</code> - Time spent doing DynamoDB requests.</p> <p><code>dynamo_consumed_capacity_total</code> - The capacity units consumed by operation.</p> <p><code>dynamo_failures_total</code> - The total number of errors while working for kv store.</p>"},{"location":"howto/sizing-guide/#reference-architectures","title":"Reference architectures","text":"<p>Below are a few example architectures for lakeFS deployment. </p>"},{"location":"howto/sizing-guide/#reference-architecture-data-scienceresearch-environment","title":"Reference Architecture: Data Science/Research environment","text":"<p>Use case: Manage Machine learning or algorithms development.  Use lakeFS branches to achieve both isolation and reproducibility of experiments.  Data being managed by lakeFS is both structured tabular data,  as well as unstructured sensor and image data used for training.  Assuming a team of 20-50 researchers, with a dataset size of 500 TiB across 20M objects.</p> <p>Environment: lakeFS will be deployed on Kubernetes.  managed by AWS EKS  with PostgreSQL on AWS RDS Aurora</p> <p>Sizing: Since most of the work is done by humans (vs. automated pipelines), most experiments tend to be small in scale,  reading and writing 10s to 1000s of objects.  The expected amount of branches active in parallel is relatively low, around 1-2 per user,  each representing a small amount of uncommitted changes at any given point in time.  Let's assume 5,000 uncommitted writes per branch = ~500k. </p> <p>To support the expected throughput, a single moderate lakeFS instance should be more than enough,  since requests per second would be on the order of 10s to 100s.  For high availability, we'll deploy 2 pods with 1 CPU core and 1 GiB of RAM each.</p> <p>Since the PostgreSQL instance is expected to hold a very small dataset  (at 500k, expected dataset size is <code>150MiB (for 100k records) * 5 = 750MiB</code>).  To ensure we have enough RAM to hold this, we'll need 3 GiB of RAM, so, a very moderate Aurora instance <code>db.t3.large</code> (2 vCPUs, 8 GB RAM) will be more than enough. An equivalent database instance on GCP or Azure should give similar results.</p> <p></p>"},{"location":"howto/sizing-guide/#reference-architecture-automated-production-pipelines","title":"Reference Architecture: Automated Production Pipelines","text":"<p>Use case: Manage multiple concurrent data pipelines using Apache Spark and Airflow.  Airflow DAGs start by creating a branch for isolation and for CI/CD.  Data being managed by lakeFS is structured, tabular data. The total dataset size is 10 PiB, spanning across 500M objects.  The expected throughput is 10k reads/second + 2k writes per second across 100 concurrent branches.</p> <p>Environment: lakeFS will be deployed on Kubernetes.  managed by AWS EKS  with PostgreSQL on AWS RDS</p> <p>Sizing: Data pipelines tend to be bursty in nature:  reading in a lot of objects concurrently, doing some calculation or aggregation, and then writing many objects concurrently.  The expected amount of branches active in parallel is high,  with many Airflow DAGs running per day, each representing a moderate amount of uncommitted changes at any given point in time.  Let's assume 1,000 uncommitted writes/branch * 2500 branches = ~2.5M records. </p> <p>To support the expected throughput, looking the benchmarking numbers above,  we're doing roughly 625 requests/core, so 24 cores should cover our peak traffic. We can deploy <code>6 * 4 CPU core pods</code>.</p> <p>On to the PostgreSQL instance - at 500k, the expected dataset size is <code>150MiB (for 100k records) * 25 = 3750 MiB</code>.  To ensure we have enough RAM to hold this, we'll need at least 15 GiB of RAM, so we'll go with a <code>db.r5.xlarge</code> (4 vCPUs, 32GB RAM) Aurora instance. An equivalent database instance on GCP or Azure should give similar results.</p> <p></p>"},{"location":"howto/virtual-host-addressing/","title":"Configuring lakeFS to use S3 Virtual-Host addressing","text":""},{"location":"howto/virtual-host-addressing/#understanding-virtual-host-addressing","title":"Understanding virtual-host addressing","text":"<p>Some systems require S3 endpoints (such as lakeFS's S3 Gateway) to support virtual-host style addressing.</p> <p>lakeFS supports this, but requires some configuration in order to extract the bucket name (used as the lakeFS repository ID) from the host address.</p> <p>For example:</p> <pre><code>GET http://foo.example.com/some/location\n</code></pre> <p>There are two ways to interpret the URL above:</p> <ul> <li>as a virtual-host URL where the endpoint URL is <code>example.com</code>, the bucket name is <code>foo</code>, and the path is <code>/some/location</code></li> <li>as a path-based URL where the endpoint is <code>foo.example.com</code>, the bucket name is <code>some</code> and the path is <code>location</code></li> </ul> <p>By default, lakeFS reads URLs as path-based. To read the URL as a virtual-host request, lakeFS requires additional configuration which includes  defining an explicit set of DNS records for the lakeFS S3 gateway.</p>"},{"location":"howto/virtual-host-addressing/#adding-an-explicit-s3-domain-name-to-the-s3-gateway-configuration","title":"Adding an explicit S3 domain name to the S3 Gateway configuration","text":"<p>The first step would be to tell the lakeFS installation which hostnames are used for the S3 Gateway. This should be a different DNS record from the one used for e.g. the UI or API.</p> <p>Typically, if the lakeFS installation is served under <code>lakefs.example.com</code>, a good choice would be <code>s3.lakefs.example.com</code>.</p> <p>This could be done using either an environment variable:</p> <pre><code>LAKEFS_GATEWAYS_S3_DOMAIN_NAME=\"s3.lakefs.example.com\"\n</code></pre> <p>Or by adding the <code>gateways.s3.domain_name</code> setting to the lakeFS <code>config.yaml</code> file:</p> <pre><code>---\ndatabase:\n  connection_string: \"...\"\n\n...\n\n# This section defines an explict S3 gateway address that supports virtual-host addressing\ngateways:\n  s3:\n    domain_name: s3.lakefs.example.com\n</code></pre> <p>Note</p> <p>For more information on how to configure lakeFS, check out the configuration reference</p>"},{"location":"howto/virtual-host-addressing/#setting-up-the-appropriate-dns-records","title":"Setting up the appropriate DNS records","text":"<p>Once our lakeFS installation is configured with an explicit S3 gateway endpoint address, we need to define 2 DNS records and have them point at our lakeFS installation. This requires 2 CNAME records:</p> <ol> <li><code>s3.lakefs.example.com</code> - CNAME to <code>lakefs.example.com</code>. This would be used as the S3 endpoint when configuring clients and will serve as our bare domain.</li> <li><code>*.s3.lakefs.example.com</code> - Also a CNAME to <code>lakefs.example.com</code>. This will resolve virtual-host requests such as <code>example-repo.s3.lakefs.example.com</code> that lakeFS would now know how to parse.</li> </ol> <p>Learn More</p> <p>For more information on how to configure these, see the official documentation of your DNS provider. On AWS, This could also be done using ALIAS records for a load balancer.</p>"},{"location":"howto/deploy/","title":"Deploy and Setup lakeFS","text":"<p>Tip</p> <p>The instructions given here are for a self-managed deployment of lakeFS.  For a hosted lakeFS service with guaranteed SLAs, try lakeFS Cloud</p> <p>This section will guide you through deploying lakeFS on top of an object store. You will require a database, and can optionally configure authentication using providers specific to your deployment platform. </p> <p>Which options are available depends on your deployment platform. For example, the object store available on Azure differs from that on AWS. </p> <p></p>"},{"location":"howto/deploy/#deployment-and-setup-details","title":"Deployment and Setup Details","text":"<p>lakeFS releases include binaries for common operating systems, a containerized option or a Helm chart.</p> <p>Check out our guides below for full deployment details: </p> <ul> <li>AWS</li> <li>Azure</li> <li>GCP</li> <li>On-premises and other cloud providers</li> </ul>"},{"location":"howto/deploy/aws/","title":"Deploy lakeFS on AWS","text":"<p>Tip</p> <p>The instructions given here are for a self-managed deployment of lakeFS on AWS.  For a hosted lakeFS service with guaranteed SLAs, try lakeFS Cloud</p> <p>When you deploy lakeFS on AWS these are the options available to use:</p> <p></p> <p>This guide walks you through the options available and how to configure them, finishing with configuring and running lakeFS itself and creating your first repository.</p> <p>\u23f0 Expected deployment time: 25 min</p>"},{"location":"howto/deploy/aws/#prerequisite-skills-and-access","title":"Prerequisite skills and access","text":"<p>To successfully deploy and operate lakeFS on AWS, you should be comfortable with the following topics and have the corresponding level of access.</p>"},{"location":"howto/deploy/aws/#aws-account-administration","title":"AWS account administration","text":"<ul> <li>Ability to create and modify IAM roles and policies.</li> <li>Ability to create and configure S3 buckets and prefixes.</li> <li>Permissions to provision networking and compute resources in your AWS account (for example, VPC, subnets, security groups, load balancers, EC2, ECS, or EKS).</li> </ul>"},{"location":"howto/deploy/aws/#core-aws-services-used-by-lakefs","title":"Core AWS services used by lakeFS","text":"<ul> <li>Amazon S3 \u2013 bucket layout, prefixes, encryption options, and lifecycle rules.</li> <li>Metadata backend, either:</li> <li>Amazon DynamoDB</li> <li>PostgreSQL / Amazon RDS for PostgreSQL</li> <li>Amazon MemoryDB</li> </ul> <p>For any metadata backend you choose, you should be able to perform configuration, connectivity, storage and persistence configuration, backups, and monitoring.</p>"},{"location":"howto/deploy/aws/#optional-container-and-orchestration-skills","title":"Optional: Container and orchestration skills","text":"<p>Depending on the chosen deployment model: - Kubernetes / Amazon EKS \u2013 understanding of Deployments, Services, Ingress, kubectl, and optionally Helm. - Amazon ECS or EC2 with containers \u2013 understanding task definitions, services, container logs, and IAM roles for tasks.</p>"},{"location":"howto/deploy/aws/#grant-lakefs-permissions-to-dynamodb","title":"Grant lakeFS permissions to DynamoDB","text":"<p>By default, lakeFS will create the required DynamoDB table if it does not already exist. You'll have to give the IAM role used by lakeFS the following permissions:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListAndDescribe\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:List*\",\n                \"dynamodb:DescribeReservedCapacity*\",\n                \"dynamodb:DescribeLimits\",\n                \"dynamodb:DescribeTimeToLive\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"kvstore\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:BatchGet*\",\n                \"dynamodb:DescribeTable\",\n                \"dynamodb:Get*\",\n                \"dynamodb:Query\",\n                \"dynamodb:Scan\",\n                \"dynamodb:BatchWrite*\",\n                \"dynamodb:CreateTable\",\n                \"dynamodb:Delete*\",\n                \"dynamodb:Update*\",\n                \"dynamodb:PutItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/kvstore\"\n        }\n    ]\n}\n</code></pre> <p>Tip</p> <p>You can also use lakeFS with PostgreSQL instead of DynamoDB! See the configuration reference for more information.</p>"},{"location":"howto/deploy/aws/#run-the-lakefs-server","title":"Run the lakeFS server","text":"EC2EKS <p>Connect to your EC2 instance using SSH:</p> <ol> <li>Create a <code>config.yaml</code> on your EC2 instance, with the following parameters:     <pre><code>---\ndatabase:\n    type: \"dynamodb\"\n\nauth:\n    encrypt:\n    # replace this with a randomly-generated string. Make sure to keep it safe!\n    secret_key: \"[ENCRYPTION_SECRET_KEY]\"\n\nblockstore:\n    type: s3\n</code></pre></li> <li>Download the binary to run on the EC2 instance.</li> <li> <p>Run the <code>lakefs</code> binary on the EC2 instance:     <pre><code>lakefs --config config.yaml run\n</code></pre></p> <p>Note</p> <p>It's preferable to run the binary as a service using systemd or your operating system's facilities.</p> </li> </ol> <p>Advanced: Deploying lakeFS behind an AWS Application Load Balancer</p> <ol> <li>Your security groups should allow the load balancer to access the lakeFS server.</li> <li>Create a target group with a listener for port 8000.</li> <li>Setup TLS termination using the domain names you wish to use (e.g., <code>lakefs.example.com</code> and potentially <code>s3.lakefs.example.com</code>, <code>*.s3.lakefs.example.com</code> if using virtual-host addressing).</li> <li>Configure the health-check to use the exposed <code>/_health</code> URL</li> </ol> <p>You can install lakeFS on Kubernetes using a Helm chart.</p> <p>To install lakeFS with Helm:</p> <ol> <li>Copy the Helm values file relevant for S3:     <pre><code>secrets:\n    # replace this with a randomly-generated string\n    authEncryptSecretKey: [ENCRYPTION_SECRET_KEY]\nlakefsConfig: |\n    database:\n        type: dynamodb\n    blockstore:\n        type: s3\n</code></pre></li> <li> <p>Fill in the missing values and save the file as <code>conf-values.yaml</code>. For more configuration options, see our Helm chart README.</p> <p>Note</p> <p>The <code>lakefsConfig</code> parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like <code>databaseConnectionString</code> is given through separate parameters, and the chart will inject it into Kubernetes secrets.</p> </li> <li> <p>In the directory where you created <code>conf-values.yaml</code>, run the following commands:     <pre><code># Add the lakeFS repository\nhelm repo add lakefs https://charts.lakefs.io\n# Deploy lakeFS\nhelm install my-lakefs lakefs/lakefs -f conf-values.yaml\n</code></pre></p> <p>my-lakefs is the Helm Release name.</p> <p>Warning</p> <p>Make sure the Kubernetes nodes have access to all buckets/containers with which you intend to use with lakeFS. If you can't provide such access, configure lakeFS with an AWS key-pair.</p> </li> </ol>"},{"location":"howto/deploy/aws/#load-balancing","title":"Load balancing","text":"<p>To configure a load balancer to direct requests to the lakeFS servers you can use the <code>LoadBalancer</code> Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a <code>/_health</code> endpoint that you can use for health checks.</p> <p>Tip</p> <p>The NGINX Ingress Controller by default limits the client body size to 1 MiB.</p> <p>Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the [S3 Gateway][s3-gateway] or a simple PUT request using the [OpenAPI Server][openapi].</p> <p>Checkout the Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO.</p>"},{"location":"howto/deploy/aws/#prepare-your-s3-bucket","title":"Prepare your S3 bucket","text":"<ol> <li>Take note of the bucket name you want to use with lakeFS</li> <li>Use the following as your bucket policy, filling in the placeholders:</li> </ol> Standard PermissionsStandard Permissions (with s3express)Minimal Permissions (Advanced) <pre><code>{\n    \"Id\": \"lakeFSPolicy\",\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"lakeFSObjects\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:ListMultipartUploadParts\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\"arn:aws:s3:::[BUCKET_NAME_AND_PREFIX]/*\"],\n            \"Principal\": {\n                \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"]\n            }\n        },\n        {\n            \"Sid\": \"lakeFSBucket\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucketMultipartUploads\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\"arn:aws:s3:::[BUCKET]\"],\n            \"Principal\": {\n                \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"]\n            }\n        }\n    ]\n}\n</code></pre> <ul> <li>Replace <code>[BUCKET_NAME]</code>, <code>[ACCOUNT_ID]</code> and <code>[IAM_ROLE]</code> with values relevant to your environment.</li> <li><code>[BUCKET_NAME_AND_PREFIX]</code> can be the bucket name. If you want to minimize the bucket policy permissions, use the bucket name together with a prefix (e.g. <code>example-bucket/a/b/c</code>). This way, lakeFS will be able to create repositories only under this specific path (see: [Storage Namespace][understand-repository]).</li> <li>lakeFS will try to assume the role <code>[IAM_ROLE]</code>.</li> </ul> <p>To use an S3 Express One Zone directory bucket, use the following policy. Note the <code>lakeFSDirectoryBucket</code> statement which is specifically required for using a directory bucket.</p> <pre><code>{\n    \"Id\": \"lakeFSPolicy\",\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"lakeFSObjects\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:ListMultipartUploadParts\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\"arn:aws:s3:::[BUCKET_NAME_AND_PREFIX]/*\"],\n            \"Principal\": {\n                \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"]\n            }\n        },\n        {\n            \"Sid\": \"lakeFSBucket\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucketMultipartUploads\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\"arn:aws:s3:::[BUCKET]\"],\n            \"Principal\": {\n                \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"]\n            }\n        },\n        {\n            \"Sid\": \"lakeFSDirectoryBucket\",\n            \"Action\": [\n                \"s3express:CreateSession\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3express:[REGION]:[ACCOUNT_ID]:bucket/[BUCKET_NAME]\"\n        }\n    ]\n}\n</code></pre> <ul> <li>Replace <code>[BUCKET_NAME]</code>, <code>[ACCOUNT_ID]</code> and <code>[IAM_ROLE]</code> with values relevant to your environment.</li> <li><code>[BUCKET_NAME_AND_PREFIX]</code> can be the bucket name. If you want to minimize the bucket policy permissions, use the bucket name together with a prefix (e.g. <code>example-bucket/a/b/c</code>). This way, lakeFS will be able to create repositories only under this specific path (see: [Storage Namespace][understand-repository]).</li> <li>lakeFS will try to assume the role <code>[IAM_ROLE]</code>.</li> </ul> <p>If required lakeFS can operate without accessing the data itself, this permission section is useful if you are using presigned URLs mode or the lakeFS Hadoop FileSystem Spark integration. Since this FileSystem performs many operations directly on the storage, lakeFS requires less permissive permissions, resulting in increased security.</p> <p>lakeFS always requires permissions to access the <code>_lakefs</code> prefix under your storage namespace, in which metadata is stored ([learn more][understand-commits]).</p> <p>By setting this policy without presign mode you'll be able to perform only metadata operations through lakeFS, meaning that you'll not be able to use lakeFS to upload or download objects. Specifically you won't be able to:</p> <ul> <li>Upload objects using the lakeFS GUI (Works with presign mode)</li> <li>Upload objects through Spark using the S3 gateway</li> <li>Run <code>lakectl fs</code> commands (unless using presign mode with <code>--pre-sign</code> flag)</li> <li>Use Actions and Hooks</li> </ul> <pre><code>{\n    \"Id\": \"[POLICY_ID]\",\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"lakeFSObjects\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::[STORAGE_NAMESPACE]/_lakefs/*\"\n            ],\n            \"Principal\": {\n                \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"]\n            }\n        },\n        {\n            \"Sid\": \"lakeFSBucket\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\"arn:aws:s3:::[BUCKET]\"],\n            \"Principal\": {\n                \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"]\n            }\n        }\n    ]\n}\n</code></pre> <p>We can use presigned URLs mode without allowing access to the data from the lakeFS server directly. We can achieve this by using condition keys such as aws:referer, aws:SourceVpc, aws:SourceVpcArn and aws:SourceIp.</p> <p>For example, assume the following scenario</p> <ul> <li>lakeFS is deployed outside the company (i.e lakeFS cloud or other VPC not <code>vpc-123</code>)</li> <li>We don't want lakeFS to be able to access the data, so we use presign URL, we still need lakeFS role to be able to sign the URL.</li> <li>We want to allow access from the internal company VPC: <code>vpc-123</code>.</li> <li>Both lakeFS and company bucket are deployed in the same region, in that case <code>us-east-1</code>.</li> </ul> <pre><code>{\n    \"Sid\": \"allowLakeFSRoleFromCompanyOnly\",\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"AWS\": \"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"\n    },\n    \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::[BUCKET]/*\",\n    ],\n    \"Condition\": {\n        \"StringEquals\": {\n            \"aws:SourceVpcArn\": \"arn:aws:ec2:us-east-1:*:vpc/vpc-123\"\n        }\n    }\n}\n</code></pre>"},{"location":"howto/deploy/aws/#s3-storage-tier-classes","title":"S3 Storage Tier Classes","text":"<p>lakeFS currently supports the following S3 Storage Classes:</p> <ol> <li>S3 Standard - The default AWS S3 storage tier. Fully supported.</li> <li>S3 Express One-Zone - Fully supported.</li> <li>S3 Glacier Instant Retrival - Supported with limitations: currently, pre-signed URLs are not supported when using Instant Retrival. The outstanding feature request could be tracked here.</li> </ol> <p>Other storage classes are currently unsupported - either because they have not been tested with lakeFS or because they cannot be supported.</p> <p>If you need lakeFS to support a storage tier that isn't currently on the supported list, please open an issue on GitHub.</p>"},{"location":"howto/deploy/aws/#alternative-use-an-aws-user","title":"Alternative: use an AWS user","text":"<p>lakeFS can authenticate with your AWS account using an AWS user, using an access key and secret. To allow this, change the policy's Principal accordingly:</p> <pre><code> \"Principal\": {\n   \"AWS\": [\"arn:aws:iam::&lt;ACCOUNT_ID&gt;:user/&lt;IAM_USER&gt;\"]\n }\n</code></pre>"},{"location":"howto/deploy/aws/#create-the-admin-user","title":"Create the admin user","text":"<p>When you first open the lakeFS UI, you will be asked to create an initial admin user.</p> <ol> <li>Open <code>http://&lt;lakefs-host&gt;/</code> in your browser. If you haven't set up a load balancer, this will likely be <code>http://&lt;instance ip address&gt;:8000/</code></li> <li>On first use, you'll be redirected to the setup page:    </li> <li>Follow the steps to create an initial administrator user. Save the credentials you\u2019ve received somewhere safe, you won\u2019t be able to see them again!    </li> <li>Follow the link and go to the login screen. Use the credentials from the previous step to log in.</li> </ol>"},{"location":"howto/deploy/aws/#create-your-first-repository","title":"Create your first repository","text":"<ol> <li>Use the credentials from the previous step to log in</li> <li>Click Create Repository and choose Blank Repository.    </li> <li>Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored.</li> <li>Click Create Repository</li> <li>You should now have a configured repository, ready to use!    </li> </ol> <p>Congratulations</p> <p>Your environment is now ready \ud83e\udd29</p>"},{"location":"howto/deploy/azure/","title":"Deploy lakeFS on Azure","text":"<p>Tip</p> <p>The instructions given here are for a self-managed deployment of lakeFS on Azure.  For a hosted lakeFS service with guaranteed SLAs, try lakeFS Cloud</p> <p>When you deploy lakeFS on Azure these are the options available to use:</p> <p></p> <p>This guide walks you through the options available and how to configure them, finishing with configuring and running lakeFS itself and creating your first repository.</p> <p>\u23f0 Expected deployment time: 25 min</p>"},{"location":"howto/deploy/azure/#object-storage","title":"Object Storage","text":"<p>lakeFS supports the following Azure Storage types:</p> <ol> <li>Azure Blob Storage</li> <li>Azure Data Lake Storage Gen2 (HNS)</li> </ol> <p>Data Lake Storage Gen1 is not supported.</p>"},{"location":"howto/deploy/azure/#authentication-method","title":"Authentication Method","text":"<p>lakeFS supports two ways to authenticate with Azure.</p> Identity Based Authentication (recommended)Storage Account Credentials <p>lakeFS uses environment variables to determine credentials to use for authentication. The following authentication methods are supported:</p> <ol> <li>Managed Service Identity (MSI)</li> <li>Service Principal RBAC</li> <li>Azure CLI</li> </ol> <p>For deployments inside the Azure ecosystem it is recommended to use a managed identity.</p> <p>More information on authentication methods and environment variables can be found here</p> <p>Storage account credentials can be set directly in the lakeFS configuration using the following parameters:</p> <ul> <li><code>blockstore.azure.storage_account</code></li> <li><code>blockstore.azure.storage_access_key</code></li> </ul> <p>Limitations</p> <p>Please note that using this authentication method limits lakeFS to the scope of the given storage account.</p> <p>Specifically, the following operations will not work:</p> <ol> <li>Import of data from different storage accounts</li> <li>Copy/Read/Write of data that was imported from a different storage account</li> <li>Create pre-signed URL for data that was imported from a different storage account</li> </ol>"},{"location":"howto/deploy/azure/#how-to-create-service-principal-for-resource-group","title":"How to Create Service Principal for Resource Group","text":"<p>It is recommended to create a resource group that consists of all the resources lakeFS should have access to.</p> <p>Using a resource group will allow dynamic removal/addition of services from the group, effectively providing/preventing access for lakeFS to these resources without requiring any changes in configuration in lakeFS or providing lakeFS with any additional credentials.</p> <p>The minimal role required for the service principal is \"Storage Blob Data Contributor\"</p> <p>The following Azure CLI command creates a service principal for a resource group called \"lakeFS\" with permission to access (read/write/delete) Blob Storage resources in the resource group and with an expiry of 5 years</p> <pre><code>az ad sp create-for-rbac \\\n--role \"Storage Blob Data Contributor\" \\\n--scopes /subscriptions/XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/resourceGroups/lakeFS --years 5\n\nCreating 'Storage Blob Data Contributor' role assignment under scope '/subscriptions/947382ea-681a-4541-99ab-b718960c6289/resourceGroups/lakeFS'\nThe output includes credentials that you must protect. Be sure that you do not include these credentials in your code or check the credentials into your source control. For more information, see https://aka.ms/azadsp-cli\n{\n\"appId\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\",\n\"displayName\": \"azure-cli-2023-01-30-06-18-30\",\n\"password\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\",\n\"tenant\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n}\n</code></pre> <p>The command output should be used to populate the following environment variables:</p> <pre><code>AZURE_CLIENT_ID      =  $appId\nAZURE_TENANT_ID      =  $tenant\nAZURE_CLIENT_SECRET  =  $password\n</code></pre> <p>Danger</p> <p>Service Principal credentials have an expiry date and lakeFS will lose access to resources unless credentials are renewed on time.</p> <p>Info</p> <p>It is possible to provide both account based credentials and environment variables to lakeFS. In that case - lakeFS will use the account credentials for any access to data located in the given account, and will try to use the identity credentials for any data located outside the given account.</p>"},{"location":"howto/deploy/azure/#kv-store","title":"K/V Store","text":"<p>lakeFS stores metadata in a database for its versioning engine. This is done via a Key-Value interface that can be implemented on any DB engine and lakeFS comes with several built-in driver implementations (You can read more about it here).</p> <p>The database used doesn't have to be a dedicated K/V database.</p> CosmosDBPostgreSQL <p>CosmosDB is a managed database service provided by Azure.</p> <p>lakeFS supports CosmosDB For NoSQL as a database backend.</p> <ol> <li>Follow the official Azure documentation on how to create a CosmosDB account for NoSQL and connect to it.</li> <li>Once your CosmosDB account is set up, you can create a Database for lakeFS. For lakeFS ACID guarantees, make sure to select the Bounded staleness consistency, for single region deployments.</li> <li>Create a new container in the database and select type <code>partitionKey</code> as the Partition key (case sensitive).</li> <li>Pass the endpoint, database name and container name to lakeFS as described in the [configuration guide][config-reference-azure-block]. You can either pass the CosmosDB's account read-write key to lakeFS, or use a managed identity to authenticate to CosmosDB, as described earlier.</li> </ol> <p>Below we show you how to create a database on Azure Database, but you can use any PostgreSQL database as long as it's accessible by your lakeFS installation.</p> <p>If you already have a database, take note of the connection string and skip to the next step</p> <ol> <li>Follow the official Azure documentation on how to create a PostgreSQL instance and connect to it. Make sure that you're using PostgreSQL version &gt;= 11.</li> <li>Once your Azure Database for PostgreSQL server is set up and the server is in the Available state, take note of the endpoint and username. </li> <li>Make sure your Access control roles allow you to connect to the database instance.</li> </ol>"},{"location":"howto/deploy/azure/#4-run-the-lakefs-server","title":"4. Run the lakeFS server","text":"<p>Now that you've chosen and configured object storage, a K/V store, and authentication\u2014you're ready to configure and run lakeFS. There are three different ways you can run lakeFS:</p> Azure VMDockerAzure Kubernetes Service (AKS) <p>Connect to your VM instance using SSH:</p> <ol> <li>Create a <code>config.yaml</code> on your VM, with the following parameters: <pre><code>---\ndatabase:\n    type: \"postgres\"\n    postgres:\n    connection_string: \"[DATABASE_CONNECTION_STRING]\"\n\nauth:\n    encrypt:\n    # replace this with a randomly-generated string. Make sure to keep it safe!\n    secret_key: \"[ENCRYPTION_SECRET_KEY]\"\n\nblockstore:\n    type: azure\n    azure:\n</code></pre></li> <li>Download the binary to run on the VM.</li> <li>Run the <code>lakefs</code> binary: <pre><code>lakefs --config config.yaml run\n</code></pre></li> </ol> <p>Note</p> <p>It's preferable to run the binary as a service using systemd or your operating system's facilities.</p> <p>To support container-based environments, you can configure lakeFS using environment variables. Here is a <code>docker run</code> command to demonstrate starting lakeFS using Docker:</p> <pre><code>docker run \\\n    --name lakefs \\\n    -p 8000:8000 \\\n    -e LAKEFS_DATABASE_TYPE=\"postgres\" \\\n    -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\\n    -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\\n    -e LAKEFS_BLOCKSTORE_TYPE=\"azure\" \\\n    -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT=\"[YOUR_STORAGE_ACCOUNT]\" \\\n    -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY=\"[YOUR_ACCESS_KEY]\" \\\n    treeverse/lakefs:latest run\n</code></pre> <p>See the [reference][config-envariables] for a complete list of environment variables.</p> <p>You can install lakeFS on Kubernetes using a Helm chart.</p> <p>To install lakeFS with Helm:</p> <ol> <li>Copy the Helm values file relevant for Azure Blob:</li> </ol> <p><pre><code>secrets:\n    # replace this with the connection string of the database you created in a previous step:\n    databaseConnectionString: [DATABASE_CONNECTION_STRING]\n    # replace this with a randomly-generated string\n    authEncryptSecretKey: [ENCRYPTION_SECRET_KEY]\nlakefsConfig: |\n    blockstore:\n        type: azure\n        azure:\n    #  If you chose to authenticate via access key, unmark the following rows and insert the values from the previous step\n    #  storage_account: [your storage account]\n    #  storage_access_key: [your access key]\n</code></pre> 1. Fill in the missing values and save the file as <code>conf-values.yaml</code>. For more configuration options, see our Helm chart README.</p> <p>Note</p> <p>The <code>lakefsConfig</code> parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like <code>databaseConnectionString</code> is given through separate parameters, and the chart will inject it into Kubernetes secrets.</p> <ol> <li>In the directory where you created <code>conf-values.yaml</code>, run the following commands: <pre><code># Add the lakeFS repository\nhelm repo add lakefs https://charts.lakefs.io\n# Deploy lakeFS\nhelm install my-lakefs lakefs/lakefs -f conf-values.yaml\n</code></pre> my-lakefs is the Helm Release name.</li> </ol>"},{"location":"howto/deploy/azure/#load-balancing","title":"Load balancing","text":"<p>To configure a load balancer to direct requests to the lakeFS servers you can use the <code>LoadBalancer</code> Service type or a Kubernetes Ingress. By default, lakeFS operates on port <code>8000</code> and exposes a <code>/_health</code> endpoint that you can use for health checks.</p> <p>Info</p> <p>The NGINX Ingress Controller by default limits the client body size to 1 MiB.</p> <p>Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the [S3-compatible Gateway][s3-gateway] or a simple PUT request using the [OpenAPI Server][openapi].</p> <p>Check out the Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO.</p>"},{"location":"howto/deploy/azure/#create-the-admin-user","title":"Create the admin user","text":"<p>When you first open the lakeFS UI, you will be asked to create an initial admin user.</p> <ol> <li>Open <code>http://&lt;lakefs-host&gt;/</code> in your browser. If you haven't set up a load balancer, this will likely be <code>http://&lt;instance ip address&gt;:8000/</code></li> <li>On first use, you'll be redirected to the setup page:    </li> <li>Follow the steps to create an initial administrator user. Save the credentials you\u2019ve received somewhere safe, you won\u2019t be able to see them again!    </li> <li>Follow the link and go to the login screen. Use the credentials from the previous step to log in.</li> </ol>"},{"location":"howto/deploy/azure/#create-your-first-repository","title":"Create your first repository","text":"<ol> <li>Use the credentials from the previous step to log in</li> <li>Click Create Repository and choose Blank Repository.    </li> <li>Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored.</li> <li>Click Create Repository</li> <li>You should now have a configured repository, ready to use!    </li> </ol> <p>Congratulations</p> <p>Your environment is now ready \ud83e\udd29</p>"},{"location":"howto/deploy/gcp/","title":"Deploy lakeFS on GCP","text":"<p>Tip</p> <p>The instructions given here are for a self-managed deployment of lakeFS on GCP.  For a hosted lakeFS service with guaranteed SLAs, please contact us for details of lakeFS Cloud on GCP.</p> <p>When you deploy lakeFS on GCP these are the options available to use:</p> <p></p> <p>\u23f0 Expected deployment time: 25 min</p>"},{"location":"howto/deploy/gcp/#create-a-database","title":"Create a Database","text":"<p>lakeFS requires a PostgreSQL database to synchronize actions on your repositories. We will show you how to create a database on Google Cloud SQL, but you can use any PostgreSQL database as long as it's accessible by your lakeFS installation.</p> <p>If you already have a database, take note of the connection string and skip to the next step</p> <ol> <li>Follow the official Google documentation on how to create a PostgreSQL instance.    Make sure you're using PostgreSQL version &gt;= 11.</li> <li>On the Users tab in the console, create a user. The lakeFS installation will use it to connect to your database.</li> <li>Choose the method by which lakeFS will connect to your database. Google recommends using    the SQL Auth Proxy.</li> </ol>"},{"location":"howto/deploy/gcp/#run-the-lakefs-server","title":"Run the lakeFS Server","text":"GCE InstanceDockerGKE <ol> <li>Save the following configuration file as <code>config.yaml</code>:</li> </ol> <pre><code>---\ndatabase:\n    type: \"postgres\"\n    postgres:\n    connection_string: \"[DATABASE_CONNECTION_STRING]\"\nauth:\n    encrypt:\n    # replace this with a randomly-generated string:\n    secret_key: \"[ENCRYPTION_SECRET_KEY]\"\nblockstore:\n    type: gs\n    # Uncomment the following lines to give lakeFS access to your buckets using a service account:\n    # gs:\n    #   credentials_json: [YOUR SERVICE ACCOUNT JSON STRING]\n</code></pre> <ol> <li>Download the binary to run on the GCE instance.</li> <li>Run the <code>lakefs</code> binary on the GCE machine: <pre><code>lakefs --config config.yaml run\n</code></pre> Note: it is preferable to run the binary as a service using systemd or your operating system's facilities.</li> </ol> <p>To support container-based environments like Google Cloud Run, lakeFS can be configured using environment variables. Here is a <code>docker run</code> command to demonstrate starting lakeFS using Docker:</p> <pre><code>docker run \\\n    --name lakefs \\\n    -p 8000:8000 \\\n    -e LAKEFS_DATABASE_TYPE=\"postgres\" \\\n    -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\\n    -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\\n    -e LAKEFS_BLOCKSTORE_TYPE=\"gs\" \\\n    treeverse/lakefs:latest run\n</code></pre> <p>See the [reference][config-envariables] for a complete list of environment variables.</p> <p>You can install lakeFS on Kubernetes using a Helm chart.</p> <p>To install lakeFS with Helm:</p> <ol> <li>Copy the Helm values file relevant for Google Storage:</li> </ol> <p><pre><code>secrets:\n    # replace DATABASE_CONNECTION_STRING with the connection string of the database you created in a previous step.\n    # e.g.: postgres://postgres:myPassword@localhost/postgres:5432\n    databaseConnectionString: [DATABASE_CONNECTION_STRING]\n    # replace this with a randomly-generated string\n    authEncryptSecretKey: [ENCRYPTION_SECRET_KEY]\nlakefsConfig: |\n    blockstore:\n        type: gs\n        # Uncomment the following lines to give lakeFS access to your buckets using a service account:\n        # gs:\n        #   credentials_json: [YOUR SERVICE ACCOUNT JSON STRING]\n</code></pre> 1. Fill in the missing values and save the file as <code>conf-values.yaml</code>. For more configuration options, see our Helm chart README.</p> <p>Note</p> <p>The <code>lakefsConfig</code> parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like <code>databaseConnectionString</code> is given through separate parameters, and the chart will inject it into Kubernetes secrets.</p> <ol> <li>In the directory where you created <code>conf-values.yaml</code>, run the following commands:</li> </ol> <pre><code># Add the lakeFS repository\nhelm repo add lakefs https://charts.lakefs.io\n# Deploy lakeFS\nhelm install my-lakefs lakefs/lakefs -f conf-values.yaml\n</code></pre> <p>my-lakefs is the Helm Release name.</p>"},{"location":"howto/deploy/gcp/#load-balancing","title":"Load balancing","text":"<p>To configure a load balancer to direct requests to the lakeFS servers you can use the <code>LoadBalancer</code> Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a <code>/_health</code> endpoint that you can use for health checks.</p> <p>Tip</p> <p>The NGINX Ingress Controller by default limits the client body size to 1 MiB.</p> <p>Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the [S3-compatible Gateway][s3-gateway] or a simple PUT request using the [OpenAPI Server][openapi].</p> <p>Checkout the Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO.</p>"},{"location":"howto/deploy/gcp/#create-the-admin-user","title":"Create the admin user","text":"<p>When you first open the lakeFS UI, you will be asked to create an initial admin user.</p> <ol> <li>Open <code>http://&lt;lakefs-host&gt;/</code> in your browser. If you haven't set up a load balancer, this will likely be <code>http://&lt;instance ip address&gt;:8000/</code></li> <li>On first use, you'll be redirected to the setup page:    </li> <li>Follow the steps to create an initial administrator user. Save the credentials you\u2019ve received somewhere safe, you won\u2019t be able to see them again!    </li> <li>Follow the link and go to the login screen. Use the credentials from the previous step to log in.</li> </ol>"},{"location":"howto/deploy/gcp/#configure-minimal-permissions-for-your-gcs-bucket","title":"Configure minimal permissions for your GCS bucket","text":"<p>If required, lakeFS can operate without accessing the data itself. This permission model is useful if you are running a zero trust architecture, using [presigned URLs mode][presigned-url], or the [lakeFS Hadoop FileSystem Spark integration][integration-hadoopfs].</p> <p>Dedicated GCP Project Recommended</p> <p>Due to the nature of VPC Service Controls which place a perimeter around the entire GCS service, it's strongly recommended to use a dedicated GCP project for your lakeFS bucket. This simplifies permission and access management significantly.</p>"},{"location":"howto/deploy/gcp/#architecture-overview","title":"Architecture Overview","text":"<p>This setup uses two service accounts:</p> <ol> <li>Metadata Service Account (SA_OPEN): Accesses bucket prefixes in the form of  <code>gs://&lt;bucket-name&gt;/&lt;prefix&gt;/_lakefs/</code> from anywhere.</li> <li>Data Service Account (SA_RESTRICTED): Accesses all data except bucket prefixes in the form of <code>gs://&lt;bucket-name&gt;/&lt;prefix&gt;/_lakefs/</code>, restricted by network using VPC Service Controls.</li> </ol> <p>lakeFS always requires permissions to access the <code>_lakefs</code> prefix under your storage namespace, where metadata is stored.</p>"},{"location":"howto/deploy/gcp/#limitations","title":"Limitations","text":"<p>This configuration supports only presign mode. This means that you won't be able to:</p> <ul> <li>Upload objects using the lakeFS Web UI (presign mode is configurable)</li> <li>Upload objects through lakeFS S3 Gateway</li> <li>Run <code>lakectl fs</code> commands</li> </ul>"},{"location":"howto/deploy/gcp/#setup-steps","title":"Setup Steps","text":""},{"location":"howto/deploy/gcp/#1-create-service-accounts","title":"1. Create Service Accounts","text":"<p>Set your environment variables:</p> <pre><code>export PROJECT_ID=\"[YOUR_PROJECT_ID]\"\nexport SA_OPEN_ID=\"lakefs-metadata\"\nexport SA_RESTRICTED_ID=\"lakefs-data\"\nexport BUCKET=\"[YOUR_BUCKET_NAME]\"\n</code></pre> <p>Create the service accounts:</p> <pre><code># Metadata service account (accesses _lakefs/** from anywhere)\ngcloud iam service-accounts create \"${SA_OPEN_ID}\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --description=\"lakeFS metadata - RW only under /_lakefs/** (from anywhere)\" \\\n  --display-name=\"lakeFS Metadata Service Account\"\n\n# Data service account (accesses everything except _lakefs/**, network-restricted)\ngcloud iam service-accounts create \"${SA_RESTRICTED_ID}\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --description=\"lakeFS data - RW everywhere except /_lakefs/** (network-restricted)\" \\\n  --display-name=\"lakeFS Data Service Account\"\n</code></pre> <p>Get the full service account emails:</p> <pre><code>export SA_OPEN=\"${SA_OPEN_ID}@${PROJECT_ID}.iam.gserviceaccount.com\"\nexport SA_RESTRICTED=\"${SA_RESTRICTED_ID}@${PROJECT_ID}.iam.gserviceaccount.com\"\n</code></pre>"},{"location":"howto/deploy/gcp/#2-configure-bucket-iam-policies","title":"2. Configure Bucket IAM Policies","text":"<p>In this step, we will configure the IAM policies on the bucket level to grant the necessary permissions for the service accounts.</p> <p>Grant the metadata service account access to <code>gs://&lt;bucket-name&gt;/&lt;prefix&gt;/_lakefs/</code> prefix only:</p> <pre><code>gcloud storage buckets add-iam-policy-binding \"gs://${BUCKET}\" \\\n  --member=\"serviceAccount:${SA_OPEN}\" \\\n  --role=\"roles/storage.objectAdmin\" \\\n  --condition='title=lakefs-metadata-only,description=Access_only_to_lakefs_prefix,expression=resource.type == \"storage.googleapis.com/Object\" &amp;&amp; resource.name.extract(\"/objects/{prefix}/_lakefs/\") != \"\"'\n</code></pre> <p>Grant the data service account access to everything except <code>gs://&lt;bucket-name&gt;/&lt;prefix&gt;/_lakefs/</code> prefix:</p> <pre><code># Object access (read/write) for non-_lakefs paths\ngcloud storage buckets add-iam-policy-binding \"gs://${BUCKET}\" \\\n  --member=\"serviceAccount:${SA_RESTRICTED}\" \\\n  --role=\"roles/storage.objectAdmin\" \\\n  --condition='title=lakefs-data-only,description=Access_except_lakefs_prefix,expression=resource.type == \"storage.googleapis.com/Object\" &amp;&amp; resource.name.extract(\"/objects/{prefix}/_lakefs/\") == \"\"'\n</code></pre> <p>Create a custom IAM role for listing objects and grant it to the data service account:</p> <pre><code># Create custom role for listing objects only\ngcloud iam roles create lakefsDataListOnly \\\n  --project=\"${PROJECT_ID}\" \\\n  --title=\"lakeFS Data List Only\" \\\n  --description=\"Custom role for lakeFS data service account to list objects\" \\\n  --permissions=\"storage.objects.list\"\n\n# Grant the custom role to the data service account\ngcloud projects add-iam-policy-binding \"${PROJECT_ID}\" \\\n  --member=\"serviceAccount:${SA_RESTRICTED}\" \\\n  --role=\"projects/${PROJECT_ID}/roles/lakefsDataListOnly\"\n</code></pre>"},{"location":"howto/deploy/gcp/#3-set-up-vpc-service-controls","title":"3. Set Up VPC Service Controls","text":"<p>Create an access level that defines your allowed networks (adjust IP ranges and VPC as needed):</p> <pre><code># Get your organization's access policy ID\nexport ORG_ID=\"[YOUR_ORG_ID]\"\nexport POLICY_ID=$(gcloud access-context-manager policies list \\\n  --organization=\"${ORG_ID}\" \\\n  --format=\"value(name)\")\n\n# Create an access level for your allowed networks\ncat &gt; access-level.yaml &lt;&lt;EOF\n- ipSubnetworks:\n  - \"[YOUR_ALLOWED_IP_CIDR]\"\n  vpcNetworkSources:\n  - vpcSubnetwork:\n      network: \"projects/[YOUR_VPC_PROJECT]/global/networks/[YOUR_VPC_NAME]\"\nEOF\n\ngcloud access-context-manager levels create Restrict_Network_Access \\\n  --policy=\"${POLICY_ID}\" \\\n  --title=\"Restrict Network Access to Approved IPs and VPCs\" \\\n  --combine-function=OR \\\n  --basic-level-spec=access-level.yaml\n</code></pre>"},{"location":"howto/deploy/gcp/#4-create-ingress-policy","title":"4. Create Ingress Policy","text":"<p>Define the ingress policy for the restricted service account:</p> <pre><code>export ACCESS_LEVEL=\"accessPolicies/${POLICY_ID}/accessLevels/Restrict_Network_Access\"\n\ncat &gt; ingress-policy.yaml &lt;&lt;EOF\n- ingressFrom:\n    identities:\n    - serviceAccount:${SA_RESTRICTED}\n    sources:\n    - accessLevel: ${ACCESS_LEVEL}\n  ingressTo:\n    operations:\n    - serviceName: storage.googleapis.com\n    resources:\n    - '*'\n  title: lakeFS Data Service Account Ingress\nEOF\n</code></pre>"},{"location":"howto/deploy/gcp/#5-create-vpc-service-controls-perimeter","title":"5. Create VPC Service Controls Perimeter","text":"<pre><code>gcloud access-context-manager perimeters create lakefs_perimeter \\\n  --policy=\"${POLICY_ID}\" \\\n  --title=\"lakeFS Security Perimeter\" \\\n  --resources=\"projects/${PROJECT_ID}\" \\\n  --restricted-services=\"storage.googleapis.com\" \\\n  --ingress-policies=ingress-policy.yaml\n</code></pre>"},{"location":"howto/deploy/gcp/#6-update-lakefs-configuration","title":"6. Update lakeFS configuration","text":"<p>In your lakeFS configuration, update the credentials for the blockstore to use the metadata and data service accounts:</p> <pre><code>blockstore:\n  type: gs\n  gs:\n    credentials_json: [SA_OPEN_SERVICE_ACCOUNT_JSON]\n    # Alternatively, you can use a file path:\n    # data_credentials_file: /path/to/sa_restricted_service_account.json\n</code></pre> <p>For data operations, your clients should use the data service account (SA_RESTRICTED) credentials and must access from within the allowed networks and VPCs defined in your VPC Service Controls.</p>"},{"location":"howto/deploy/gcp/#network-access-control","title":"Network Access Control","text":"<p>With this setup:</p> <ul> <li>lakeFS server uses SA_OPEN to access metadata (<code>_lakefs/</code>) from any network</li> <li>Data access through SA_RESTRICTED is only permitted from approved IP addresses and VPCs defined in the VPC Service Controls access level</li> <li>Clients accessing data must be within the allowed network perimeter</li> </ul> <p>This provides similar security to AWS's condition keys (like <code>aws:SourceVpc</code> and <code>aws:SourceIp</code>) but using GCP's VPC Service Controls.</p>"},{"location":"howto/deploy/gcp/#create-your-first-repository","title":"Create your first repository","text":"<ol> <li>Use the credentials from the previous step to log in</li> <li>Click Create Repository and choose Blank Repository.    </li> <li>Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored.</li> <li>Click Create Repository</li> <li>You should now have a configured repository, ready to use!    </li> </ol> <p>Congratulations</p> <p>Your environment is now ready \ud83e\udd29</p>"},{"location":"howto/deploy/onprem/","title":"On-Premises Deployment","text":"<p>Tip</p> <p>The instructions given here are for a self-managed deployment of lakeFS. For a hosted lakeFS service with guaranteed SLAs, try lakeFS Cloud</p> <p>\u23f0 Expected deployment time: 25 min</p>"},{"location":"howto/deploy/onprem/#prerequisites","title":"Prerequisites","text":"<p>To use lakeFS on-premises, you can either use the local blockstore adapter or have access to an S3-compatible object store such as MinIO.</p> <p>For more information on how to set up MinIO, see the official deployment guide</p>"},{"location":"howto/deploy/onprem/#setting-up-a-database","title":"Setting up a database","text":"<p>lakeFS requires a PostgreSQL database to synchronize actions on your repositories. This section assumes that you already have a PostgreSQL &gt;= 11.0 database accessible.</p>"},{"location":"howto/deploy/onprem/#setting-up-a-lakefs-server","title":"Setting up a lakeFS Server","text":"LinuxDockerKubernetes <p>Connect to your host using SSH: 1. Create a <code>config.yaml</code> on your VM, with the following parameters: <pre><code>---\ndatabase:\n    type: \"postgres\"\n    postgres:\n    connection_string: \"[DATABASE_CONNECTION_STRING]\"\n\nauth:\n    encrypt:\n    # replace this with a randomly-generated string. Make sure to keep it safe!\n    secret_key: \"[ENCRYPTION_SECRET_KEY]\"\n\nblockstore:\n    type: s3\n    s3:\n        force_path_style: true\n        endpoint: http://&lt;minio_endpoint&gt;\n        discover_bucket_region: false\n        credentials:\n        access_key_id: &lt;minio_access_key&gt;\n        secret_access_key: &lt;minio_secret_key&gt;\n</code></pre></p> <p>Info</p> <p>Notice that the lakeFS Blockstore type is set to <code>s3</code> - This configuration works with S3-compatible storage engines such as MinIO.</p> <ol> <li>Download the binary to the server.</li> <li>Run the <code>lakefs</code> binary: <pre><code>lakefs --config config.yaml run\n</code></pre></li> </ol> <p>Note</p> <p>It's preferable to run the binary as a service using systemd or your operating system's facilities.</p> <p>To support container-based environments, you can configure lakeFS using environment variables. Here is a <code>docker run</code> command to demonstrate starting lakeFS using Docker:</p> <pre><code>docker run \\\n    --name lakefs \\\n    -p 8000:8000 \\\n    -e LAKEFS_DATABASE_TYPE=\"postgres\" \\\n    -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\\n    -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\\n    -e LAKEFS_BLOCKSTORE_TYPE=\"s3\" \\\n    -e LAKEFS_BLOCKSTORE_S3_FORCE_PATH_STYLE=\"true\" \\\n    -e LAKEFS_BLOCKSTORE_S3_ENDPOINT=\"http://&lt;minio_endpoint&gt;\" \\\n    -e LAKEFS_BLOCKSTORE_S3_DISCOVER_BUCKET_REGION=\"false\" \\\n    -e LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID=\"&lt;minio_access_key&gt;\" \\\n    -e LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY=\"&lt;minio_secret_key&gt;\" \\\n    treeverse/lakefs:latest run\n</code></pre> <p>Info</p> <p>Notice that the lakeFS Blockstore type is set to <code>s3</code> - This configuration works with S3-compatible storage engines such as MinIO.</p> <p>Local Database and Local Blockstore</p> <p>When using local database (<code>LAKEFS_DATABASE_TYPE=local</code>) and local blockstore (<code>LAKEFS_BLOCKSTORE_TYPE=local</code>),  lakeFS stores all data and metadata under <code>${HOME}/lakefs</code> in the container by default (configurable). You can mount this location to the host using Docker volumes to persist data and metadata on host storage:</p> <pre><code>docker run \\\n    --name lakefs \\\n    -p 8000:8000 \\\n    -v /path/on/host:/home/lakefs/lakefs \\\n    -e LAKEFS_DATABASE_TYPE=\"local\" \\\n    -e LAKEFS_BLOCKSTORE_TYPE=\"local\" \\\n    -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\\n    treeverse/lakefs:latest run\n</code></pre> <p>See the [reference][config-envariables] for a complete list of environment variables.</p> <p>You can install lakeFS on Kubernetes using a Helm chart.</p> <p>To install lakeFS with Helm:</p> <ol> <li> <p>Copy the Helm values file relevant for S3-Compatible storage (MinIO in this example):</p> <pre><code>secrets:\n    # replace this with the connection string of the database you created in a previous step:\n    databaseConnectionString: [DATABASE_CONNECTION_STRING]\n    # replace this with a randomly-generated string\n    authEncryptSecretKey: [ENCRYPTION_SECRET_KEY]\nlakefsConfig: |\n    blockstore:\n        type: s3\n        s3:\n        force_path_style: true\n        endpoint: http://&lt;minio_endpoint&gt;\n        discover_bucket_region: false\n        credentials:\n            access_key_id: &lt;minio_access_key&gt;\n            secret_access_key: &lt;minio_secret_key&gt;\n</code></pre> <p>Tip</p> <p>Notice that the lakeFS Blockstore type is set to <code>s3</code> - This configuration works with S3-compatible storage engines such as MinIO</p> </li> <li> <p>Fill in the missing values and save the file as <code>conf-values.yaml</code>. For more configuration options, see our Helm chart README.     !!! note     The <code>lakefsConfig</code> parameter is the lakeFS configuration documented here but without sensitive information.     Sensitive information like <code>databaseConnectionString</code> is given through separate parameters, and the chart will inject it into Kubernetes secrets.</p> </li> <li>In the directory where you created <code>conf-values.yaml</code>, run the following commands:     <pre><code># Add the lakeFS repository\nhelm repo add lakefs https://charts.lakefs.io\n# Deploy lakeFS\nhelm install my-lakefs lakefs/lakefs -f conf-values.yaml\n</code></pre></li> </ol> <p>my-lakefs is the Helm Release name.</p>"},{"location":"howto/deploy/onprem/#load-balancing","title":"Load balancing","text":"<p>To configure a load balancer to direct requests to the lakeFS servers you can use the <code>LoadBalancer</code> Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a <code>/_health</code> endpoint that you can use for health checks.</p> <p>Info</p> <p>The NGINX Ingress Controller by default limits the client body size to 1 MiB.</p> <p>Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the [S3-compatible Gateway][s3-gateway] or a simple PUT request using the [OpenAPI Server][openapi].</p> <p>Checkout the Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO.</p>"},{"location":"howto/deploy/onprem/#secure-connection","title":"Secure connection","text":"<p>Using a load balancer or cluster manager for TLS/SSL termination is recommended. It helps speed the decryption process and reduces the processing burden from lakeFS.</p> <p>In case lakeFS needs to listen and serve with HTTPS, for example for development purposes, update its config yaml with the following section:</p> <pre><code>tls:\n  enabled: true\n  cert_file: server.crt   # provide path to your certificate file\n  key_file: server.key    # provide path to your server private key\n</code></pre>"},{"location":"howto/deploy/onprem/#local-blockstore","title":"Local Blockstore","text":"<p>You can configure a block adapter to a POSIX compatible storage location shared by all lakeFS instances. Using the shared storage location, both data and metadata will be stored there.</p> <p>Using the local blockstore import and allowing lakeFS access to a specific prefix, it is possible to import files from a shared location. Import is not enabled by default, as it doesn't assume the local path is shared and there is a security concern about accessing a path outside the specified in the blockstore configuration. Enabling is done by <code>blockstore.local.import_enabled</code> and <code>blockstore.local.allowed_external_prefixes</code> as described in the configuration reference.</p>"},{"location":"howto/deploy/onprem/#sample-configuration-using-local-blockstore","title":"Sample configuration using local blockstore","text":"<pre><code>database:\n  type: \"postgres\"\n  postgres:\n    connection_string: \"[DATABASE_CONNECTION_STRING]\"\n\nauth:\n  encrypt:\n    # replace this with a randomly-generated string. Make sure to keep it safe!\n    secret_key: \"[ENCRYPTION_SECRET_KEY]\"\n\nblockstore:\n  type: local\n  local:\n    path: /shared/location/lakefs_data    # location where data and metadata kept by lakeFS\n    import_enabled: true                  # required to be true to enable import files\n                                          # from `allowed_external_prefixes` locations\n    allowed_external_prefixes:\n      - /shared/location/files_to_import  # location with files we can import into lakeFS, require access from lakeFS\n</code></pre>"},{"location":"howto/deploy/onprem/#limitations","title":"Limitations","text":"<ul> <li>lakeFS doesn't control the way a shared location is managed across machines</li> <li>When using lakectl or the lakeFS UI, you can currently import only directories. If you need to import a single file, use the HTTP API or API Clients with <code>type=object</code> in the request body and <code>destination=&lt;full-path-to-file&gt;</code>.</li> <li>Garbage collector (for committed and uncommitted) and lakeFS Hadoop FileSystem currently unsupported</li> </ul>"},{"location":"howto/deploy/onprem/#create-the-admin-user","title":"Create the admin user","text":"<p>When you first open the lakeFS UI, you will be asked to create an initial admin user.</p> <ol> <li>Open <code>http://&lt;lakefs-host&gt;/</code> in your browser. If you haven't set up a load balancer, this will likely be <code>http://&lt;instance ip address&gt;:8000/</code></li> <li>On first use, you'll be redirected to the setup page:    </li> <li>Follow the steps to create an initial administrator user. Save the credentials you\u2019ve received somewhere safe, you won\u2019t be able to see them again!    </li> <li>Follow the link and go to the login screen. Use the credentials from the previous step to log in.</li> </ol>"},{"location":"howto/deploy/onprem/#create-your-first-repository","title":"Create your first repository","text":"<ol> <li>Use the credentials from the previous step to log in</li> <li>Click Create Repository and choose Blank Repository.    </li> <li>Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored.</li> <li>Click Create Repository</li> <li>You should now have a configured repository, ready to use!    </li> </ol> <p>Congratulations</p> <p>Your environment is now ready \ud83e\udd29</p>"},{"location":"howto/deploy/upgrade/","title":"Upgrading lakeFS","text":"<p>Info</p> <p>For a fully managed lakeFS service with guaranteed SLAs, try lakeFS Cloud</p> <p>Upgrading lakeFS from a previous version usually just requires re-deploying with the latest image (or downloading the latest version if you're using the binary). If you're upgrading, check whether the release requires a migration.</p>"},{"location":"howto/deploy/upgrade/#when-db-migrations-are-required","title":"When DB migrations are required","text":""},{"location":"howto/deploy/upgrade/#lakefs-01030-or-greater","title":"lakeFS 0.103.0 or greater","text":"<p>Version 0.103.0 added support for rolling KV upgrade. This means that users who already migrated to the KV ref-store (versions 0.80.0 and above) no longer have to pass through specific versions for migration. This includes ACL migration which was introduced in lakeFS version 0.98.0. Running <code>lakefs migrate up</code> on the latest lakeFS version will perform all the necessary migrations up to that point.</p>"},{"location":"howto/deploy/upgrade/#lakefs-0800-or-greater-kv-migration","title":"lakeFS 0.80.0 or greater (KV Migration)","text":"<p>Starting with version 0.80.2, lakeFS has transitioned from using a PostgreSQL based database implementation to a Key-Value datastore interface supporting multiple database implementations. More information can be found here. Users upgrading from a previous version of lakeFS must pass through the KV migration version (0.80.2) before upgrading to newer versions of lakeFS.</p> <p>Important</p> <p>Pre Migrate Requirements:</p> <ul> <li>Users using OS environment variables for database configuration must define the <code>connection_string</code> explicitly or as environment variable before proceeding with the migration.</li> <li>Database storage free capacity of at least twice the amount of the currently used capacity</li> <li>It is strongly recommended to perform these additional steps:</li> <li>Commit all uncommitted data on branches</li> <li>Create a snapshot of your database</li> <li>By default, old database tables are not being deleted by the migration process, and should be removed manually after a successful migration. To enable table drop as part of the migration, set the <code>database.drop_tables</code> configuration param to <code>true</code></li> </ul>"},{"location":"howto/deploy/upgrade/#migration-steps","title":"Migration Steps","text":"<p>For each lakeFS instance currently running with the database</p> <ol> <li> <p>Modify the <code>database</code> section under lakeFS configuration yaml:</p> <ol> <li>Add <code>type</code> field with <code>\"postgres\"</code> as value</li> <li>Copy the current configuration parameters to a new section called <code>postgres</code> <pre><code>---\ndatabase:\ntype: \"postgres\"\nconnection_string: \"postgres://localhost:5432/postgres?sslmode=disable\"\nmax_open_connections: 20\n\npostgres:\n  connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\"\n  max_open_connections: 20\n</code></pre></li> </ol> </li> <li> <p>Stop all lakeFS instances</p> </li> <li>Using the <code>lakefs</code> binary for the new version (0.80.2), run the following:    <pre><code>lakefs migrate up\n</code></pre></li> <li>lakeFS will run the migration process, which in the end should display the following message with no errors:    <pre><code>time=\"2022-08-10T14:46:25Z\" level=info msg=\"KV Migration took 717.629563ms\" func=\"pkg/logging.(*logrusEntryWrapper).Infof\" file=\"build/pkg/logging/logger.go:246\" TempDir=/tmp/kv_migrate_2913402680\n</code></pre></li> <li>It is now possible to remove the old database configuration. The updated configuration should look as such:    <pre><code>---\ndatabase:\n type: \"postgres\"\n\n postgres:\n   connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\"\n   max_open_connections: 20\n</code></pre></li> <li>Deploy (or run) the new version of lakeFS.</li> </ol>"},{"location":"howto/deploy/upgrade/#lakefs-0300-or-greater","title":"lakeFS 0.30.0 or greater","text":"<p>In case migration is required, you first need to stop the running lakeFS service. Using the <code>lakefs</code> binary for the new version, run the following:</p> <pre><code>lakefs migrate up\n</code></pre> <p>Deploy (or run) the new version of lakeFS.</p> <p>Note that an older version of lakeFS cannot run on a migrated database.</p>"},{"location":"howto/deploy/upgrade/#prior-to-lakefs-0300","title":"Prior to lakeFS 0.30.0","text":"<p>Note</p> <p>with lakeFS &lt; 0.30.0, you should first upgrade to 0.30.0 following this guide. Then, proceed to upgrade to the newest version.</p> <p>Starting version 0.30.0, lakeFS handles your committed metadata in a new way, which is more robust and has better performance. To move your existing data, you will need to run the following upgrade commands.</p> <p>Verify lakeFS version == 0.30.0 (can skip if using Docker)</p> <pre><code>lakefs --version\n</code></pre> <p>Migrate data from the previous format:</p> <pre><code>lakefs migrate db\n</code></pre> <p>Or migrate using Docker image:</p> <pre><code>docker run --rm -it -e LAKEFS_DATABASE_CONNECTION_STRING=&lt;database connection string&gt; treeverse/lakefs:rocks-migrate migrate db\n</code></pre> <p>Once migrated, it is possible to now use more recent lakeFS versions. Please refer to their release notes for more information on ugrading and usage).</p> <p>If you want to start over, discarding your existing data, you need to explicitly state this in your lakeFS configuration file. To do so, add the following to your configuration (relevant only for 0.30.0):</p> <pre><code>cataloger:\n  type: rocks\n</code></pre>"},{"location":"howto/deploy/upgrade/#data-migration-for-version-v0500","title":"Data Migration for Version v0.50.0","text":"<p>Warning</p> <p>If you are using a version before 0.50.0, you must first perform the previous upgrade to that version.</p>"},{"location":"howto/garbage-collection/gc/","title":"Garbage Collection","text":"<p>Tip</p> <p>lakeFS Cloud users enjoy a managed garbage collection service, and do not need to run this Spark program.</p> <p>Tip</p> <p>lakeFS Enterprise users can run a stand alone GC program, instead of this Spark program.</p> <p>By default, lakeFS keeps all your objects forever. This allows you to travel back in time to previous versions of your data. However, sometimes you may want to remove the objects from the underlying storage completely. Reasons for this include cost-reduction and privacy policies.</p> <p>The garbage collection (GC) job is a Spark program that removes the following from the underlying storage:</p> <ol> <li>Committed objects that have been deleted (or replaced) in lakeFS, and are considered expired according to rules you define.</li> <li>Uncommitted objects that are no longer accessible<ul> <li>For example, objects deleted before ever being committed.</li> </ul> </li> </ol>"},{"location":"howto/garbage-collection/gc/#garbage-collection-rules","title":"Garbage collection rules","text":"<p>Info</p> <p>These rules only apply to objects that have been committed at some point. Without retention rules, only inaccessible uncommitted objects will be removed by the job.</p> <p>Garbage collection rules determine for how long an object is kept in the storage after it is deleted (or replaced) in lakeFS. For every branch, the GC job retains deleted objects for the number of days defined for the branch. In the absence of a branch-specific rule, the default rule for the repository is used. If an object is present in more than one branch ancestry, it is removed only after the retention period has ended for all relevant branches.</p> <p>Example GC rules for a repository:</p> <pre><code>{\n  \"default_retention_days\": 14,\n  \"branches\": [\n    {\"branch_id\": \"main\", \"retention_days\": 21},\n    {\"branch_id\": \"dev\", \"retention_days\": 7}\n  ]\n}\n</code></pre> <p>In the above example, objects will be retained for 14 days after deletion by default. However, if present in the branch <code>main</code>, objects will be retained for 21 days. Objects present only in the <code>dev</code> branch will be retained for 7 days after they are deleted.</p>"},{"location":"howto/garbage-collection/gc/#how-to-configure-garbage-collection-rules","title":"How to configure garbage collection rules","text":"<p>To define retention rules, either use the <code>lakectl</code> command, the lakeFS web UI, or API:</p> CLIWeb UI <p>Create a JSON file with your GC rules:</p> <pre><code>cat &lt;&lt;EOT &gt;&gt; example_repo_gc_rules.json\n{\n\"default_retention_days\": 14,\n\"branches\": [\n    {\"branch_id\": \"main\", \"retention_days\": 21},\n    {\"branch_id\": \"dev\", \"retention_days\": 7}\n]\n}\nEOT\n</code></pre> <p>Set the GC rules using <code>lakectl</code>: <pre><code>lakectl gc set-config lakefs://example-repo -f example_repo_gc_rules.json \n</code></pre></p> <p>From the lakeFS web UI:</p> <ol> <li>Navigate to the main page of your repository.</li> <li>Go to Settings -&gt; Garbage Collection.</li> <li>Click Edit policy and paste your GC rule into the text box as a JSON.</li> <li>Save your changes.</li> </ol> <p></p>"},{"location":"howto/garbage-collection/gc/#how-to-run-the-garbage-collection-job","title":"How to run the garbage collection job","text":"<p>To run the job, use the following <code>spark-submit</code> command (or using your preferred method of running Spark programs).</p> AWSAzureGCP <pre><code>spark-submit --class io.treeverse.gc.GarbageCollection \\\n    --packages org.apache.hadoop:hadoop-aws:2.7.7 \\\n    -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1  \\\n    -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\\n    -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\\n    -c spark.hadoop.fs.s3a.access.key=&lt;S3_ACCESS_KEY&gt; \\\n    -c spark.hadoop.fs.s3a.secret.key=&lt;S3_SECRET_KEY&gt; \\\n    http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client/0.17.0/lakefs-spark-client-assembly-0.17.0.jar \\\n    example-repo us-east-1\n</code></pre> <p>If you want to access your storage using the account key:</p> <pre><code>spark-submit --class io.treeverse.gc.GarbageCollection \\\n    --packages org.apache.hadoop:hadoop-aws:3.2.1 \\\n    -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1  \\\n    -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\\n    -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\\n    -c spark.hadoop.fs.azure.account.key.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=&lt;AZURE_STORAGE_ACCESS_KEY&gt; \\\n    http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client/0.17.0/lakefs-spark-client-assembly-0.17.0.jar \\\n    example-repo\n</code></pre> <p>Or, if you want to access your storage using an Azure service principal:</p> <pre><code>spark-submit --class io.treeverse.gc.GarbageCollection \\\n    --packages org.apache.hadoop:hadoop-aws:3.2.1 \\\n    -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1  \\\n    -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\\n    -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\\n    -c spark.hadoop.fs.azure.account.auth.type.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=OAuth \\\n    -c spark.hadoop.fs.azure.account.oauth.provider.type.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider \\\n    -c spark.hadoop.fs.azure.account.oauth2.client.id.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=&lt;application-id&gt; \\\n    -c spark.hadoop.fs.azure.account.oauth2.client.secret.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=&lt;service-credential-key&gt; \\\n    -c spark.hadoop.fs.azure.account.oauth2.client.endpoint.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token \\\n    http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client/0.17.0/lakefs-spark-client-assembly-0.17.0.jar \\\n    example-repo\n</code></pre> <p>Note</p> <ul> <li>On Azure, GC was tested only on Spark 3.3.0, but may work with other Spark and Hadoop versions.</li> <li>In case you don't have <code>hadoop-azure</code> package as part of your environment, you should add the package to your spark-submit with <code>--packages org.apache.hadoop:hadoop-azure:3.2.1</code></li> <li>For GC to work on Azure blob, soft delete should be disabled.</li> </ul> <p>For Garbage Collection to work with GCP, you must provide it with a service account key JSON file.  The use service account must have <code>Storage Object User</code> permissions for the repository namespace (bucket).</p> <pre><code>spark-submit --class  io.treeverse.gc.GarbageCollection \\\n    -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1  \\\n    -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\\n    -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\\n    -c spark.hadoop.google.cloud.auth.service.account.enable=true \\\n    -c spark.hadoop.google.cloud.auth.service.account.json.keyfile=&lt;PATH_TO_JSON_KEYFILE&gt; \\\n    -c spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem \\\n    -c spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS \\\n    http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client/0.17.0/lakefs-spark-client-assembly-0.17.0.jar \\\n    example-repo\n</code></pre>"},{"location":"howto/garbage-collection/gc/#mark-and-sweep-stages","title":"Mark and Sweep stages","text":"<p>You can break the job into two stages:</p> <ul> <li>Mark: find objects to remove, without actually removing them.</li> <li>Sweep: remove the objects.</li> </ul>"},{"location":"howto/garbage-collection/gc/#mark-only-mode","title":"Mark-only mode","text":"<p>To make GC run the mark stage only, add the following to your spark-submit command:</p> <pre><code>spark.hadoop.lakefs.gc.do_sweep=false\n</code></pre> <p>In mark-only mode, GC will write the keys of the expired objects under: <code>&lt;REPOSITORY_STORAGE_NAMESPACE&gt;/_lakefs/retention/gc/unified/&lt;MARK_ID&gt;/</code>. MARK_ID is generated by the job. You can find it in the driver's output:</p> <pre><code>Report for mark_id=gmc6523jatlleurvdm30 path=s3a://example-bucket/_lakefs/retention/gc/unified/gmc6523jatlleurvdm30\n</code></pre>"},{"location":"howto/garbage-collection/gc/#sweep-only-mode","title":"Sweep-only mode","text":"<p>To make GC run the sweep stage only, add the following properties to your spark-submit command:</p> <pre><code>spark.hadoop.lakefs.gc.do_mark=false\nspark.hadoop.lakefs.gc.mark_id=&lt;MARK_ID&gt; # Replace &lt;MARK_ID&gt; with the identifier you obtained from a previous mark-only run\n</code></pre>"},{"location":"howto/garbage-collection/gc/#garbage-collection-notes","title":"Garbage collection notes","text":"<ol> <li>In order for an object to be removed, it must not exist on the HEAD of any branch.    You should remove stale branches to prevent them from retaining old objects.    For example, consider a branch that has been merged to <code>main</code> and has become stale.    An object which is later deleted from <code>main</code> will always be present in the stale branch, preventing it from being removed.</li> <li>lakeFS will never delete objects outside your repository's storage namespace.    In particular, objects that were imported using <code>lakectl import</code> or the UI import wizard will not be affected by GC jobs.</li> <li>In cases where deleted objects are brought back to life while a GC job is running (for example, by reverting a commit),    the objects may or may not be deleted.</li> <li>Garbage collection does not remove any commits: you will still be able to use commits containing removed objects,    but trying to read these objects from lakeFS will result in a <code>410 Gone</code> HTTP status.</li> </ol>"},{"location":"howto/garbage-collection/managed-gc/","title":"Managed Garbage Collection","text":"<p>lakeFS Cloud</p> <p>Note</p> <p>Managed GC is only available for lakeFS Cloud. If you are using the self-managed lakeFS, garbage collection is available to run manually.</p>"},{"location":"howto/garbage-collection/managed-gc/#benefits-of-using-managed-gc","title":"Benefits of using managed GC","text":"<ul> <li>The quick and safe way to delete your unnecessary objects</li> <li>No operational overhead</li> <li>SLA for when your objects are deleted</li> <li>Support from the Treeverse team</li> </ul>"},{"location":"howto/garbage-collection/managed-gc/#how-it-works","title":"How it works","text":"<p>Similarly to the self-managed lakeFS, managed GC uses garbage collection rules to determine which objects to delete. However, it uses our super-fast and efficient engine to detect stale objects and branches (depends on your configuration) and prioritize them for deletion.</p>"},{"location":"howto/garbage-collection/managed-gc/#setting-up","title":"Setting up","text":"<p>Enable managed GC through the lakeFS Cloud onboarding setup wizard. This will create additional cloud resources for us to use and have access to delete those objects.</p>"},{"location":"howto/garbage-collection/standalone-gc/","title":"Standalone Garbage Collection","text":"<p>Info</p> <p>Standalone GC is only available for lakeFS Enterprise.</p>"},{"location":"howto/garbage-collection/standalone-gc/#what-is-standalone-gc","title":"What is Standalone GC?","text":"<p>Standalone GC is a simplified version of the Spark-backed GC that runs without any external dependencies, delivered as a standalone docker image. It supports S3 and self-managed S3 compatible storages such as MinIO.    </p>"},{"location":"howto/garbage-collection/standalone-gc/#limitations","title":"Limitations","text":"<ol> <li>No horizontal scalability: Only a single instance of <code>lakefs-sgc</code> can operate on a given repository at a time.</li> <li>Mark phase only: Standalone GC supports only the mark phase, identifying objects for deletion but not executing  the sweep stage to delete them. It functions similarly to the GC's mark-only mode.</li> <li>Only supports AWS S3 and S3-compatible object storages. However, supporting Azure blob and GCS are in our roadmap.</li> </ol>"},{"location":"howto/garbage-collection/standalone-gc/#installation","title":"Installation","text":""},{"location":"howto/garbage-collection/standalone-gc/#step-1-obtain-dockerhub-token","title":"Step 1: Obtain DockerHub token","text":""},{"location":"howto/garbage-collection/standalone-gc/#lakefs-enterprise-customers","title":"lakeFS Enterprise customers","text":"<p>Contact your account manager to verify that Standalone GC is included in your license. Then use your DockerHub token for  the <code>externallakefs</code> user.</p>"},{"location":"howto/garbage-collection/standalone-gc/#new-to-lakefs-enterprise","title":"New to lakeFS Enterprise","text":"<p>Please contact us to get trial access to Standalone GC.</p>"},{"location":"howto/garbage-collection/standalone-gc/#step-2-login-to-dockerhub-with-this-token","title":"Step 2: Login to DockerHub with this token","text":"<pre><code>docker login -u &lt;token&gt;\n</code></pre>"},{"location":"howto/garbage-collection/standalone-gc/#step-3-download-the-docker-image","title":"Step 3: Download the docker image","text":"<p>Download the <code>treeverse/lakefs-sgc</code> image from Docker Hub:</p> <pre><code>docker pull treeverse/lakefs-sgc:&lt;tag&gt;\n</code></pre>"},{"location":"howto/garbage-collection/standalone-gc/#setup","title":"Setup","text":""},{"location":"howto/garbage-collection/standalone-gc/#permissions","title":"Permissions","text":"<p>To run <code>lakefs-sgc</code>, you need both AWS (or S3-compatible) storage and lakeFS user permissions as outlined below:</p>"},{"location":"howto/garbage-collection/standalone-gc/#storage-permissions","title":"Storage permissions","text":"<p>The minimum required permissions for AWS or S3-compatible storage are:</p> <p><pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::some-bucket/some/prefix/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::some-bucket\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListAllMyBuckets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::*\"\n      ]\n    }\n  ]\n}\n</code></pre> In this example, the repository storage namespace is <code>s3://some-bucket/some/prefix</code>.</p>"},{"location":"howto/garbage-collection/standalone-gc/#lakefs-permissions","title":"lakeFS permissions","text":"<p>The minimum required permissions for lakeFS are:</p> <pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\n        \"retention:PrepareGarbageCollectionCommits\",\n        \"retention:PrepareGarbageCollectionUncommitted\",\n        \"fs:ReadConfig\",\n        \"fs:ReadRepository\",\n        \"fs:ListObjects\",\n        \"fs:ReadConfig\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:fs:::repository/&lt;repository&gt;\"\n    }\n  ]\n}\n</code></pre>"},{"location":"howto/garbage-collection/standalone-gc/#credentials","title":"Credentials","text":"<p>Standalone GC supports S3 and S3-compatible storage backends and relies on AWS credentials for authentication. To set up credentials on the <code>lakefs-sgc</code> docker container, follow AWS guidelines, such as those outlined in this guide. For details on how to pass credentials to <code>lakefs-sgc</code>, refer to the instructions in How to Run Standalone GC.</p>"},{"location":"howto/garbage-collection/standalone-gc/#using-s3-compatible-clients","title":"Using S3-compatible clients","text":"<p><code>lakefs-sgc</code> leverages AWS credentials to work seamlessly with S3-compatible storage solutions, such as MinIO.  Follow the steps below to set up and use <code>lakefs-sgc</code> with an S3-compatible client:</p> <ol> <li>Add a profile to your <code>~/.aws/config</code> file:     <code>[profile minio]    region = us-east-1    endpoint_url = &lt;MinIO URL&gt;    s3 =        signature_version = s3v4</code></li> <li>Add an access and secret keys to your <code>~/.aws/credentials</code> file:     <code>[minio]    aws_access_key_id     = &lt;MinIO access key&gt;    aws_secret_access_key = &lt;MinIO secret key&gt;</code></li> <li>Run the <code>lakefs-sgc</code> docker image and pass it the <code>minio</code> profile - see example below.</li> </ol>"},{"location":"howto/garbage-collection/standalone-gc/#configuration","title":"Configuration","text":"<p>The following configuration keys are available:</p> Key Description Default value Possible values <code>logging.format</code> Logs output format \"text\" \"text\",\"json\" <code>logging.level</code> Logs level \"info\" \"error\",\"warn\",info\",\"debug\",\"trace\" <code>logging.output</code> Where to output the logs to \"-\" \"-\" (stdout), \"=\" (stderr), or any string for file path <code>cache_dir</code> Directory to use for caching data during run ~/.lakefs-sgc/data string <code>aws.max_page_size</code> Max number of items per page when listing objects in AWS 1000 number <code>aws.s3.addressing_path_style</code> Whether or not to use path-style when reading objects from AWS true boolean <code>lakefs.endpoint_url</code> The URL to the lakeFS installation - should end with <code>/api/v1</code> NOT SET URL <code>lakefs.access_key_id</code> Access key to the lakeFS installation NOT SET string <code>lakefs.secret_access_key</code> Secret access key to the lakeFS installation NOT SET string <p>These keys can be provided in the following ways: 1. Config file: Create a YAML file with the keys, each <code>.</code> is a new nesting level. \\    For example, <code>logging.level</code> will be:    <pre><code>logging:\n  level: &lt;value&gt; # info,debug...\n</code></pre>    Then, pass it to the program using the <code>--config path/to/config.yaml</code> argument. 2. Environment variables: by setting <code>LAKEFS_SGC_&lt;KEY&gt;</code>, with uppercase letters and <code>.</code>s converted to <code>_</code>s. \\    For example <code>logging.level</code> will be:    <pre><code>export LAKEFS_SGC_LOGGING_LEVEL=info\n</code></pre></p> <p>Example (minimalistic) config file: <pre><code>logging:\n  level: debug\nlakefs:\n  endpoint_url: https://your.url/api/v1\n  access_key_id: &lt;lakeFS access key&gt;\n  secret_access_key: &lt;lakeFS secret key&gt;\n</code></pre></p>"},{"location":"howto/garbage-collection/standalone-gc/#how-to-run-standalone-gc","title":"How to Run Standalone GC?","text":""},{"location":"howto/garbage-collection/standalone-gc/#command-line-reference","title":"Command line reference","text":""},{"location":"howto/garbage-collection/standalone-gc/#flags","title":"Flags","text":"<ul> <li><code>-c, --config</code>: config file to use (default is $HOME/.lakefs-sgc.yaml)</li> </ul>"},{"location":"howto/garbage-collection/standalone-gc/#commands","title":"Commands","text":"<p>run</p> <p>Usage:</p> <p><code>lakefs-sgc run &lt;repository&gt;</code></p> <p>Flags:</p> <ul> <li><code>--cache-dir</code>: directory to cache read files (default is <code>$HOME/.lakefs-sgc/data/</code>)</li> <li><code>--parallelism</code>: number of parallel downloads for metadata files (default 10)</li> <li><code>--presign</code>: use pre-signed URLs when downloading/uploading data (recommended) (default true)</li> </ul> <p>To run standalone GC, choose the method you prefer to pass AWS credentials and invoke the commands below.  </p>"},{"location":"howto/garbage-collection/standalone-gc/#directly-passing-in-credentials-parsed-from-awscredentials","title":"Directly passing in credentials parsed from <code>~/.aws/credentials</code>","text":"<pre><code>docker run \\\n    -e AWS_REGION=&lt;region&gt; \\\n    -e AWS_SESSION_TOKEN=\"$(grep 'aws_session_token' ~/.aws/credentials | awk -F' = ' '{print $2}')\" \\\n    -e AWS_ACCESS_KEY_ID=\"$(grep 'aws_access_key_id' ~/.aws/credentials | awk -F' = ' '{print $2}')\" \\\n    -e AWS_SECRET_ACCESS_KEY=\"$(grep 'aws_secret_access_key' ~/.aws/credentials | awk -F' = ' '{print $2}')\" \\\n    -e LAKEFS_SGC_LAKEFS_ENDPOINT_URL=&lt;lakefs endpoint URL&gt; \\\n    -e LAKEFS_SGC_LAKEFS_ACCESS_KEY_ID=&lt;lakefs accesss key&gt; \\\n    -e LAKEFS_SGC_LAKEFS_SECRET_ACCESS_KEY=&lt;lakefs secret key&gt; \\\n    -e LAKEFS_SGC_LOGGING_LEVEL=debug \\\n    treeverse/lakefs-sgc:&lt;tag&gt; run &lt;repository&gt;\n</code></pre>"},{"location":"howto/garbage-collection/standalone-gc/#mounting-the-aws-directory","title":"Mounting the <code>~/.aws</code> directory","text":"<p>When working with S3-compatible clients, it's often more convenient to mount the <code>~/.aws</code> directory and pass in the desired profile.</p> <p>First, change the permissions for <code>~/.aws/*</code> to allow the docker container to read this directory:</p> <pre><code>chmod 644 ~/.aws/*\n</code></pre> <p>Then, run the docker image and mount <code>~/.aws</code> to the <code>lakefs-sgc</code> home directory on the docker container:</p> <pre><code>docker run \\\n--network=host \\\n-v ~/.aws:/home/lakefs-sgc/.aws \\\n-e AWS_REGION=us-east-1 \\\n-e AWS_PROFILE=&lt;profile&gt; \\\n-e LAKEFS_SGC_LAKEFS_ENDPOINT_URL=&lt;lakefs endpoint URL&gt; \\\n-e LAKEFS_SGC_LAKEFS_ACCESS_KEY_ID=&lt;lakefs accesss key&gt; \\\n-e LAKEFS_SGC_LAKEFS_SECRET_ACCESS_KEY=&lt;lakefs secret key&gt; \\\n-e LAKEFS_SGC_LOGGING_LEVEL=debug \\\ntreeverse/lakefs-sgc:&lt;tag&gt; run &lt;repository&gt;\n</code></pre>"},{"location":"howto/garbage-collection/standalone-gc/#get-the-list-of-objects-marked-for-deletion","title":"Get the List of Objects Marked for Deletion","text":"<p><code>lakefs-sgc</code> will write its reports to <code>&lt;REPOSITORY_STORAGE_NAMESPACE&gt;/_lakefs/retention/gc/reports/&lt;RUN_ID&gt;/</code>. \\ RUN_ID is generated during runtime by the Standalone GC. You can find it in the logs:</p> <pre><code>\"Marking objects for deletion\" ... run_id=gcoca17haabs73f2gtq0\n</code></pre> <p>In this prefix, you'll find 2 objects:</p> <ul> <li> <p><code>deleted.csv</code> - Containing all marked objects in a CSV containing one <code>address</code> column.</p> <p>Example</p> <pre><code>address\n\"data/gcnobu7n2efc74lfa5ug/csfnri7n2efc74lfa69g,_e7P9j-1ahTXtofw7tWwJUIhTfL0rEs_dvBrClzc_QE\"\n\"data/gcnobu7n2efc74lfa5ug/csfnri7n2efc74lfa78g,mKZnS-5YbLzmK0pKsGGimdxxBlt8QZzCyw1QeQrFvFE\"\n...\n</code></pre> <ul> <li><code>summary.json</code> - A small json summarizing the GC run. </li> </ul> <p>Example</p> <pre><code>{\n    \"run_id\": \"gcoca17haabs73f2gtq0\",\n    \"success\": true,\n    \"first_slice\": \"gcss5tpsrurs73cqi6e0\",\n    \"start_time\": \"2024-10-27T13:19:26.890099059Z\",\n    \"cutoff_time\": \"2024-10-27T07:19:26.890099059Z\",\n    \"num_deleted_objects\": 33000\n}\n</code></pre> </li> </ul>"},{"location":"howto/garbage-collection/standalone-gc/#delete-marked-objects","title":"Delete marked objects","text":"<p>We recommend starting by backing up the marked objects to a different bucket before deleting them. After ensuring the  backup is complete, you can proceed to delete the objects directly from the backup location.</p> <p>Use the following script to backup marked objects to another bucket:</p> <pre><code># Update these variables with your actual values\nstorage_ns=&lt;storage namespace (s3://...)&gt;\noutput_bucket=&lt;output bucket (s3://...)&gt;\nrun_id=&lt;GC run id&gt;\n\n# Download the CSV file\naws s3 cp \"$storage_ns/_lakefs/retention/gc/reports/$run_id/deleted.csv\" \"./run_id-$run_id.csv\"\n\n# Move all addresses to the output bucket under the \"run_id=$run_id\" prefix\ncat run_id-$run_id.csv | tail -n +2 | xargs -I {} aws s3 mv \"$storage_ns/{}\" \"$output_bucket/run_id=$run_id/\"\n</code></pre> <p>To delete the marked objects, use the following script:</p> <pre><code># Update these variables with your actual values\noutput_bucket=&lt;output bucket (s3://...)&gt;\nrun_id=&lt;GC run id&gt;\n\naws s3 rm $output_bucket/run_id=$run_id --recursive\n</code></pre> <p>Tip</p> <p>Remember to periodically delete the backups to actually reduce storage costs.</p>"},{"location":"howto/garbage-collection/standalone-gc/#lab-tests","title":"Lab tests","text":"<p>Standalone GC was tested on the lakeFS setup below.   </p>"},{"location":"howto/garbage-collection/standalone-gc/#repository-spec","title":"Repository spec","text":"<ul> <li>100k objects</li> <li>250 commits</li> <li>100 branches</li> </ul>"},{"location":"howto/garbage-collection/standalone-gc/#machine-spec","title":"Machine spec","text":"<ul> <li>4GiB RAM</li> <li>8 CPUs</li> </ul>"},{"location":"howto/garbage-collection/standalone-gc/#testing-results","title":"Testing results","text":"<ul> <li>Time: &lt; 5m</li> <li>Disk space: 123MB</li> </ul>"},{"location":"howto/hooks/","title":"Actions and Hooks in lakeFS","text":"<p>Like other version control systems, lakeFS allows you to configure Actions to trigger when predefined events occur. There are numerous uses for Actions, including:</p> <ol> <li>Format Validator:    A webhook that checks new files to ensure they are of a set of allowed data formats.</li> <li>Schema Validator:    A webhook that reads new Parquet and ORC files to ensure they don't contain a block list of column names (or name prefixes).    This is useful for avoiding accidental PII exposure.</li> <li>Integration with external systems:    Post-merge and post-commit hooks could be used to export metadata about the change to another system. A common example is exporting <code>symlink.txt</code> files that allow e.g. AWS Athena to read data from lakeFS.</li> <li>Notifying downstream consumers:    Running a post-merge hook to trigger an Airflow DAG or to send a Webhook to an API, notifying it of the change that happened</li> </ol> <p>For step-by-step examples of hooks in action check out the lakeFS Quickstart and the lakeFS samples repository.</p>"},{"location":"howto/hooks/#overview","title":"Overview","text":"<p>An action defines one or more hooks to execute. lakeFS supports three types of hook:</p> <ol> <li>Lua - uses an embedded Lua VM</li> <li>Webhook - makes a REST call to an external URL</li> <li>Airflow - triggers a DAG in Airflow</li> </ol> <p>\"Before\" hooks must run successfully before their action. If the hook fails, it aborts the action. Lua hooks and Webhooks are synchronous, and lakeFS waits for them to run to completion. Airflow hooks are asynchronous: lakeFS stops waiting as soon as Airflow accepts triggering the DAG.</p>"},{"location":"howto/hooks/#configuration","title":"Configuration","text":"<p>There are two parts to configuration an Action:</p> <ol> <li>Create an Action file and upload it to the lakeFS repository</li> <li>Configure the hook(s) that you specified in the Action file. How these are configured will depend on the type of hook.</li> </ol>"},{"location":"howto/hooks/#action-files","title":"Action files","text":"<p>An Action is a list of Hooks with the same trigger configuration, i.e. an event will trigger all Hooks under an Action or none at all.</p> <p>The Hooks under an Action are ordered and so is their execution.</p> <p>Before each hook execution the <code>if</code> boolean expression is evaluated. The expression can use the functions <code>success()</code> and <code>failure()</code>, which return true if the hook's actions succeeded or failed, respectively.</p> <p>By default, when <code>if</code> is empty or omitted, the step will run only if no error occurred (the same as <code>success()</code>).</p>"},{"location":"howto/hooks/#action-file-schema","title":"Action File schema","text":"Property Description Data Type Required Default Value <code>name</code> Identifes the Action file String no Action filename <code>on</code> List of events that will trigger the hooks List yes <code>on&lt;event&gt;.branches</code> Glob pattern list of branches that triggers the hooks List no Not applicable to Tag events. If empty, Action runs on all branches <code>hooks</code> List of hooks to be executed List yes <code>hook.id</code> ID of the hook, must be unique within the action. String yes <code>hook.type</code> Type of the hook (types) String yes <code>hook.description</code> Description for the hook String no <code>hook.if</code> Expression that will be evaluated before execute the hook String no No value is the same as evaluate <code>success()</code> <code>hook.properties</code> Hook's specific configuration, see Lua, WebHook, and Airflow for details Dictionary true"},{"location":"howto/hooks/#example-action-file","title":"Example Action File","text":"<p>_lakefs_actions/file_checker.yaml</p> <pre><code>name: Good files check\ndescription: set of checks to verify that branch is good\non:\npre-commit:\npre-merge:\n    branches:\n    - main\nhooks:\n- id: no_temp\n    type: webhook\n    description: checking no temporary files found\n    properties:\n    url: \"https://example.com/webhook?notmp=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\"\n- id: no_freeze\n    type: webhook\n    description: check production is not in dev freeze\n    properties:\n    url: \"https://example.com/webhook?nofreeze=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\"\n- id: alert\n    type: webhook\n    if: failure()\n    description: notify alert system when check failed\n    properties:\n    url: \"https://example.com/alert\"\n    query_params:\n        title: good files webhook failed\n- id: notification\n    type: webhook\n    if: true\n    description: notify that will always run - no matter if one of the previous steps failed\n    properties:\n    url: \"https://example.com/notification\"\n    query_params:\n        title: good files completed\n</code></pre> <p>Note</p> <p>lakeFS will validate action files only when an Event has occurred.  Use <code>lakectl actions validate &lt;path&gt;</code> to validate your action files locally.</p>"},{"location":"howto/hooks/#uploading-action-files","title":"Uploading Action files","text":"<p>Action files should be uploaded with the prefix <code>_lakefs_actions/</code> to the lakeFS repository. When an actionable event (see Supported Events above) takes place, lakeFS will read all files with prefix <code>_lakefs_actions/</code> in the repository branch where the action occurred. A failure to parse an Action file will result with a failing Run.</p> <p>For example, lakeFS will search and execute all the matching Action files with the prefix <code>lakefs://example-repo/feature-1/_lakefs_actions/</code> on:</p> <ol> <li>Commit to <code>feature-1</code> branch on <code>example-repo</code> repository.</li> <li>Merge to <code>main</code> branch from <code>feature-1</code> branch on <code>repo1</code> repository.</li> </ol>"},{"location":"howto/hooks/#supported-events","title":"Supported Events","text":"Event Description <code>prepare-commit</code> (EXPERIMENTAL) Runs before the commit occurs; branch modification will be included in the commit <code>pre-commit</code> Runs when the commit occurs, before the commit is finalized <code>post-commit</code> Runs after the commit is finalized <code>pre-merge</code> Runs on the source branch when the merge occurs, before the merge is finalized <code>post-merge</code> Runs on the merge result, after the merge is finalized <code>pre-create-branch</code> Runs on the source branch prior to creating a new branch <code>post-create-branch</code> Runs on the new branch after the branch was created <code>pre-delete-branch</code> Runs prior to deleting a branch <code>post-delete-branch</code> Runs after the branch was deleted <code>pre-revert</code> Runs prior to performing a revert operation on a branch <code>post-revert</code> Runs after performing a revert operation on a branch <code>pre-create-tag</code> Runs prior to creating a new tag <code>post-create-tag</code> Runs after the tag was created <code>pre-delete-tag</code> Runs prior to deleting a tag <code>post-delete-tag</code> Runs after the tag was deleted <code>pre-cherry-pick</code> Runs when a cherry-pick occurs, before it is finalized <code>post-cherry-pick</code> Runs after the cherry-pick is finalized <p>Warning</p> <p>The <code>prepare-commit</code> hook is experimental. During its execution (between <code>prepare-commit</code> and <code>pre-commit</code>), other changes may be applied to the branch as there is no branch-level locking mechanism at this point. If you need to verify or validate the content that will be committed, use the <code>pre-commit</code> hook instead, as it provides a consistent view of the changes that will be included in the commit.</p> <p>lakeFS Actions are handled per repository and cannot be shared between repositories. A failure of any Hook under any Action of a <code>pre-*</code> event will result in aborting the lakeFS operation that is taking place. Hook failures under any Action of a <code>post-*</code> event will not revert the operation.</p> <p>Hooks are managed by Action files that are written to a prefix in the lakeFS repository. This allows configuration-as-code inside lakeFS, where Action files are declarative and written in YAML.</p>"},{"location":"howto/hooks/#runs-api-cli","title":"Runs API &amp; CLI","text":"<p>A Run is an instantiation of the repository's Action files when the triggering event occurs. For example, if your repository contains a pre-commit hook, every commit would generate a Run for that specific commit.</p> <p>lakeFS will fetch, parse and filter the repository Action files and start to execute the Hooks under each Action. All executed Hooks (each with <code>hook_run_id</code>) exist in the context of that Run (<code>run_id</code>).</p> <p>The lakeFS API and lakectl expose the results of executions per repository, branch, commit, and specific Action. The endpoint also allows to download the execution log of any executed Hook under each Run for observability.</p>"},{"location":"howto/hooks/#result-files","title":"Result Files","text":"<p>The metadata section of lakeFS repository with each Run contains two types of files:</p> <ol> <li><code>_lakefs/actions/log/&lt;runID&gt;/&lt;hookRunID&gt;.log</code> - Execution log of the specific Hook run.</li> <li><code>_lakefs/actions/log/&lt;runID&gt;/run.manifest</code> - Manifest with all Hooks execution for the run with their results and additional metadata.</li> </ol> <p>Note</p> <p>Metadata section of a lakeFS repository is where lakeFS keeps its metadata, like commits and metaranges. Metadata files stored in the metadata section aren't accessible like user stored files.</p>"},{"location":"howto/hooks/airflow/","title":"Airflow Hooks","text":"<p>Airflow Hook triggers a DAG run in an Airflow installation using Airflow's REST API. The hook run succeeds if the DAG was triggered, and fails otherwise.</p>"},{"location":"howto/hooks/airflow/#action-file-airflow-hook-properties","title":"Action file Airflow hook properties","text":"<p>Info</p> <p>See the Action configuration for overall configuration schema and details</p> Property Description Data Type Example Required Environment Variables Supported url The URL of the Airflow instance String <code>http://localhost:8080</code> yes no dag_id The DAG to trigger String <code>example_dag</code> yes no username The name of the Airflow user performing the request String <code>admin</code> yes no password The password of the Airflow user performing the request String <code>admin</code> yes yes dag_conf DAG run configuration that will be passed as is JSON no no wait_for_dag Wait for DAG run to complete and reflect state (default: false) Boolean no no timeout Time to wait for the DAG run to complete (default: 1m) String (golang's Duration representation) no no <p>Example</p> <pre><code>...\nhooks:\n  - id: trigger_my_dag\n    type: airflow\n    description: Trigger an example_dag\n    properties:\n      url: \"http://localhost:8000\"\n      dag_id: \"example_dag\"\n      username: \"admin\"\n      password: \"{% raw %}{{{% endraw %} ENV.AIRFLOW_SECRET {% raw %}}}{% endraw %}\"\n      dag_conf:\n          some: \"additional_conf\"\n...\n</code></pre>"},{"location":"howto/hooks/airflow/#hook-record-in-configuration-field","title":"Hook Record in configuration field","text":"<p>lakeFS will add an entry to the Airflow request configuration property (<code>conf</code>) with the event that triggered the action.</p> <p>The key of the record will be <code>lakeFS_event</code> and the value will match the one described here</p>"},{"location":"howto/hooks/lua/","title":"Lua Hooks","text":"<p>lakeFS supports running hooks without relying on external components using an embedded Lua VM</p> <p>Using Lua hooks, it is possible to pass a Lua script to be executed directly by the lakeFS server when an action occurs.</p> <p>The Lua runtime embedded in lakeFS is limited for security reasons. It provides a narrow set of APIs and functions that by default do not allow:</p> <ol> <li>Accessing any of the running lakeFS server's environment</li> <li>Accessing the local filesystem available the lakeFS process</li> </ol>"},{"location":"howto/hooks/lua/#action-file-lua-hook-properties","title":"Action File Lua Hook Properties","text":"<p>Info</p> <p>See the Action configuration for overall configuration schema and details.</p> Property Description Data Type Required Default Value <code>args</code> One or more arguments to pass to the hook Dictionary false <code>script</code> An inline Lua script String either this or <code>script_path</code> must be specified <code>script_path</code> The path in lakeFS to a Lua script String either this or <code>script</code> must be specified"},{"location":"howto/hooks/lua/#example-lua-hooks","title":"Example Lua Hooks","text":"<p>For more examples and configuration samples, check out the examples/hooks/ directory in the lakeFS repository. You'll also find step-by-step examples of hooks in action in the lakeFS samples repository.</p> <p>Display information about an event</p> <p>This example will print out a JSON representation of the event that occurred:</p> <pre><code>name: dump_all\non:\n  post-commit:\n  post-merge:\n  post-create-tag:\n  post-create-branch:\nhooks:\n  - id: dump_event\n    type: lua\n    properties:\n      script: |\n        json = require(\"encoding/json\")\n        print(json.marshal(action))\n</code></pre> <p>Ensure that a commit includes a mandatory metadata field</p> <p>A more useful example: ensure every commit contains a required metadata field:</p> <pre><code>name: pre commit metadata field check\non:\npre-commit:\n    branches:\n    - main\n    - dev\nhooks:\n  - id: ensure_commit_metadata\n    type: lua\n    properties:\n      args:\n        notebook_url: {\"pattern\": \"my-jupyter.example.com/.*\"}\n        spark_version:  {}\n      script_path: lua_hooks/ensure_metadata_field.lua\n</code></pre> <p>Lua code at <code>lakefs://repo/main/lua_hooks/ensure_metadata_field.lua</code>:</p> <pre><code>regexp = require(\"regexp\")\nfor k, props in pairs(args) do\n  current_value = action.commit.metadata[k]\n  if current_value == nil then\n    error(\"missing mandatory metadata field: \" .. k)\n  end\n  if props.pattern and not regexp.match(props.pattern, current_value) then\n    error(\"current value for commit metadata field \" .. k .. \" does not match pattern: \" .. props.pattern .. \" - got: \" .. current_value)\n  end\nend\n</code></pre> <p>For more examples and configuration samples, check out the examples/hooks/ directory in the lakeFS repository.</p>"},{"location":"howto/hooks/lua/#lua-library-reference","title":"Lua Library reference","text":"<p>The Lua runtime embedded in lakeFS is limited for security reasons. The provided APIs are shown below.</p>"},{"location":"howto/hooks/lua/#arraytable","title":"<code>array(table)</code>","text":"<p>Helper function to mark a table object as an array for the runtime by setting <code>_is_array: true</code> metatable field.</p>"},{"location":"howto/hooks/lua/#aws","title":"<code>aws</code>","text":""},{"location":"howto/hooks/lua/#awss3_client","title":"<code>aws/s3_client</code>","text":"<p>S3 client library.</p> <p>Example</p> <pre><code>local aws = require(\"aws\")\n-- pass valid AWS credentials\nlocal client = aws.s3_client(\"ACCESS_KEY_ID\", \"SECRET_ACCESS_KEY\", \"REGION\")\n</code></pre>"},{"location":"howto/hooks/lua/#awss3_clientget_objectbucket-key","title":"<code>aws/s3_client.get_object(bucket, key)</code>","text":"<p>Returns the body (as a Lua string) of the requested object and a boolean value that is true if the requested object exists</p>"},{"location":"howto/hooks/lua/#awss3_clientput_objectbucket-key-value","title":"<code>aws/s3_client.put_object(bucket, key, value)</code>","text":"<p>Sets the object at the given bucket and key to the value of the supplied value string</p>"},{"location":"howto/hooks/lua/#awss3_clientdelete_objectbucket-key","title":"<code>aws/s3_client.delete_object(bucket [, key])</code>","text":"<p>Deletes the object at the given key</p>"},{"location":"howto/hooks/lua/#awss3_clientlist_objectsbucket-prefix-continuation_token-delimiter","title":"<code>aws/s3_client.list_objects(bucket [, prefix, continuation_token, delimiter])</code>","text":"<p>Returns a table of results containing the following structure:</p> <ul> <li><code>is_truncated</code>: (boolean) whether there are more results to paginate through using the continuation token</li> <li><code>next_continuation_token</code>: (string) to pass in the next request to get the next page of results</li> <li><code>results</code> (table of tables) information about the objects (and prefixes if a delimiter is used)</li> </ul> <p>a result could in one of the following structures</p> <pre><code>{\n   [\"key\"] = \"a/common/prefix/\",\n   [\"type\"] = \"prefix\"\n}\n</code></pre> <p>or:</p> <pre><code>{\n   [\"key\"] = \"path/to/object\",\n   [\"type\"] = \"object\",\n   [\"etag\"] = \"etagString\",\n   [\"size\"] = 1024,\n   [\"last_modified\"] = \"2023-12-31T23:10:00Z\"\n}\n</code></pre>"},{"location":"howto/hooks/lua/#awss3_clientdelete_recursivebucket-prefix","title":"<code>aws/s3_client.delete_recursive(bucket, prefix)</code>","text":"<p>Deletes all objects under the given prefix</p>"},{"location":"howto/hooks/lua/#awsglue","title":"<code>aws/glue</code>","text":"<p>Glue client library.</p> <p>Example</p> <pre><code>local aws = require(\"aws\")\n-- pass valid AWS credentials\nlocal glue = aws.glue_client(\"ACCESS_KEY_ID\", \"SECRET_ACCESS_KEY\", \"REGION\")\n</code></pre>"},{"location":"howto/hooks/lua/#awsgluecreate_databasedatabase-options","title":"<code>aws/glue.create_database(database, options)</code>","text":"<p>Create a new Database in Glue Catalog.</p> <p>Parameters:</p> <ul> <li><code>database(string)</code>: Glue Database name.</li> <li><code>options(table)</code> (optional):</li> <li><code>error_on_already_exists(boolean)</code>: Whether the call fail with an error if a DB with this name already exists</li> <li><code>create_db_input(Table)</code>: a Table that is passed \"as is\" to AWS and is parallel to the AWS SDK CreateDatabaseInput</li> </ul> <p>Example</p> <pre><code>local opts = {\n    error_on_already_exists = false,\n    create_db_input = {DatabaseInput = {Description = \"Created via LakeFS Action\"}, Tags = {Owner = \"Joe\"}}\n}\nglue.create_database(db, opts)\n</code></pre>"},{"location":"howto/hooks/lua/#awsgluedelete_databasedatabase-catalog_id","title":"<code>aws/glue.delete_database(database, catalog_id)</code>","text":"<p>Delete an existing Database in Glue Catalog.</p> <p>Parameters:</p> <ul> <li><code>database(string)</code>: Glue Database name.</li> <li><code>catalog_id(string)</code> (optional): Glue Catalog ID</li> </ul> <p>Example</p> <pre><code>glue.delete_database(db, \"461129977393\")\n</code></pre>"},{"location":"howto/hooks/lua/#awsglueget_tabledatabase-table-catalog_id","title":"<code>aws/glue.get_table(database, table [, catalog_id)</code>","text":"<p>Describe a table from the Glue Catalog.</p> <p>Example</p> <pre><code>local table, exists = glue.get_table(db, table_name)\nif exists then\n    print(json.marshal(table))\n</code></pre>"},{"location":"howto/hooks/lua/#awsgluecreate_tabledatabase-table_input-catalog_id","title":"<code>aws/glue.create_table(database, table_input, [, catalog_id])</code>","text":"<p>Create a new table in Glue Catalog. The <code>table_input</code> argument is a JSON that is passed \"as is\" to AWS and is parallel to the AWS SDK TableInput</p> <p>Example</p> <pre><code>local json = require(\"encoding/json\")\nlocal input = {\n    Name = \"my-table\",\n    PartitionKeys = array(partitions),\n    -- etc...\n}\nlocal json_input = json.marshal(input)\nglue.create_table(\"my-db\", table_input)\n</code></pre>"},{"location":"howto/hooks/lua/#awsglueupdate_tabledatabase-table_input-catalog_id-version_id-skip_archive","title":"<code>aws/glue.update_table(database, table_input, [, catalog_id, version_id, skip_archive])</code>","text":"<p>Update an existing Table in Glue Catalog. The <code>table_input</code> is the same as the argument in <code>glue.create_table</code> function.</p>"},{"location":"howto/hooks/lua/#awsgluedelete_tabledatabase-table_input-catalog_id","title":"<code>aws/glue.delete_table(database, table_input, [, catalog_id])</code>","text":"<p>Delete an existing Table in Glue Catalog.</p>"},{"location":"howto/hooks/lua/#azure","title":"<code>azure</code>","text":""},{"location":"howto/hooks/lua/#azureblob_client","title":"<code>azure/blob_client</code>","text":"<p>Azure blob client library.</p> <p>Example</p> <pre><code>local azure = require(\"azure\")\n-- pass valid Azure credentials\nlocal client = azure.blob_client(\"AZURE_STORAGE_ACCOUNT\", \"AZURE_ACCESS_KEY\")\n</code></pre>"},{"location":"howto/hooks/lua/#azureblob_clientget_objectpath_uri","title":"<code>azure/blob_client.get_object(path_uri)</code>","text":"<p>Returns the body (as a Lua string) of the requested object and a boolean value that is true if the requested object exists <code>path_uri</code> - A valid Azure blob storage uri in the form of <code>https://myaccount.blob.core.windows.net/mycontainer/myblob</code></p>"},{"location":"howto/hooks/lua/#azureblob_clientput_objectpath_uri-value","title":"<code>azure/blob_client.put_object(path_uri, value)</code>","text":"<p>Sets the object at the given bucket and key to the value of the supplied value string <code>path_uri</code> - A valid Azure blob storage uri in the form of <code>https://myaccount.blob.core.windows.net/mycontainer/myblob</code></p>"},{"location":"howto/hooks/lua/#azureblob_clientdelete_objectpath_uri","title":"<code>azure/blob_client.delete_object(path_uri)</code>","text":"<p>Deletes the object at the given key <code>path_uri</code> - A valid Azure blob storage uri in the form of <code>https://myaccount.blob.core.windows.net/mycontainer/myblob</code></p>"},{"location":"howto/hooks/lua/#azureabfss_transform_pathpath","title":"<code>azure/abfss_transform_path(path)</code>","text":"<p>Transform an HTTPS Azure URL to a ABFSS scheme. Used by the delta_exporter function to support Azure Unity catalog use cases <code>path</code> - A valid Azure blob storage URL in the form of <code>https://myaccount.blob.core.windows.net/mycontainer/myblob</code></p>"},{"location":"howto/hooks/lua/#crypto","title":"<code>crypto</code>","text":""},{"location":"howto/hooks/lua/#cryptoaesencryptcbckey-plaintext","title":"<code>crypto/aes/encryptCBC(key, plaintext)</code>","text":"<p>Returns a ciphertext for the aes encrypted text</p>"},{"location":"howto/hooks/lua/#cryptoaesdecryptcbckey-ciphertext","title":"<code>crypto/aes/decryptCBC(key, ciphertext)</code>","text":"<p>Returns the decrypted (plaintext) string for the encrypted ciphertext</p>"},{"location":"howto/hooks/lua/#cryptohmacsign_sha256message-key","title":"<code>crypto/hmac/sign_sha256(message, key)</code>","text":"<p>Returns a SHA256 hmac signature for the given message with the supplied key (using the SHA256 hashing algorithm)</p>"},{"location":"howto/hooks/lua/#cryptohmacsign_sha1message-key","title":"<code>crypto/hmac/sign_sha1(message, key)</code>","text":"<p>Returns a SHA1 hmac signature for the given message with the supplied key (using the SHA1 hashing algorithm)</p>"},{"location":"howto/hooks/lua/#cryptomd5digestdata","title":"<code>crypto/md5/digest(data)</code>","text":"<p>Returns the MD5 digest (string) of the given data</p>"},{"location":"howto/hooks/lua/#cryptosha256digestdata","title":"<code>crypto/sha256/digest(data)</code>","text":"<p>Returns the SHA256 digest (string) of the given data</p>"},{"location":"howto/hooks/lua/#databricksclientdatabricks_host-databricks_service_principal_token","title":"<code>databricks/client(databricks_host, databricks_service_principal_token)</code>","text":"<p>Returns a table representing a Databricks client with the <code>register_external_table</code> and <code>create_or_get_schema</code> methods.</p>"},{"location":"howto/hooks/lua/#databricksclientcreate_schemaschema_name-catalog_name-get_if_exists","title":"<code>databricks/client.create_schema(schema_name, catalog_name, get_if_exists)</code>","text":"<p>Creates a schema, or retrieves it if exists, in the configured Databricks host's Unity catalog. If a schema doesn't exist, a new schema with the given <code>schema_name</code> will be created under the given <code>catalog_name</code>. Returns the created/fetched schema name.</p> <p>Parameters:</p> <ul> <li><code>schema_name(string)</code>: The required schema name</li> <li><code>catalog_name(string)</code>: The catalog name under which the schema will be created (or from which it will be fetched)</li> <li><code>get_if_exists(boolean)</code>: In case of failure due to an existing schema with the given <code>schema_name</code> in the given <code>catalog_name</code>, return the schema.</li> </ul> <p>Example</p> <pre><code>local databricks = require(\"databricks\")\nlocal client = databricks.client(\"https://my-host.cloud.databricks.com\", \"my-service-principal-token\")\nlocal schema_name = client.create_schema(\"main\", \"mycatalog\", true)\n</code></pre>"},{"location":"howto/hooks/lua/#databricksclientexecute_statementstatement-warehouse_id-catalog_name-schema_name","title":"<code>databricks/client.execute_statement(statement, warehouse_id, catalog_name, schema_name)</code>","text":"<p>Parameters:</p> <ul> <li><code>statement(boolean)</code>: The SQL statement to execute on the databricks table</li> <li><code>warehouse_id(string)</code>: The SQL warehouse ID used in Databricks to run the <code>CREATE TABLE</code> query (fetched from the SQL warehouse</li> <li><code>catalog_name(string)</code>: The catalog name under which the schema will be created (or from which it will be fetched)</li> <li><code>schema_name(string)</code>: The required schema name   <code>status</code>, return the SQL status i.e. SUCCEEDED or an error code/message</li> </ul> <p>Example</p> <pre><code>local databricks = require(\"databricks\")\nlocal client = databricks.client(\"https://my-host.cloud.databricks.com\", \"my-service-principal-token\")\nlocal statement = \"ALTER TABLE \" .. table_descriptor.name .. \" ALTER COLUMN ID SET MASK mask_num\"\ndatabricks_client.execute_statement(statement, args.warehouse_id, table_descriptor.catalog, table_descriptor.schema)\n</code></pre>"},{"location":"howto/hooks/lua/#databricksclientregister_external_tabletable_name-physical_path-warehouse_id-catalog_name-schema_name-metadata","title":"<code>databricks/client.register_external_table(table_name, physical_path, warehouse_id, catalog_name, schema_name, metadata)</code>","text":"<p>Registers an external table under the provided warehouse ID, catalog name, and schema name. In order for this method call to succeed, an external location should be configured in the catalog, with the <code>physical_path</code>'s root storage URI (for example: <code>s3://mybucket</code>). Returns the table's creation status.</p> <p>Parameters:</p> <ul> <li><code>table_name(string)</code>: Table name.</li> <li><code>physical_path(string)</code>: A location to which the external table will refer, e.g. <code>s3://mybucket/the/path/to/mytable</code>.</li> <li><code>warehouse_id(string)</code>: The SQL warehouse ID used in Databricks to run the <code>CREATE TABLE</code> query (fetched from the SQL warehouse <code>Connection Details</code>, or by running <code>databricks warehouses get</code>, choosing your SQL warehouse and fetching its ID).</li> <li><code>catalog_name(string)</code>: The name of the catalog under which a schema will be created (or fetched from).</li> <li><code>schema_name(string)</code>: The name of the schema under which the table will be created.</li> <li><code>metadata(table)</code>: A table of metadata to be added to the table's registration. The metadata table should be of the form:   <code>{key1 = \"value1\", key2 = \"value2\", ...}</code>.</li> </ul> <p>Example</p> <pre><code>local databricks = require(\"databricks\")\nlocal client = databricks.client(\"https://my-host.cloud.databricks.com\", \"my-service-principal-token\")\nlocal status = client.register_external_table(\"mytable\", \"s3://mybucket/the/path/to/mytable\", \"examwarehouseple\", \"my-catalog-name\", \"myschema\")\n</code></pre> <ul> <li>For the Databricks permissions needed to run this method, check out the Unity Catalog Exporter docs.</li> </ul>"},{"location":"howto/hooks/lua/#encodingbase64encodedata","title":"<code>encoding/base64/encode(data)</code>","text":"<p>Encodes the given data to a base64 string</p>"},{"location":"howto/hooks/lua/#encodingbase64decodedata","title":"<code>encoding/base64/decode(data)</code>","text":"<p>Decodes the given base64 encoded data and return it as a string</p>"},{"location":"howto/hooks/lua/#encodingbase64url_encodedata","title":"<code>encoding/base64/url_encode(data)</code>","text":"<p>Encodes the given data to an unpadded alternate base64 encoding defined in RFC 4648.</p>"},{"location":"howto/hooks/lua/#encodingbase64url_decodedata","title":"<code>encoding/base64/url_decode(data)</code>","text":"<p>Decodes the given unpadded alternate base64 encoding defined in RFC 4648 and return it as a string</p>"},{"location":"howto/hooks/lua/#encodinghexencodevalue","title":"<code>encoding/hex/encode(value)</code>","text":"<p>Encode the given value string to hexadecimal values (string)</p>"},{"location":"howto/hooks/lua/#encodinghexdecodevalue","title":"<code>encoding/hex/decode(value)</code>","text":"<p>Decode the given hexadecimal string back to the string it represents (UTF-8)</p>"},{"location":"howto/hooks/lua/#encodingjsonmarshaltable","title":"<code>encoding/json/marshal(table)</code>","text":"<p>Encodes the given table into a JSON string</p>"},{"location":"howto/hooks/lua/#encodingjsonunmarshalstring","title":"<code>encoding/json/unmarshal(string)</code>","text":"<p>Decodes the given string into the equivalent Lua structure</p>"},{"location":"howto/hooks/lua/#encodingyamlmarshaltable","title":"<code>encoding/yaml/marshal(table)</code>","text":"<p>Encodes the given table into a YAML string</p>"},{"location":"howto/hooks/lua/#encodingyamlunmarshalstring","title":"<code>encoding/yaml/unmarshal(string)</code>","text":"<p>Decodes the given YAML encoded string into the equivalent Lua structure</p>"},{"location":"howto/hooks/lua/#encodingparquetget_schemapayload","title":"<code>encoding/parquet/get_schema(payload)</code>","text":"<p>Read the payload (string) as the contents of a Parquet file and return its schema in the following table structure:</p> <pre><code>{\n  { [\"name\"] = \"column_a\", [\"type\"] = \"INT32\" },\n  { [\"name\"] = \"column_b\", [\"type\"] = \"BYTE_ARRAY\" }\n}\n</code></pre>"},{"location":"howto/hooks/lua/#formats","title":"<code>formats</code>","text":""},{"location":"howto/hooks/lua/#formatsdelta_clientkey-secret-region","title":"<code>formats/delta_client(key, secret, region)</code>","text":"<p>Creates a new Delta Lake client used to interact with the lakeFS server. * <code>key</code>: lakeFS access key id * <code>secret</code>: lakeFS secret access key * <code>region</code>: The region in which your lakeFS server is configured at.</p>"},{"location":"howto/hooks/lua/#formatsdelta_clientget_tablerepository_id-reference_id-prefix","title":"<code>formats/delta_client.get_table(repository_id, reference_id, prefix)</code>","text":"<p>Returns a representation of a Delta Lake table under the given repository, reference, and prefix. The format of the response is two tables:</p> <ol> <li>the first is a table of the format <code>{number, {string}}</code> where <code>number</code> is a version in the Delta Log, and the mapped <code>{string}</code> array contains JSON strings of the different Delta Lake log operations listed in the mapped version entry. e.g.:</li> </ol> <pre><code>{\n  0 = {\n    \"{\\\"commitInfo\\\":...}\",\n    \"{\\\"add\\\": ...}\",\n    \"{\\\"remove\\\": ...}\"\n  },\n  1 = {\n    \"{\\\"commitInfo\\\":...}\",\n    \"{\\\"add\\\": ...}\",\n    \"{\\\"remove\\\": ...}\"\n  }\n}\n</code></pre> <ol> <li>the second is a table of the metadata of the current table snapshot. The metadata table can be used to initialize the Delta Lake table in an external Catalog. It consists of the following fields:<ul> <li><code>id</code>: The table's ID</li> <li><code>name</code>: The table's name</li> <li><code>description</code>: The table's description</li> <li><code>schema_string</code>: The table's schema string</li> <li><code>partition_columns</code>: The table's partition columns</li> <li><code>configuration</code>: The table's configuration</li> <li><code>created_time</code>: The table's creation time</li> </ul> </li> </ol>"},{"location":"howto/hooks/lua/#gcloud","title":"<code>gcloud</code>","text":""},{"location":"howto/hooks/lua/#gcloudgs_clientgcs_credentials_json_string","title":"<code>gcloud/gs_client(gcs_credentials_json_string)</code>","text":"<p>Create a new Google Cloud Storage client using a string that contains a valid <code>credentials.json</code> file content.</p>"},{"location":"howto/hooks/lua/#gcloudgswrite_fuse_symlinksource-destination-mount_info","title":"<code>gcloud/gs.write_fuse_symlink(source, destination, mount_info)</code>","text":"<p>Will create a gcsfuse symlink from the source (typically a lakeFS physical address for an object) to a given destination.</p> <p><code>mount_info</code> is a Lua table with <code>\"from\"</code> and <code>\"to\"</code> keys - since symlinks don't work for <code>gs://...</code> URIs, they need to point to the mounted location instead. <code>from</code> will be removed from the beginning of <code>source</code>, and <code>destination</code> will be added instead.</p> <p>Example</p> <pre><code>source = \"gs://bucket/lakefs/data/abc/def\"\ndestination = \"gs://bucket/exported/path/to/object\"\nmount_info = {\n    [\"from\"] = \"gs://bucket\",\n    [\"to\"] = \"/home/user/gcs-mount\"\n}\ngs.write_fuse_symlink(source, destination, mount_info)\n-- Symlink: \"/home/user/gcs-mount/exported/path/to/object\" -&gt; \"/home/user/gcs-mount/lakefs/data/abc/def\"\n</code></pre>"},{"location":"howto/hooks/lua/#hook","title":"<code>hook</code>","text":"<p>A set of utilities to aide in writing user friendly hooks.</p>"},{"location":"howto/hooks/lua/#hookfailmessage","title":"<code>hook/fail(message)</code>","text":"<p>Will abort the current hook's execution with the given message. This is similar to using <code>error()</code>, but is typically used to separate generic runtime errors (an API call that returned an unexpected response) and explicit failure of the calling hook.</p> <p>When called, errors will appear without a stack-trace, and the error message will be directly the one given as <code>message</code>.</p> <p>Example</p> <pre><code>&gt; hook = require(\"hook\")\n&gt; hook.fail(\"this hook shall not pass because of: \" .. reason)\n</code></pre>"},{"location":"howto/hooks/lua/#lakefs","title":"<code>lakefs</code>","text":"<p>The Lua Hook library allows calling back to the lakeFS API using the identity of the user that triggered the action. For example, if user A tries to commit and triggers a <code>pre-commit</code> hook - any call made inside that hook to the lakeFS API, will automatically use user A's identity for authorization and auditing purposes.</p>"},{"location":"howto/hooks/lua/#lakefscreate_tagrepository_id-reference_id-tag_id","title":"<code>lakefs/create_tag(repository_id, reference_id, tag_id)</code>","text":"<p>Create a new tag for the given reference</p>"},{"location":"howto/hooks/lua/#lakefsdiff_refsrepository_id-left_reference_id-right_reference_id-after-prefix-delimiter-amount","title":"<code>lakefs/diff_refs(repository_id, left_reference_id, right_reference_id [, after, prefix, delimiter, amount])</code>","text":"<p>Returns an object-wise diff between <code>left_reference_id</code> and <code>right_reference_id</code>.</p>"},{"location":"howto/hooks/lua/#lakefslist_objectsrepository_id-reference_id-after-prefix-delimiter-amount","title":"<code>lakefs/list_objects(repository_id, reference_id [, after, prefix, delimiter, amount])</code>","text":"<p>List objects in the specified repository and reference (branch, tag, commit ID, etc.). If delimiter is empty, will default to a recursive listing. Otherwise, common prefixes up to <code>delimiter</code> will be shown as a single entry.</p>"},{"location":"howto/hooks/lua/#lakefsget_objectrepository_id-reference_id-path","title":"<code>lakefs/get_object(repository_id, reference_id, path)</code>","text":"<p>Returns 2 values:</p> <ol> <li>The HTTP status code returned by the lakeFS API</li> <li>The content of the specified object as a Lua string</li> </ol>"},{"location":"howto/hooks/lua/#lakefsdiff_branchrepository_id-branch_id-after-amount-prefix-delimiter","title":"<code>lakefs/diff_branch(repository_id, branch_id [, after, amount, prefix, delimiter])</code>","text":"<p>Returns an object-wise diff of uncommitted changes on <code>branch_id</code>.</p>"},{"location":"howto/hooks/lua/#lakefsstat_objectrepository_id-ref_id-path-user_metadata","title":"<code>lakefs/stat_object(repository_id, ref_id, path[, user_metadata])</code>","text":"<p>Returns a stat object for the given path under the given reference and repository. Returns 2 values:</p> <ol> <li>The HTTP status code returned by the lakeFS API</li> <li>The stat response object as a JSON string</li> </ol> <p>Parameters:</p> <ul> <li><code>repository_id</code>: The repository ID</li> <li><code>ref_id</code>: The reference to stat from (branch, tag, commit ID)</li> <li><code>path</code>: Path to the object to stat</li> <li><code>user_metadata</code>: (Optional) Boolean flag to include user metadata in response</li> </ul>"},{"location":"howto/hooks/lua/#lakefsupdate_object_user_metadatarepository_id-branch_id-path-metadata","title":"<code>lakefs/update_object_user_metadata(repository_id, branch_id, path, metadata)</code>","text":"<p>Update user metadata for an object.</p> <p>Parameters:</p> <ul> <li><code>repository_id</code>: The repository ID</li> <li><code>branch_id</code>: The branch containing the object</li> <li><code>path</code>: Path to the object to update</li> <li><code>metadata</code>: A table containing key-value pairs to set as user metadata</li> </ul> <p>Returns 2 values:</p> <ol> <li>The HTTP status code returned by the lakeFS API (204 on success)</li> <li>Empty on success, error on failure</li> </ol>"},{"location":"howto/hooks/lua/#lakefscatalogexportglue_exporterget_full_table_namedescriptor-action_info","title":"<code>lakefs/catalogexport/glue_exporter.get_full_table_name(descriptor, action_info)</code>","text":"<p>Generate glue table name.</p> <p>Parameters:</p> <ul> <li><code>descriptor(Table)</code>: Object from (e.g. _lakefs_tables/my_table.yaml).</li> <li><code>action_info(Table)</code>: The global action object.</li> </ul>"},{"location":"howto/hooks/lua/#lakefscatalogexportdelta_exporter","title":"<code>lakefs/catalogexport/delta_exporter</code>","text":"<p>A package used to export Delta Lake tables from lakeFS to an external cloud storage.</p>"},{"location":"howto/hooks/lua/#lakefscatalogexportdelta_exporterexport_delta_logaction-table_def_names-write_object-delta_client-table_descriptors_path-path_transformer","title":"<code>lakefs/catalogexport/delta_exporter.export_delta_log(action, table_def_names, write_object, delta_client, table_descriptors_path, path_transformer)</code>","text":"<p>The function used to export Delta Lake tables. The return value is a table with mapping of table names to external table location (from which it is possible to query the data) and latest Delta table version's metadata. The response is of the form: <code>{&lt;table_name&gt; = {path = \"s3://mybucket/mypath/mytable\", metadata = {id = \"table_id\", name = \"table_name\", ...}}}</code>.</p> <p>Parameters:</p> <ul> <li><code>action</code>: The global action object</li> <li><code>table_def_names</code>: Delta tables name list (e.g. <code>{\"table1\", \"table2\"}</code>)</li> <li><code>write_object</code>: A writer function with <code>function(bucket, key, data)</code> signature, used to write the exported Delta Log (e.g. <code>aws/s3_client.put_object</code> or <code>azure/blob_client.put_object</code>)</li> <li><code>delta_client</code>: A Delta Lake client that implements <code>get_table: function(repo, ref, prefix)</code></li> <li><code>table_descriptors_path</code>: The path under which the table descriptors of the provided <code>table_def_names</code> reside</li> <li><code>path_transformer</code>: (Optional) A function(path) used for transforming the path of the saved delta logs path fields as well as the saved table physical path (used to support Azure Unity catalog use cases)</li> </ul> <p>Delta export example for AWS S3</p> <pre><code>---\nname: delta_exporter\non:\npost-commit: null\nhooks:\n- id: delta_export\n    type: lua\n    properties:\n    script: |\n        local aws = require(\"aws\")\n        local formats = require(\"formats\")\n        local delta_exporter = require(\"lakefs/catalogexport/delta_exporter\")\n        local json = require(\"encoding/json\")\n\n        local table_descriptors_path = \"_lakefs_tables\"\n        local sc = aws.s3_client(args.aws.access_key_id, args.aws.secret_access_key, args.aws.region)\n        local delta_client = formats.delta_client(args.lakefs.access_key_id, args.lakefs.secret_access_key, args.aws.region)\n        local delta_table_details = delta_export.export_delta_log(action, args.table_defs, sc.put_object, delta_client, table_descriptors_path)\n\n        for t, details in pairs(delta_table_details) do\n        print(\"Delta Lake exported table \\\"\" .. t .. \"\\\"'s location: \" .. details[\"path\"] .. \"\\n\")\n        print(\"Delta Lake exported table \\\"\" .. t .. \"\\\"'s metadata:\\n\")\n        for k, v in pairs(details[\"metadata\"]) do\n            if type(v) == \"table\" then\n            print(\"\\t\" .. k .. \" = \" .. json.marshal(v) .. \"\\n\")\n            else\n            print(\"\\t\" .. k .. \" = \" .. v .. \"\\n\")\n            end\n        end\n        end\n    args:\n        aws:\n        access_key_id: &lt;AWS_ACCESS_KEY_ID&gt;\n        secret_access_key: &lt;AWS_SECRET_ACCESS_KEY&gt;\n        region: us-east-1\n        lakefs:\n        access_key_id: &lt;LAKEFS_ACCESS_KEY_ID&gt; \n        secret_access_key: &lt;LAKEFS_SECRET_ACCESS_KEY&gt;\n        table_defs:\n        - mytable\n</code></pre> <p>For the table descriptor under the <code>_lakefs_tables/mytable.yaml</code>:</p> <pre><code>---\nname: myTableActualName\ntype: delta\npath: a/path/to/my/delta/table\n</code></pre> <p>Delta export example for Azure Blob Storage:</p> <pre><code>name: Delta Exporter\non:\npost-commit:\n    branches: [\"{% raw %}{{ .Branch }}{% endraw %}*\"]\nhooks:\n- id: delta_exporter\n    type: lua\n    properties:\n    script: |\n        local azure = require(\"azure\")\n        local formats = require(\"formats\")\n        local delta_exporter = require(\"lakefs/catalogexport/delta_exporter\")\n\n        local table_descriptors_path = \"_lakefs_tables\"\n        local sc = azure.blob_client(args.azure.storage_account, args.azure.access_key)\n        local function write_object(_, key, buf)\n        return sc.put_object(key,buf)\n        end\n        local delta_client = formats.delta_client(args.lakefs.access_key_id, args.lakefs.secret_access_key)\n        local delta_table_details = delta_export.export_delta_log(action, args.table_defs, sc.put_object, delta_client, table_descriptors_path)\n\n        for t, details in pairs(delta_table_details) do\n        print(\"Delta Lake exported table \\\"\" .. t .. \"\\\"'s location: \" .. details[\"path\"] .. \"\\n\")\n        print(\"Delta Lake exported table \\\"\" .. t .. \"\\\"'s metadata:\\n\")\n        for k, v in pairs(details[\"metadata\"]) do\n            if type(v) == \"table\" then\n            print(\"\\t\" .. k .. \" = \" .. json.marshal(v) .. \"\\n\")\n            else\n            print(\"\\t\" .. k .. \" = \" .. v .. \"\\n\")\n            end\n        end\n        end\n    args:\n        azure:\n        storage_account: \"{% raw %}{{ .AzureStorageAccount }}{% endraw %}\"\n        access_key: \"{% raw %}{{ .AzureAccessKey }}{% endraw %}\"\n        lakefs: # provide credentials of a user that has access to the script and Delta Table\n        access_key_id: \"{% raw %}{{ .LakeFSAccessKeyID }}{% endraw %}\"\n        secret_access_key: \"{% raw %}{{ .LakeFSSecretAccessKey }}{% endraw %}\"\n        table_defs:\n        - mytable\n</code></pre>"},{"location":"howto/hooks/lua/#lakefscatalogexportdelta_exporterchanged_table_defstable_def_names-table_descriptors_path-repository_id-ref-compare_ref","title":"<code>lakefs/catalogexport/delta_exporter.changed_table_defs(table_def_names, table_descriptors_path, repository_id, ref, compare_ref)</code>","text":"<p>Utility function to filter list of table defs based on those that have changed. Returns the subset of the tables in the table_def_names parameter that have changed.</p> <p>Parameters:</p> <ul> <li><code>table_def_names(table of strings)</code>: List of table names to filter based on the diff</li> <li><code>table_descriptors_path(string)</code>: The path under which the table descriptors of the provided <code>table_def_names</code> reside</li> <li><code>repository_id(string)</code>: The repository ID</li> <li><code>ref(string)</code>: base reference pointing at a specific version of the data i.e. a branch, commit ID, or tag</li> <li><code>compare_ref(string)</code>: compared-to reference for the diff to determine which tables changed</li> </ul> <p>Example</p> <pre><code>local delta_export = require(\"lakefs/catalogexport/delta_exporter\")\nlocal ref = action.commit.parents[1]\nlocal compare_ref = action.commit_id\nlocal changed_table_defs = delta_export.changed_table_defs(args.table_defs, args.table_descriptors_path, action.repository_id, ref, compare_ref)\nfor i = 1, #changed_table_defs do\n    print(changed_table_defs[i])\nend\n</code></pre>"},{"location":"howto/hooks/lua/#lakefscatalogexporttable_extractor","title":"<code>lakefs/catalogexport/table_extractor</code>","text":"<p>Utility package to parse <code>_lakefs_tables/</code> descriptors.</p>"},{"location":"howto/hooks/lua/#lakefscatalogexporttable_extractorlist_table_descriptor_entriesclient-repo_id-commit_id","title":"<code>lakefs/catalogexport/table_extractor.list_table_descriptor_entries(client, repo_id, commit_id)</code>","text":"<p>List all YAML files under <code>_lakefs_tables/*</code> and return a list of type <code>[{physical_address, path}]</code>, ignores hidden files. The <code>client</code> is <code>lakefs</code> client.</p>"},{"location":"howto/hooks/lua/#lakefscatalogexporttable_extractorget_table_descriptorclient-repo_id-ref-logical_path","title":"<code>lakefs/catalogexport/table_extractor.get_table_descriptor(client, repo_id, ref, logical_path)</code>","text":"<p>Read a table descriptor and parse YAML object. Will set <code>partition_columns</code> to <code>{}</code> if no partitions are defined.</p> <p>Parameters: * <code>client</code>: <code>lakefs</code> client * <code>repo_id(string)</code>: The repository ID * <code>ref(string)</code>: reference pointing at a specific version of the data i.e. a branch, commit ID, or tag * <code>logical_path(string)</code>: logical path of the table descriptor file within the repo</p>"},{"location":"howto/hooks/lua/#lakefscatalogexporthiveextract_partition_pagerclient-repo_id-commit_id-base_path-partition_cols-page_size","title":"<code>lakefs/catalogexport/hive.extract_partition_pager(client, repo_id, commit_id, base_path, partition_cols, page_size)</code>","text":"<p>Hive format partition iterator each result set is a collection of files under the same partition in lakeFS.</p> <p>Example</p> <pre><code>local lakefs = require(\"lakefs\")\nlocal pager = hive.extract_partition_pager(lakefs, repo_id, commit_id, prefix, partitions, 10)\nfor part_key, entries in pager do\n    print(\"partition: \" .. part_key)\n    for _, entry in ipairs(entries) do\n        print(\"path: \" .. entry.path .. \" physical: \" .. entry.physical_address)\n    end\nend\n</code></pre>"},{"location":"howto/hooks/lua/#lakefscatalogexportsymlink_exporter","title":"<code>lakefs/catalogexport/symlink_exporter</code>","text":"<p>Writes metadata for a table using Hive's SymlinkTextInputFormat. Currently only <code>S3</code> is supported.</p> <p>The default export paths per commit:</p> <pre><code>${storageNamespace}\n_lakefs/\n    exported/\n        ${ref}/\n            ${commitId}/\n                ${tableName}/\n                    p1=v1/symlink.txt\n                    p1=v2/symlink.txt\n                    p1=v3/symlink.txt\n                    ...\n</code></pre>"},{"location":"howto/hooks/lua/#lakefscatalogexportsymlink_exporterexport_s3s3_client-table_src_path-action_info-options","title":"<code>lakefs/catalogexport/symlink_exporter.export_s3(s3_client, table_src_path, action_info [, options])</code>","text":"<p>Export Symlink files that represent a table to S3 location.</p> <p>Parameters:</p> <ul> <li><code>s3_client</code>: Configured client.</li> <li><code>table_src_path(string)</code>: Path to the table spec YAML file in <code>_lakefs_tables</code> (e.g. _lakefs_tables/my_table.yaml).</li> <li><code>action_info(table)</code>: The global action object.</li> <li><code>options(table)</code>:</li> <li><code>debug(boolean)</code>: Print extra info.</li> <li><code>export_base_uri(string)</code>: Override the prefix in S3 e.g. <code>s3://other-bucket/path/</code>.</li> <li><code>writer(function(bucket, key, data))</code>: If passed then will not use s3 client, helpful for debug.</li> </ul> <p>Example</p> <pre><code>local exporter = require(\"lakefs/catalogexport/symlink_exporter\")\nlocal aws = require(\"aws\")\n-- args are user inputs from a lakeFS action.\nlocal s3 = aws.s3_client(args.aws.aws_access_key_id, args.aws.aws_secret_access_key, args.aws.aws_region)\nexporter.export_s3(s3, args.table_descriptor_path, action, {debug=true})\n</code></pre>"},{"location":"howto/hooks/lua/#lakefscatalogexportglue_exporter","title":"<code>lakefs/catalogexport/glue_exporter</code>","text":"<p>A Package for automating the export process from lakeFS stored tables into Glue catalog.</p>"},{"location":"howto/hooks/lua/#lakefscatalogexportglue_exporterexport_glueglue-db-table_src_path-create_table_input-action_info-options","title":"<code>lakefs/catalogexport/glue_exporter.export_glue(glue, db, table_src_path, create_table_input, action_info, options)</code>","text":"<p>Represent lakeFS table in Glue Catalog. This function will create a table in Glue based on configuration. It assumes that there is a symlink location that is already created and only configures it by default for the same commit.</p> <p>Parameters:</p> <ul> <li><code>glue</code>: AWS glue client</li> <li><code>db(string)</code>: glue database name</li> <li><code>table_src_path(string)</code>: path to table spec (e.g. _lakefs_tables/my_table.yaml)</li> <li><code>create_table_input(table)</code>: Input equal mapping to table_input in AWS, the same as we use for <code>glue.create_table</code>. should contain inputs describing the data format (e.g. InputFormat, OutputFormat, SerdeInfo) since the exporter is agnostic to this. by default this function will configure table location and schema.</li> <li><code>action_info(table)</code>: the global action object.</li> <li><code>options(table)</code>:</li> <li><code>table_name(string)</code>: Override default glue table name</li> <li><code>debug(boolean</code></li> <li><code>export_base_uri(string)</code>: Override the default prefix in S3 for symlink location e.g. s3://other-bucket/path/</li> <li><code>create_db_input(table)</code>: if this is specified, then it indicates we want to create a new database for the table export. The parameter expects a table that is converted to JSON and passed \"as is\" to AWS and is parallel to the AWS SDK CreateDatabaseInput</li> </ul> <p>When creating a glue table, the final table input will consist of the <code>create_table_input</code> input parameter and lakeFS computed defaults that will override it:</p> <ul> <li><code>Name</code> Gable table name <code>get_full_table_name(descriptor, action_info)</code>.</li> <li><code>PartitionKeys</code> Partition columns usually deduced from <code>_lakefs_tables/${table_src_path}</code>.</li> <li><code>TableType</code> = \"EXTERNAL_TABLE\"</li> <li><code>StorageDescriptor</code>: Columns usually deduced from <code>_lakefs_tables/${table_src_path}</code>.</li> <li><code>StorageDescriptor.Location</code> = symlink_location</li> </ul> <p>Example</p> <pre><code>local aws = require(\"aws\")\nlocal exporter = require(\"lakefs/catalogexport/glue_exporter\")\nlocal glue = aws.glue_client(args.aws_access_key_id, args.aws_secret_access_key, args.aws_region)\n-- table_input can be passed as a simple Key-Value object in YAML as an argument from an action, this is inline example:\nlocal table_input = {\nStorageDescriptor:\n    InputFormat: \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\"\n    OutputFormat: \"org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\"\n    SerdeInfo:\n    SerializationLibrary: \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\nParameters:\n    classification: \"parquet\"\n    EXTERNAL: \"TRUE\"\n    \"parquet.compression\": \"SNAPPY\"\n}\nexporter.export_glue(glue, \"my-db\", \"_lakefs_tables/animals.yaml\", table_input, action, {debug=true, create_db_input = {DatabaseInput = {Description=\"DB exported from LakeFS\"}, Tags = {Owner = \"Joe\"}}})\n</code></pre>"},{"location":"howto/hooks/lua/#lakefscatalogexportglue_exporterget_full_table_namedescriptor-action_info_1","title":"<code>lakefs/catalogexport/glue_exporter.get_full_table_name(descriptor, action_info)</code>","text":"<p>Generate glue table name.</p> <p>Parameters:</p> <ul> <li><code>descriptor(Table)</code>: Object from (e.g. _lakefs_tables/my_table.yaml).</li> <li><code>action_info(Table)</code>: The global action object.</li> </ul>"},{"location":"howto/hooks/lua/#lakefscatalogexportunity_exporter","title":"<code>lakefs/catalogexport/unity_exporter</code>","text":"<p>A package used to register exported Delta Lake tables to Databricks' Unity catalog.</p>"},{"location":"howto/hooks/lua/#lakefscatalogexportunity_exporterregister_tablesaction-table_descriptors_path-delta_table_details-databricks_client-warehouse_id","title":"<code>lakefs/catalogexport/unity_exporter.register_tables(action, table_descriptors_path, delta_table_details, databricks_client, warehouse_id)</code>","text":"<p>The function used to register exported Delta Lake tables in Databricks' Unity Catalog. The registration will use the following paths to register the table: <code>&lt;catalog&gt;.&lt;branch name&gt;.&lt;table_name&gt;</code> where the branch name will be used as the schema name. The return value is a table with mapping of table names to registration request status.</p> <p>Note: (Azure users) Databricks catalog external locations is supported only for ADLS Gen2 storage accounts. When exporting Delta tables using the <code>lakefs/catalogexport/delta_exporter.export_delta_log</code> function, the <code>path_transformer</code> must be used to convert the paths scheme to <code>abfss</code>. The built-in <code>azure</code> Lua library provides this functionality with <code>transformPathToAbfss</code>.</p> <p>Parameters:</p> <ul> <li><code>action(table)</code>: The global action table</li> <li><code>table_descriptors_path(string)</code>: The path under which the table descriptors of the provided <code>table_paths</code> reside.</li> <li><code>delta_table_details(table)</code>: Table names to physical paths mapping and table metadata (e.g. <code>{table1 = {path = \"s3://mybucket/mytable1\", metadata = {id = \"table_1_id\", name = \"table1\", ...}}, table2 = {path = \"s3://mybucket/mytable2\", metadata = {id = \"table_2_id\", name = \"table2\", ...}}}</code>.)</li> <li><code>databricks_client(table)</code>: A Databricks client that implements <code>create_or_get_schema: function(id, catalog_name)</code> and <code>register_external_table: function(table_name, physical_path, warehouse_id, catalog_name, schema_name)</code></li> <li><code>warehouse_id(string)</code>: Databricks warehouse ID.</li> </ul> <p>Example</p> <p>The following registers an exported Delta Lake table to Unity Catalog.</p> <pre><code>local databricks = require(\"databricks\")\nlocal unity_export = require(\"lakefs/catalogexport/unity_exporter\")\n\nlocal delta_table_locations = {\n[\"table1\"] = \"s3://mybucket/mytable1\",\n}\n-- Register the exported table in Unity Catalog:\nlocal action_details = {\nrepository_id = \"my-repo\",\ncommit_id = \"commit_id\",\nbranch_id = \"main\",\n}\nlocal databricks_client = databricks.client(\"&lt;DATABRICKS_HOST&gt;\", \"&lt;DATABRICKS_TOKEN&gt;\")\nlocal registration_statuses = unity_export.register_tables(action_details, \"_lakefs_tables\", delta_table_locations, databricks_client, \"&lt;WAREHOUSE_ID&gt;\")\n\nfor t, status in pairs(registration_statuses) do\nprint(\"Unity catalog registration for table \\\"\" .. t .. \"\\\" completed with status: \" .. status .. \"\\n\")\nend\n</code></pre> <p>For the table descriptor under the <code>_lakefs_tables/delta-table-descriptor.yaml</code>: <pre><code>---\nname: my_table_name\ntype: delta\npath: path/to/delta/table/data\ncatalog: my-catalog\n</code></pre></p> <p>For detailed step-by-step guide on how to use <code>unity_exporter.register_tables</code> as a part of a lakeFS action refer to the Unity Catalog docs.</p>"},{"location":"howto/hooks/lua/#pathparsepath_string","title":"<code>path/parse(path_string)</code>","text":"<p>Returns a table for the given path string with the following structure:</p> <p>Example</p> <pre><code>&gt; require(\"path\")\n&gt; path.parse(\"a/b/c.csv\")\n{\n    [\"parent\"] = \"a/b/\"\n    [\"base_name\"] = \"c.csv\"\n}\n</code></pre>"},{"location":"howto/hooks/lua/#pathjoinpath_parts","title":"<code>path/join(*path_parts)</code>","text":"<p>Receives a variable number of strings and returns a joined string that represents a path:</p> <p>Example</p> <pre><code>&gt; path = require(\"path\")\n&gt; path.join(\"/\", \"path/\", \"to\", \"a\", \"file.data\")\npath/o/a/file.data\n</code></pre>"},{"location":"howto/hooks/lua/#pathis_hiddenpath_string-seperator-prefix","title":"<code>path/is_hidden(path_string [, seperator, prefix])</code>","text":"<p>returns a boolean - <code>true</code> if the given path string is hidden (meaning it starts with <code>prefix</code>) - or if any of its parents start with <code>prefix</code>.</p> <p>Example</p> <pre><code>&gt; require(\"path\")\n&gt; path.is_hidden(\"a/b/c\") -- false\n&gt; path.is_hidden(\"a/b/_c\") -- true\n&gt; path.is_hidden(\"a/_b/c\") -- true\n&gt; path.is_hidden(\"a/b/_c/\") -- true\n</code></pre>"},{"location":"howto/hooks/lua/#pathdefault_separator","title":"<code>path/default_separator()</code>","text":"<p>Returns a constant string (<code>/</code>)</p>"},{"location":"howto/hooks/lua/#regexpmatchpattern-s","title":"<code>regexp/match(pattern, s)</code>","text":"<p>Returns true if the string <code>s</code> matches <code>pattern</code>. This is a thin wrapper over Go's regexp.MatchString.</p>"},{"location":"howto/hooks/lua/#regexpquote_metas","title":"<code>regexp/quote_meta(s)</code>","text":"<p>Escapes any meta-characters in string <code>s</code> and returns a new string</p>"},{"location":"howto/hooks/lua/#regexpcompilepattern","title":"<code>regexp/compile(pattern)</code>","text":"<p>Returns a regexp match object for the given pattern</p>"},{"location":"howto/hooks/lua/#regexpcompiled_patternfind_alls-n","title":"<code>regexp/compiled_pattern.find_all(s, n)</code>","text":"<p>Returns a table list of all matches for the pattern, (up to <code>n</code> matches, unless <code>n == -1</code> in which case all possible matches will be returned)</p>"},{"location":"howto/hooks/lua/#regexpcompiled_patternfind_all_submatchs-n","title":"<code>regexp/compiled_pattern.find_all_submatch(s, n)</code>","text":"<p>Returns a table list of all sub-matches for the pattern, (up to <code>n</code> matches, unless <code>n == -1</code> in which case all possible matches will be returned). Submatches are matches of parenthesized subexpressions (also known as capturing groups) within the regular expression, numbered from left to right in order of opening parenthesis. Submatch 0 is the match of the entire expression, submatch 1 is the match of the first parenthesized subexpression, and so on</p>"},{"location":"howto/hooks/lua/#regexpcompiled_patternfinds","title":"<code>regexp/compiled_pattern.find(s)</code>","text":"<p>Returns a string representing the left-most match for the given pattern in string <code>s</code></p>"},{"location":"howto/hooks/lua/#regexpcompiled_patternfind_submatchs","title":"<code>regexp/compiled_pattern.find_submatch(s)</code>","text":"<p>find_submatch returns a table of strings holding the text of the leftmost match of the regular expression in <code>s</code> and the matches, if any, of its submatches</p>"},{"location":"howto/hooks/lua/#stringssplits-sep","title":"<code>strings/split(s, sep)</code>","text":"<p>returns a table of strings, the result of splitting <code>s</code> with <code>sep</code>.</p>"},{"location":"howto/hooks/lua/#stringstrims","title":"<code>strings/trim(s)</code>","text":"<p>Returns a string with all leading and trailing white space removed, as defined by Unicode</p>"},{"location":"howto/hooks/lua/#stringsreplaces-old-new-n","title":"<code>strings/replace(s, old, new, n)</code>","text":"<p>Returns a copy of the string s with the first n non-overlapping instances of <code>old</code> replaced by <code>new</code>. If <code>old</code> is empty, it matches at the beginning of the string and after each UTF-8 sequence, yielding up to k+1 replacements for a k-rune string.</p> <p>If n &lt; 0, there is no limit on the number of replacements</p>"},{"location":"howto/hooks/lua/#stringshas_prefixs-prefix","title":"<code>strings/has_prefix(s, prefix)</code>","text":"<p>Returns <code>true</code> if <code>s</code> begins with <code>prefix</code></p>"},{"location":"howto/hooks/lua/#stringshas_suffixs-suffix","title":"<code>strings/has_suffix(s, suffix)</code>","text":"<p>Returns <code>true</code> if <code>s</code> ends with <code>suffix</code></p>"},{"location":"howto/hooks/lua/#stringscontainss-substr","title":"<code>strings/contains(s, substr)</code>","text":"<p>Returns <code>true</code> if <code>substr</code> is contained anywhere in <code>s</code></p>"},{"location":"howto/hooks/lua/#timenow","title":"<code>time/now()</code>","text":"<p>Returns a <code>float64</code> representing the amount of nanoseconds since the unix epoch (01/01/1970 00:00:00).</p>"},{"location":"howto/hooks/lua/#timeformatepoch_nano-layout-zone","title":"<code>time/format(epoch_nano, layout, zone)</code>","text":"<p>Returns a string representation of the given epoch_nano timestamp for the given Timezone (e.g. <code>\"UTC\"</code>, <code>\"America/Los_Angeles\"</code>, ...) The <code>layout</code> parameter should follow Go's time layout format.</p>"},{"location":"howto/hooks/lua/#timeformat_isoepoch_nano-zone","title":"<code>time/format_iso(epoch_nano, zone)</code>","text":"<p>Returns a string representation of the given <code>epoch_nano</code> timestamp for the given Timezone (e.g. <code>\"UTC\"</code>, <code>\"America/Los_Angeles\"</code>, ...) The returned string will be in ISO8601 format.</p>"},{"location":"howto/hooks/lua/#timesleepduration_ns","title":"<code>time/sleep(duration_ns)</code>","text":"<p>Sleep for <code>duration_ns</code> nanoseconds</p>"},{"location":"howto/hooks/lua/#timesinceepoch_nano","title":"<code>time/since(epoch_nano)</code>","text":"<p>Returns the amount of nanoseconds elapsed since <code>epoch_nano</code></p>"},{"location":"howto/hooks/lua/#timeaddepoch_time-duration_table","title":"<code>time/add(epoch_time, duration_table)</code>","text":"<p>Returns a new timestamp (in nanoseconds passed since 01/01/1970 00:00:00) for the given <code>duration</code>. The <code>duration</code> should be a table with the following structure:</p> <p>Example</p> <pre><code>&gt; require(\"time\")\n&gt; time.add(time.now(), {\n    [\"hour\"] = 1,\n    [\"minute\"] = 20,\n    [\"second\"] = 50\n})\n</code></pre> <p>You may omit any of the fields from the table, resulting in a default value of <code>0</code> for omitted fields</p>"},{"location":"howto/hooks/lua/#timeparselayout-value","title":"<code>time/parse(layout, value)</code>","text":"<p>Returns a <code>float64</code> representing the amount of nanoseconds since the unix epoch (01/01/1970 00:00:00). This timestamp will represent date <code>value</code> parsed using the <code>layout</code> format.</p> <p>The <code>layout</code> parameter should follow Go's time layout format</p>"},{"location":"howto/hooks/lua/#timeparse_isovalue","title":"<code>time/parse_iso(value)</code>","text":"<p>Returns a <code>float64</code> representing the amount of nanoseconds since the unix epoch (01/01/1970 00:00:00 for <code>value</code>. The <code>value</code> string should be in ISO8601 format</p>"},{"location":"howto/hooks/lua/#uuidnew","title":"<code>uuid/new()</code>","text":"<p>Returns a new 128-bit RFC 4122 UUID in string representation.</p>"},{"location":"howto/hooks/lua/#neturl","title":"<code>net/url</code>","text":"<p>Provides a <code>parse</code> function parse a URL string into parts, returns a table with the URL's host, path, scheme, query and fragment.</p> <p>Example</p> <pre><code>&gt; local url = require(\"net/url\")\n&gt; url.parse(\"https://example.com/path?p1=a#section\")\n{\n    [\"host\"] = \"example.com\"\n    [\"path\"] = \"/path\"\n    [\"scheme\"] = \"https\"\n    [\"query\"] = \"p1=a\"\n    [\"fragment\"] = \"section\"\n}\n</code></pre>"},{"location":"howto/hooks/lua/#nethttp-optional","title":"<code>net/http</code> (optional)","text":"<p>Provides a <code>request</code> function that performs an HTTP request. For security reasons, this package is not available by default as it enables http requests to be sent out from the lakeFS instance network. The feature should be enabled under <code>actions.lua.net_http_enabled</code> configuration. Request will time out after 30 seconds.</p> <p>Example</p> <pre><code>http.request(url [, body])\nhttp.request{\nurl = string,\n[method = string,]\n[headers = header-table,]\n[body = string,]\n}\n</code></pre> <p>Returns a code (number), body (string), headers (table) and status (string).</p> <ul> <li>code - status code number</li> <li>body - string with the response body</li> <li>headers - table with the response request headers (key/value or table of values)</li> <li>status - status code text</li> </ul> <p>The first form of the call will perform GET requests or POST requests if the body parameter is passed.</p> <p>The second form accepts a table and allows you to customize the request method and headers.</p> <p>Example of a GET request</p> <p>Example</p> <pre><code>local http = require(\"net/http\")\nlocal code, body = http.request(\"&lt;https://example.com&gt;\")\nif code == 200 then\n    print(body)\nelse\n    print(\"Failed to get example.com - status code: \" .. code)\nend\n</code></pre> <p>Example of a POST request</p> <pre><code>local http = require(\"net/http\")\nlocal code, body = http.request{\n    url=\"https://httpbin.org/post\",\n    method=\"POST\",\n    body=\"custname=tester\",\n    headers={[\"Content-Type\"]=\"application/x-www-form-urlencoded\"},\n}\nif code == 200 then\n    print(body)\nelse\n    print(\"Failed to post data - status code: \" .. code)\nend\n</code></pre>"},{"location":"howto/hooks/webhooks/","title":"Webhooks","text":"<p>A Webhook is a Hook type that sends an HTTP POST request to the configured URL. Any non <code>2XX</code> response by the responding endpoint will fail the Hook, cancel the execution of the following Hooks under the same Action. For <code>pre-*</code> hooks, the triggering operation will also be aborted.</p> <p>Warning</p> <p>You should not use <code>pre-*</code> webhooks for long-running tasks, since they block the performed operation. Moreover, the branch is locked during the execution of <code>pre-*</code> hooks, so the webhook server cannot perform any write operations on the branch (like uploading or commits).</p>"},{"location":"howto/hooks/webhooks/#action-file-webhook-properties","title":"Action File Webhook properties","text":"<p>See the Action configuration for overall configuration schema and details.</p> Property Description Data Type Required Default Value Env Vars Support url The URL address of the request String true no timeout Time to wait for response before failing the hook String (golang's Duration representation) false 1 minute no query_params List of query params that will be added to the request Dictionary(String:String or String:List(String) false yes headers Headers to add to the request Dictionary(String:String) false yes <p>Secrets &amp; Environment Variables</p> <p>lakeFS Actions supports secrets by using environment variables. The format <code>{% raw %}{{{% endraw %} ENV.SOME_ENV_VAR {% raw %}}}{% endraw %}</code> will be replaced with the value of <code>$SOME_ENV_VAR</code> during the execution of the action. If that environment variable doesn't exist in the lakeFS server environment, the action run will fail.</p> <p>Info</p> <p>All environment variables need to begin with \"LAKEFSACTIONS_\". Otherwise, they will be blocked. Additionally, the <code>actions.env.enabled</code> configuration parameter can be set to <code>false</code> to block access to all environment variables.</p> <p>Example</p> <pre><code>...\nhooks:\n- id: prevent_user_columns\n    type: webhook\n    description: Ensure no user_* columns under public/\n    properties:\n    url: \"http://&lt;host:port&gt;/webhooks/schema\"\n    timeout: 1m30s\n    query_params:\n        disallow: [\"user_\", \"private_\"]\n        prefix: public/\n    headers:\n        secret_header: \"{% raw %}{{{% endraw %} ENV.MY_SECRET {% raw %}}}{% endraw %}\"\n...\n</code></pre>"},{"location":"howto/hooks/webhooks/#request-body-schema","title":"Request body schema","text":"<p>Upon execution, a webhook will send a request containing a JSON object with the following fields:</p> Field Description Type event_type Type of the event that triggered the Action string event_time Time of the event that triggered the Action (RFC3339 formatted) string action_name Containing Hook Action's Name string hook_id ID of the Hook string repository_id ID of the Repository string branch_id<sup>1</sup> ID of the Branch string source_ref Reference to the source on which the event was triggered string commit_message<sup>2</sup> The message for the commit (or merge) that is taking place string committer<sup>2</sup> Name of the committer string commit_metadata<sup>2</sup> The metadata for the commit that is taking place string commit_id<sup>3</sup> The ID of the commit that is being created string tag_id The ID of the created/deleted tag (available for Tag events) string merge_source The source branch/tag/ref on merge (available for Merge events) string <p>Example</p> <pre><code>{\n    \"event_type\": \"pre-merge\",\n    \"event_time\": \"2021-02-28T14:03:31Z\",\n    \"action_name\": \"test action\",\n    \"hook_id\": \"prevent_user_columns\",\n    \"repository_id\": \"repo1\",\n    \"branch_id\": \"feature-1\",\n    \"source_ref\": \"feature-1\",\n    \"commit_message\": \"merge commit message\",\n    \"commit_id\": \"5891b5b522d5df086d0ff0b110fbd9d21bb4fc7163af34d08286a2e846f6be03\",\n    \"committer\": \"committer\",\n    \"commit_metadata\": {\n        \"key\": \"value\"\n    }\n}\n</code></pre> <ol> <li> <p>N/A for Tag events\u00a0\u21a9</p> </li> <li> <p>N/A for Tag and Create/Delete Branch events\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Available for Commit/Merge events only. In Merge, this represents the merge commit ID to be created if the merge operation succeeds.\u00a0\u21a9</p> </li> </ol>"},{"location":"integrations/airbyte/","title":"Airbyte","text":"<p>Airbyte is an open-source platform for syncing data from applications, APIs, and databases to warehouses, lakes, and other destinations. You can use Airbyte's connectors to get your data pipelines to consolidate many input sources.</p> <p>The integration between Airbyte and lakeFS brings resilience and manageability when you use Airbyte connectors to sync data to your S3 buckets by leveraging lakeFS branches and atomic commits and merges.</p>"},{"location":"integrations/airbyte/#use-cases","title":"Use cases","text":"<p>You can take advantage of lakeFS consistency guarantees and Data Lifecycle Management when ingesting data to S3 using lakeFS:</p> <ol> <li>Consolidate many data sources to a single branch and expose them to consumers simultaneously when merging to the <code>main</code> branch.</li> <li>Test incoming data for breaking schema changes using lakeFS hooks.</li> <li>Prevent consumers from reading partial data from connectors which failed half-way through sync.</li> <li>Experiment with ingested data on a branch before exposing it.</li> </ol>"},{"location":"integrations/airbyte/#s3-connector","title":"S3 Connector","text":"<p>lakeFS exposes an S3 Gateway that enables applications to communicate with lakeFS the same way they would with Amazon S3. You can use Airbyte's S3 Connector to upload data to lakeFS.</p> <p>Warning</p> <p>If using Airbyte OSS, please ensure you are using S3 destination connector version 0.3.17 or higher. Previous connector versions are not supported.</p>"},{"location":"integrations/airbyte/#configuring-lakefs-using-the-connector","title":"Configuring lakeFS using the connector","text":"<p>Set the following parameters when creating a new Destination of type S3:</p> Name Value Example Endpoint The lakeFS S3 gateway URL <code>https://cute-axolotol.lakefs-demo.io</code> S3 Bucket Name The lakeFS repository where the data will be written <code>example-repo</code> S3 Bucket Path The branch and the path where the data will be written <code>main/data/from/airbyte</code> Where <code>main</code> is the branch name, and <code>data/from/airbyte</code> is the path under the branch. S3 Bucket Region Not applicable to lakeFS, use <code>us-east-1</code> <code>us-east-1</code> S3 Key ID The lakeFS access key id used to authenticate to lakeFS. <code>AKIAlakefs12345EXAMPLE</code> S3 Access Key The lakeFS secret access key used to authenticate to lakeFS. <code>abc/lakefs/1234567bPxRfiCYEXAMPLEKEY</code> <p>The UI configuration will look as follows:</p> <p></p>"},{"location":"integrations/airflow/","title":"Using lakeFS with Apache Airflow","text":"<p>Apache Airflow is a platform that allows users to programmatically author, schedule, and monitor workflows.</p> <p>To run Airflow with lakeFS, you need to follow a few steps.</p>"},{"location":"integrations/airflow/#create-a-lakefs-connection-on-airflow","title":"Create a lakeFS connection on Airflow","text":"<p>To access the lakeFS server and authenticate with it, create a new Airflow Connection of type HTTP and add it to your DAG.  You can do that using the Airflow UI or the CLI. Here\u2019s an example Airflow command that does just that:</p> <pre><code>airflow connections add conn_lakefs --conn-type=HTTP --conn-host=http://&lt;LAKEFS_ENDPOINT&gt; \\\n    --conn-extra='{\"access_key_id\":\"&lt;LAKEFS_ACCESS_KEY_ID&gt;\",\"secret_access_key\":\"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\"}'\n</code></pre>"},{"location":"integrations/airflow/#install-the-lakefs-airflow-package","title":"Install the lakeFS Airflow package","text":"<p>You can use <code>pip</code> to install the package</p> <pre><code>pip install airflow-provider-lakefs\n</code></pre>"},{"location":"integrations/airflow/#use-the-package","title":"Use the package","text":""},{"location":"integrations/airflow/#operators","title":"Operators","text":"<p>The package exposes several operations to interact with a lakeFS server:</p> <ol> <li><code>CreateBranchOperator</code> creates a new lakeFS branch from the source branch (<code>main</code> by default).     <pre><code>task_create_branch = CreateBranchOperator(\n    task_id='create_branch',\n    repo='example-repo',\n    branch='example-branch',\n    source_branch='main'\n)\n</code></pre></li> <li><code>CommitOperator</code> commits uncommitted changes to a branch.     <pre><code>task_commit = CommitOperator(\n    task_id='commit',\n    repo='example-repo',\n    branch='example-branch',\n    msg='committing to lakeFS using airflow!',\n    metadata={'committed_from\": \"airflow-operator'}\n)\n</code></pre></li> <li><code>MergeOperator</code> merges 2 lakeFS branches.     <pre><code>task_merge = MergeOperator(\n    task_id='merge_branches',\n    source_ref='example-branch',\n    destination_branch='main',\n    msg='merging job outputs',\n    metadata={'committer': 'airflow-operator'}\n)\n</code></pre></li> </ol>"},{"location":"integrations/airflow/#sensors","title":"Sensors","text":"<p>Sensors are also available that allow synchronizing a running DAG with external operations:</p> <ol> <li><code>CommitSensor</code> waits until a commit has been applied to the branch     <pre><code>task_sense_commit = CommitSensor(\n    repo='example-repo',\n    branch='example-branch',\n    task_id='sense_commit'\n)\n</code></pre></li> <li><code>FileSensor</code> waits until a given file is present on a branch.     <pre><code>task_sense_file = FileSensor(\n    task_id='sense_file',\n    repo='example-repo',\n    branch='example-branch',\n    path=\"file/to/sense\"\n)\n</code></pre></li> </ol>"},{"location":"integrations/airflow/#example","title":"Example","text":"<p>This example DAG in the airflow-provider-lakeFS repository shows how to use all of these.</p>"},{"location":"integrations/airflow/#performing-other-operations","title":"Performing other operations","text":"<p>Sometimes an operator might not be supported by airflow-provider-lakeFS yet. You can access lakeFS directly by using:</p> <ul> <li><code>SimpleHttpOperator</code> to send API requests to lakeFS. </li> <li><code>BashOperator</code> with lakectl commands.</li> </ul> <p>For example, deleting a branch using <code>BashOperator</code>:</p> <pre><code>commit_extract = BashOperator(\n    task_id='delete_branch',\n    bash_command='lakectl branch delete lakefs://example-repo/example-branch',\n    dag=dag,\n)\n</code></pre>"},{"location":"integrations/athena/","title":"Using lakeFS with Amazon Athena","text":"<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.</p>"},{"location":"integrations/athena/#integration-overview","title":"Integration Overview","text":"<p>To query lakeFS data from Athena, you'll need to first export data to AWS Glue, and then use Athena to query it directly. To do that, use the automated Data Catalog Exports feature, which allows you to:</p> <ul> <li>Query data directly from lakeFS branches and commits</li> <li>Access tables using branch names as schemas</li> <li>Leverage lakeFS versioning capabilities in your SQL queries</li> </ul>"},{"location":"integrations/athena/#getting-started","title":"Getting Started","text":"<p>For a complete step-by-step guide on setting up Athena with lakeFS, see the Glue Data Catalog integration guide, which includes:</p> <ol> <li>Table Configuration: Define your table schema using <code>_lakefs_tables/&lt;table&gt;.yaml</code></li> <li>Automated Export: Set up Lua hooks to export table metadata to Glue Catalog</li> <li>Query Setup: Use Athena to query your lakeFS data with branch-specific schemas</li> </ol>"},{"location":"integrations/aws_cli/","title":"Using lakeFS with AWS CLI","text":"<p>lakeFS exposes an S3-compatible API, so you can use the AWS S3 CLI to interact with objects in your repositories.</p>"},{"location":"integrations/aws_cli/#configuration","title":"Configuration","text":"<p>You would like to configure an AWS profile for lakeFS.</p> <p>To configure the lakeFS credentials, run:</p> <pre><code>aws configure --profile lakefs\n</code></pre> <p>You will be prompted to enter the AWS Access Key ID and the AWS Secret Access Key.</p> <p>It should look like this:</p> <pre><code>aws configure --profile lakefs\n# output:  \n# AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE    \n# AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n# Default region name [None]: \n# Default output format [None]:\n</code></pre>"},{"location":"integrations/aws_cli/#path-convention","title":"Path convention","text":"<p>When accessing objects in S3, you will need to use the lakeFS path convention:</p> <pre><code>s3://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT\n</code></pre>"},{"location":"integrations/aws_cli/#usage","title":"Usage","text":"<p>After configuring the credentials, this is what a command should look:</p> <pre><code>aws s3 --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    ls s3://example-repo/main/example-directory\n</code></pre> <p>You can use an alias to make it shorter and more convenient.</p>"},{"location":"integrations/aws_cli/#examples","title":"Examples","text":""},{"location":"integrations/aws_cli/#list-directory","title":"List directory","text":"<pre><code>aws --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    s3 ls s3://example-repo/main/example-directory\n</code></pre>"},{"location":"integrations/aws_cli/#copy-from-lakefs-to-lakefs","title":"Copy from lakeFS to lakeFS","text":"<pre><code>aws --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    s3 cp s3://example-repo/main/example-file-1 s3://example-repo/main/example-file-2\n</code></pre>"},{"location":"integrations/aws_cli/#copy-from-lakefs-to-a-local-path","title":"Copy from lakeFS to a local path","text":"<pre><code>aws --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    s3 cp s3://example-repo/main/example-file-1 /path/to/local/file\n</code></pre>"},{"location":"integrations/aws_cli/#copy-from-a-local-path-to-lakefs","title":"Copy from a local path to lakeFS","text":"<pre><code>aws --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    s3 cp /path/to/local/file s3://example-repo/main/example-file-1\n</code></pre>"},{"location":"integrations/aws_cli/#delete-file","title":"Delete file","text":"<pre><code>aws --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    s3 rm s3://example-repo/main/example-directory/example-file\n</code></pre>"},{"location":"integrations/aws_cli/#delete-directory","title":"Delete directory","text":"<pre><code>aws --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    s3 rm s3://example-repo/main/example-directory/ --recursive\n</code></pre>"},{"location":"integrations/aws_cli/#adding-an-alias","title":"Adding an alias","text":"<p>To make the command shorter and more convenient, you can create an alias:</p> <pre><code>alias awslfs='aws --endpoint https://lakefs.example.com --profile lakefs'\n</code></pre> <p>Now, the ls command using the alias will be as follows:</p> <pre><code>awslfs s3 ls s3://example-repo/main/example-directory\n</code></pre>"},{"location":"integrations/cloudera/","title":"Using lakeFS with Cloudera Spark","text":"<p>Use the lakeFS Hadoop FileSystem to integrate lakeFS with Cloudera Spark.</p> <p>Review Cloudera Partner Listing for the Cloudera certification of lakeFS integration with Cloudera Data Platform (CDP) and Cloudera Spark.</p>"},{"location":"integrations/databricks/","title":"Using lakeFS with Databricks","text":""},{"location":"integrations/databricks/#overview","title":"Overview","text":"<p>Databricks is a unified, open analytics platform for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale.</p> <p>In this document, we will cover the various Databricks products and how they integrate with lakeFS.</p> <p></p>"},{"location":"integrations/databricks/#databricks-compute-options","title":"Databricks Compute Options","text":"<p>Databricks offers several compute options for running workloads. All of these options can be used with lakeFS.  At a basic level, Databricks compute products are Spark clusters that run on top of cloud infrastructure and offer different configuration options.  From a lakeFS integration perspective, the main difference is how you configure the storage operations that perform read/writes to lakeFS.</p> <p>lakeFS storage operations will either use the lakeFS Hadoop Filesystem, which utilizes the lakeFS OpenAPI, or the s3a Filesystem, which uses the lakeFS S3 Gateway. In short, the lakeFS S3 Gateway is the fastest way to get started, but it routes all traffic through the lakeFS server. The lakeFS Hadoop Filesystem requires more setup, but all data transfers will occur directly on the bucket. The lakeFS Hadoop Filesystem can write to storage using the s3a filesystem or using pre-signed URLs generated by the lakeFS server. To read more about the alternatives, see the Spark integration page.</p>"},{"location":"integrations/databricks/#all-purpose-compute","title":"All-Purpose Compute","text":"<p>Provisioned compute used to analyze data in notebooks.</p> <p>When you create a Databricks compute cluster, you can configure it to use lakeFS with the lakeFS Hadoop Filesystem (see Databricks installation guide) or the lakeFS S3 Gateway. The lakeFS S3 Gateway can be configured in the notebook or during cluster setup (Advanced Options -&gt; Spark -&gt; Spark config).</p>"},{"location":"integrations/databricks/#jobs-compute","title":"Jobs Compute","text":"<p>Provisioned compute used to run automated jobs. The Databricks job scheduler automatically creates a job compute whenever a job is configured to run on new compute.</p> <p>To use lakeFS with Databricks jobs, a compute cluster needs to be configured in the cluster setup, just like with All-Purpose compute. </p> <p>Note</p> <p>Serverless compute for Databricks jobs is currently not supported.</p>"},{"location":"integrations/databricks/#sql-warehouses","title":"SQL Warehouses","text":"<p>Classic &amp; Pro warehouses are used to run SQL commands on data objects in the SQL editor or interactive notebooks. Serverless warehouses do the same, except that they are on-demand elastic compute.</p> <p>All warehouses do not allow the installation of external jars, such as the lakeFS Hadoop Filesystem. To use SQL warehouses with lakeFS, utilize the lakeFS S3 Gateway.</p>"},{"location":"integrations/databricks/#unity-catalog","title":"Unity Catalog","text":"<p>Unity Catalog is Databricks' metastore that provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces.</p> <p>lakeFS can be used with Unity Catalog to provide a versioned view of the data and the Unity Catalog metadata.</p> <p>lakeFS support for Unity Catalog differs between lakeFS OSS and lakeFS Enterprise &amp; Cloud.</p>"},{"location":"integrations/databricks/#catalog-exports-lakefs","title":"Catalog Exports lakeFS","text":"<p>Leveraging the external tables feature within Unity Catalog, you can register a Delta Lake table exported from lakeFS and access it through the unified catalog.</p> <p>Note</p> <p>lakeFS Catalog exporters offer read-only table exports.</p> <p>Catalog Exports relies on lakeFS Actions and offers a way to export changes from lakeFS to Unity Catalog.</p> <p>For the full guide on how to use Catalog Exports with Unity Catalog, see the documentation.</p>"},{"location":"integrations/databricks/#delta-lake","title":"Delta Lake","text":"<p>lakeFS supports Delta Lake tables, and it provides a versioned view of the data and the Delta Lake metadata. Read the Delta Lake docs for more information.</p>"},{"location":"integrations/delta/","title":"Using lakeFS with Delta Lake","text":"<p>Delta Lake is an open-source storage framework designed to improve performance and provide transactional guarantees to data lake tables.</p> <p>Because lakeFS is format-agnostic, you can save data in Delta format within a lakeFS repository and benefit from the advantages of both technologies. Specifically:</p> <ol> <li>ACID operations can span across multiple Delta tables.</li> <li>CI/CD hooks can validate Delta table contents, schema, or even referential integrity.</li> <li>lakeFS supports zero-copy branching for quick experimentation with full isolation.</li> </ol>"},{"location":"integrations/delta/#delta-lake-tables-from-the-lakefs-perspective","title":"Delta Lake Tables from the lakeFS Perspective","text":"<p>lakeFS is a data versioning tool, functioning at the object level. This implies that, by default, lakeFS remains agnostic to whether the objects within a Delta table location represent a table, table metadata, or data. As per the Delta Lake protocol, any modification to a table\u2014whether it involves adding data or altering table metadata\u2014results in the creation of a new object in the table's transaction log. Typically, residing under the <code>_delta_log</code> path, relative to the root of the table's directory. This new object has an incremented version compared to its predecessor.</p> <p>Consequently, when making changes to a Delta table within the lakeFS environment, these changes are reflected as changes to objects within the table location. For instance, inserting a record into a table named \"my-table,\" which is partitioned by 'category' and 'country,' is represented in lakeFS as added objects within the table prefix (i.e., the table data) and the table transaction log. </p> <p>Similarly, when performing a metadata operation such as renaming a table column, new objects are appended to the table transaction log, indicating the schema change. </p>"},{"location":"integrations/delta/#using-delta-lake-with-lakefs-from-apache-spark","title":"Using Delta Lake with lakeFS from Apache Spark","text":"<p>Note</p> <p>Given the native integration between Delta Lake and Spark, it's most common that you'll interact with Delta tables in a Spark environment</p> <p>To configure a Spark environment to read from and write to a Delta table within a lakeFS repository, you need to set the proper credentials and endpoint in the S3 Hadoop configuration, like you'd do with any Spark environment.</p> <p>Once set, you can interact with Delta tables using regular Spark path URIs. Make sure that you include the lakeFS repository and branch name:</p> <pre><code>df.write.format(\"delta\").save(\"s3a://&lt;repo-name&gt;/&lt;branch-name&gt;/path/to/delta-table\")\n</code></pre> <p>Info</p> <p>If using the Databricks Analytics Platform, see the integration guide for configuring a Databricks cluster to use lakeFS.</p> <p>To see the integration in action see this notebook in the lakeFS Samples Repository.</p>"},{"location":"integrations/delta/#using-delta-lake-with-lakefs-from-python","title":"Using Delta Lake with lakeFS from Python","text":"<p>The delta-rs library provides bindings for Python. This means that you can use Delta Lake and lakeFS directly from Python without needing Spark. Integration is done through the lakeFS S3 Gateway</p> <p>The documentation for the <code>deltalake</code> Python module details how to read, write, and query Delta Lake tables. To use it with lakeFS use an <code>s3a</code> path for the table based on your repository and branch (for example, <code>s3a://delta-lake-demo/main/my_table/</code>) and specify the following <code>storage_options</code>:</p> <pre><code>storage_options = {\n    \"AWS_ENDPOINT\": &lt;your lakeFS endpoint&gt;,\n    \"AWS_ACCESS_KEY_ID\": &lt;your lakeFS access key&gt;,\n    \"AWS_SECRET_ACCESS_KEY\": &lt;your lakeFS secret key&gt;,\n    \"AWS_REGION\": \"us-east-1\",\n    \"AWS_S3_ALLOW_UNSAFE_RENAME\": \"true\"\n}\n</code></pre> <p>If your lakeFS is not using HTTPS (for example, you're just running it locally) then add the option</p> <pre><code>\"AWS_STORAGE_ALLOW_HTTP\": \"true\"\n</code></pre> <p>To see the integration in action see this notebook in the lakeFS Samples Repository.</p>"},{"location":"integrations/delta/#exporting-delta-lake-tables-from-lakefs-into-unity-catalog","title":"Exporting Delta Lake tables from lakeFS into Unity Catalog","text":"<p>This option is for users who are managing Delta Lake tables with lakeFS and access them through Databricks Unity Catalog. lakeFS offers a Data Catalog Export functionality that provides read-only access to your Delta tables from within Unity catalog. Using the data catalog exporters, you can work on Delta tables in isolation and easily explore them within the Unity Catalog.</p> <p>Once exported, you can query the versioned table data with:</p> <pre><code>SELECT * FROM my_catalog.main.my_delta_table\n</code></pre> <p>Here, <code>main</code> is the name of the lakeFS branch from which the delta table was exported.</p> <p>To enable Delta table exports to Unity catalog use the Unity catalog integration guide.</p>"},{"location":"integrations/delta/#limitations","title":"Limitations","text":"Multi-Writer Support in lakeFS for Delta Lake Tables <p>lakeFS currently supports a single writer for Delta Lake tables. Attempting to utilize multiple writers for writing to a Delta table may result in two types of issues:</p> <ol> <li>Merge Conflicts: These conflicts arise when multiple writers modify a Delta table on different branches, and an attempt is made to merge these branches.     </li> <li>Concurrent File Overwrite: This issue occurs when multiple writers concurrently modify a Delta table on the same branch.     </li> </ol> <p>Note</p> <p>lakeFS currently lacks its own implementation for a LogStore, and the default Log store used does not control concurrency.</p> <p>To address these limitations, consider following best practices for implementing multi-writer support.</p>"},{"location":"integrations/delta/#best-practices","title":"Best Practices","text":""},{"location":"integrations/delta/#implementing-multi-writer-support-through-lakefs-branches-and-merges","title":"Implementing Multi-Writer Support through lakeFS Branches and Merges","text":"<p>To achieve safe multi-writes to a Delta Lake table on lakeFS, we recommend following these best practices:</p> <ol> <li>Isolate Changes: Make modifications to your table in isolation. Each set of changes should be associated with a dedicated lakeFS branch, branching off from the main branch.</li> <li>Merge Atomically: After making changes in isolation, try to merge them back into the main branch. This approach guarantees that the integration of changes is cohesive.</li> </ol> <p>The workflow involves:</p> <ul> <li>Creating a new lakeFS branch from the main branch for any table change.</li> <li>Making modifications in isolation.</li> <li>Attempting to merge the changes back into the main branch.</li> <li>Iterating the process in case of a merge failure due to conflicts.</li> </ul> <p>The diagram below provides a visual representation of how branches and merges can be utilized to manage concurrency effectively: </p>"},{"location":"integrations/delta/#follow-vacuum-by-garbage-collection","title":"Follow Vacuum by Garbage Collection","text":"<p>To delete unused files from a table directory while working with Delta Lake over lakeFS you need to first use Delta lake Vacuum to soft-delete the files, and then use lakeFS Garbage Collection to hard-delete them from the storage.</p> <p>Tip</p> <p>lakeFS enables you to recover from undesired vacuum runs by reverting the changes done by a vacuum run before running Garbage Collection.</p>"},{"location":"integrations/delta/#when-running-lakefs-inside-your-vpc-on-aws","title":"When running lakeFS inside your VPC (on AWS)","text":"<p>When lakeFS runs inside your private network, your Databricks cluster needs to be able to access it. This can be done by setting up a VPC peering between the two VPCs (the one where lakeFS runs and the one where Databricks runs). For this to work on Delta Lake tables, you would also have to disable multi-cluster writes with:</p> <pre><code>spark.databricks.delta.multiClusterWrites.enabled false\n</code></pre>"},{"location":"integrations/delta/#using-multi-cluster-writes-on-aws","title":"Using multi cluster writes (on AWS)","text":"<p>When using multi-cluster writes, Databricks overrides Delta\u2019s S3-commit action.</p> <p>The new action tries to contact lakeFS from servers on Databricks\u2019 own AWS account, which of course won\u2019t be able to access your private network. So, if you must use multi-cluster writes, you\u2019ll have to allow access from Databricks\u2019 AWS account to lakeFS. If you are trying to achieve that, please reach out on Slack and the community will try to assist.</p>"},{"location":"integrations/delta/#further-reading","title":"Further Reading","text":"<p>See Guaranteeing Consistency in Your Delta Lake Tables With lakeFS post on the lakeFS blog to learn how to guarantee data quality in a Delta table by utilizing lakeFS branches.</p>"},{"location":"integrations/dremio/","title":"Using lakeFS with Dremio","text":"<p>Dremio is a next-generation data lake engine that liberates your data with live,  interactive queries directly on cloud data lake storage, including S3 and lakeFS.</p>"},{"location":"integrations/dremio/#iceberg-rest-catalog","title":"Iceberg REST Catalog","text":"<p>lakeFS Iceberg REST Catalog allow you to use lakeFS as a spec-compliant Apache Iceberg REST catalog, allowing Dremio to manage and access tables using a standard REST API.</p> <p></p> <p>This is the recommended way to use lakeFS with Dremio, as it allows lakeFS to stay completely outside the data path: data itself is read and written by Dremio executors, directly to the underlying object store. Metadata is managed by Iceberg at the table level, while lakeFS keeps track of new snapshots to provide versioning and isolation.</p> <p>Read more about using the Iceberg REST Catalog.</p>"},{"location":"integrations/dremio/#configuration","title":"Configuration","text":"<p>To configure Dremio to work with the Iceberg REST Catalog, you need to configure the Iceberg REST Catalog in Dremio.</p> <ol> <li>On the Datasets page, to the right of Sources in the left panel, click <code>+</code></li> <li> <p>In the Add Data Source dialog, under Lakehouse Catalogs, select Iceberg REST Catalog Source. The New Iceberg REST Catalog Source dialog box appears, which contains the following tabs:</p> <ol> <li>In General \u2192<ul> <li>Enter a name for your Iceberg REST Catalog source, specify the endpoint URI (i.e. <code>https://lakefs.example.com/iceberg/api</code>)</li> <li>Uncheck \"Use vended credentials\"</li> </ul> </li> <li> <p>In Advanced Options \u2192 Catalog Properties, add the following key-value pairs (left = key, right = value):</p> Key Value Notes <code>oauth2-server-uri</code> <code>https://lakefs.example.com/iceberg/api/v1/oauth/tokens</code> Your lakeFS OAuth2 token endpoint (not the catalog URL). <code>credential</code> <code>&lt;lakefs_access_key&gt;:&lt;lakefs_secret_key&gt;</code> Your lakeFS credentials. <code>fs.s3a.aws.credentials.provider</code> <code>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</code> Use static AWS credentials. <code>fs.s3a.access.key</code> <code>&lt;aws_access_key_id&gt;</code> AWS key with read/write access to your data bucket. <code>fs.s3a.secret.key</code> <code>&lt;aws_secret_access_key&gt;</code> AWS secret key. <code>dremio.s3.list.all.buckets</code> <code>false</code> Avoid listing all buckets during initialization. </li> </ol> </li> <li> <p>Click Save to create the Iceberg REST Catalog source.</p> </li> </ol>"},{"location":"integrations/dremio/#data-bucket-permissions","title":"Data Bucket Permissions","text":"<p>The lakeFS Iceberg Catalog manages table metadata, while Dremio reads and writes data files directly from your underlying  storage (for example, Amazon S3).</p> <p>You must ensure that the IAM role or user Dremio uses has read/write access to your data bucket. The following AWS IAM policy provides the required permissions for direct access: </p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DremioIcebergAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;lakefs_repo_storage_namespace&gt;/_managed/\", \n                \"arn:aws:s3:::&lt;lakefs_repo_storage_namespace&gt;/_managed/*\"\n            ]\n        },\n        {\n            \"Sid\": \"BucketLevelRequiredForDremio\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": \"arn:aws:s3:::&lt;lakefs_repo_storage_namespace_bucket_name&gt;\"\n        }\n    ]\n}\n</code></pre> <p>Tip</p> <p>To learn more about the Iceberg REST Catalog, see the Iceberg REST Catalog documentation.</p>"},{"location":"integrations/dremio/#using-dremio-with-the-s3-gateway","title":"Using Dremio with the S3 Gateway","text":"<p>Alternatively, you can use the S3 Gateway to read and write data to lakeFS from Dremio.</p> <p>While flexible, this approach requires lakeFS to be involved in the data path, which can be less efficient than the Iceberg  REST Catalog approach, since lakeFS has to proxy all data operations through the lakeFS server. This is particularly true  for large data sets where network bandwidth might incur some overhead.</p>"},{"location":"integrations/dremio/#configuration_1","title":"Configuration","text":"<p>Starting from version 3.2.3, Dremio supports Minio as an experimental S3-compatible plugin. Similarly, you can connect lakeFS with Dremio.</p> <p>Suppose you already have both lakeFS and Dremio deployed, and want to use Dremio to query your data in the lakeFS repositories. You can follow the steps listed below to configure on Dremio UI:</p> <ol> <li>click Add Data Lake.</li> <li>Under File Stores, choose Amazon S3.</li> <li>Under Advanced Options, check Enable compatibility mode (experimental).</li> <li>Under Advanced Options &gt; Connection Properties, add <code>fs.s3a.path.style.access</code> and set the value to <code>true</code>.</li> <li>Under Advanced Options &gt; Connection Properties, add <code>fs.s3a.endpoint</code> and set lakeFS S3 endpoint to the value. </li> <li>Under the General tab, specify the access_key_id and secret_access_key provided by lakeFS server.</li> <li>Click Save, and now you should be able to browse lakeFS repositories on Dremio.</li> </ol>"},{"location":"integrations/duckdb/","title":"Using lakeFS with DuckDB","text":"<p>DuckDB is an in-process SQL OLAP database management system. You can access data in lakeFS from DuckDB, as well as use DuckDB from within the web interface of lakeFS</p>"},{"location":"integrations/duckdb/#accessing-lakefs-from-duckdb","title":"Accessing lakeFS from DuckDB","text":"<p>The recommended way to access lakeFS from DuckDB is to use the Iceberg REST Catalog. </p> <p></p> <p>This allows you to query and update Iceberg tables using a standards-compliant catalog, built into lakeFS Enterprise. In this mode, lakeFS stays completely outside the data path: data itself is read and written by DuckDB executors, directly to the underlying object store. Metadata is managed by Iceberg at the table level, while lakeFS keeps track of new snapshots to provide versioning and isolation.</p> <pre><code>LOAD iceberg;\nLOAD httpfs;\n\nCREATE SECRET lakefs_credentials (\n    TYPE ICEBERG,\n    CLIENT_ID 'AKIAIOSFODNN7EXAMPLE',\n    CLIENT_SECRET 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n    OAUTH2_SERVER_URI 'https://lakefs.example.com/iceberg/api/v1/oauth/tokens'\n);\n\nATTACH '' AS main_branch (\n    TYPE iceberg,\n    SECRET lakefs_credentials,\n    ENDPOINT 'https://lakefs.example.com/iceberg/relative_to/my-repo.main/api'\n);\n\nUSE main_branch.inventory;\nSELECT * FROM books;\n</code></pre> <p>Tip</p> <p>To learn more about the Iceberg REST Catalog, see the Iceberg REST Catalog documentation.</p>"},{"location":"integrations/duckdb/#using-duckdb-with-the-s3-gateway","title":"Using DuckDB with the S3 Gateway","text":"<p>Using the S3 Gateway allows reading and writing data to lakeFS from DuckDB, in any format supported by DuckDB (i.e. not just Iceberg tables). While flexible, this approach requires lakeFS to be involved in the data path, which can be less efficient than the Iceberg REST Catalog approach, since lakeFS has to proxy all data operations through the lakeFS server.</p>"},{"location":"integrations/duckdb/#configuration","title":"Configuration","text":"<p>Querying data in lakeFS from DuckDB is similar to querying data in S3 from DuckDB. It is done using the httpfs extension connecting to the S3 Gateway that lakeFS provides.</p> <p>If not loaded already, install and load the <code>HTTPFS</code> extension: </p> <pre><code>INSTALL httpfs;\nLOAD httpfs;\n</code></pre> <p>Then run the following to configure the connection. </p> <pre><code>-- \"s3_region\" is the S3 region on which your bucket resides. If local storage, or not S3, then just set it to \"us-east-1\".\nSET s3_region='us-east-1';\n-- the host (and port, if necessary) of your lakeFS server\nSET s3_endpoint='lakefs.example.com';\n-- the access credentials for your lakeFS user\nSET s3_access_key_id='AKIAIOSFODNN7EXAMPLE'; \n-- the access credentials for your lakeFS user\nSET s3_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'; \nSET s3_url_style='path';\n\n-- Uncomment in case the endpoint listen on non-secure, for example running lakeFS locally.\n-- SET s3_use_ssl=false;\n</code></pre>"},{"location":"integrations/duckdb/#querying-data","title":"Querying Data","text":"<p>Once configured, you can query data using the lakeFS S3 Gateway using the following URI pattern:</p> <pre><code>s3://&lt;REPOSITORY NAME&gt;/&lt;REFERENCE ID&gt;/&lt;PATH TO DATA&gt;\n</code></pre> <p>Since the S3 Gateway implemenets all S3 functionality required by DuckDB, you can query using globs and patterns, including support for Hive-partitioned data.</p> <p>Example:</p> <pre><code>SELECT * \nFROM parquet_scan('s3://example-repo/main/data/population/by-region/*.parquet', HIVE_PARTITIONING=1) \nORDER BY name;\n</code></pre>"},{"location":"integrations/duckdb/#writing-data","title":"Writing Data","text":"<p>No special configuration required for writing to a branch. Assuming the configuration above and write permissions to a <code>dev</code> branch, a write operation would look like any DuckDB write:</p> <pre><code>CREATE TABLE sampled_population AS SELECT * \nFROM parquet_scan('s3://example-repo/main/data/population/by-region/*.parquet', HIVE_PARTITIONING=1) \nUSING SAMPLE reservoir(50000 ROWS) REPEATABLE (100);\n\nCOPY sampled_population TO 's3://example-repo/main/data/population/sample.parquet'; -- actual write happens here\n</code></pre>"},{"location":"integrations/duckdb/#using-duckdb-in-python-with-lakefs-spec","title":"Using DuckDB in Python with lakefs-spec","text":"<p>Python users can use DuckDB by leveraging the lakefs-spec package. </p> <p>Note</p> <p>This library is a third-party package and not maintained by the lakeFS developers; please file issues and bug reports directly in the lakefs-spec repository.</p> <p>Using lakefs-spec, querying lakeFS could be done using pre-signed URLs, allowing for efficient and secure I/O, where the data files are read directly from the underlying object store.</p> <pre><code>import duckdb\nfrom fsspec import filesystem\n\nduckdb.register_filesystem(filesystem('lakefs'))\n\nduckdb.sql(\"SELECT * FROM 'lakefs://example-repo/main/data/population/sample.parquet'\")\n</code></pre>"},{"location":"integrations/duckdb/#using-duckdb-in-the-lakefs-web-ui","title":"Using DuckDB in the lakeFS web UI","text":"<p>The lakeFS web UI includes DuckDB in the Object viewer page. </p> <p></p> <p>Using this you can query objects in lakeFS directly using a <code>lakefs</code> path: </p> <pre><code>lakefs://&lt;repository&gt;/&lt;branch&gt;/object/path/foo.parquet\n</code></pre> <p>The DuckDB query editor is provided by DuckDB WASM. It renders and provides querying capabilities for any objects of the following types:</p> <ul> <li>Parquet</li> <li>CSV</li> <li>TSV</li> </ul>"},{"location":"integrations/git/","title":"Integrating lakeFS with Git","text":"<p>Integrating code and data version control systems allows you to associate code versions with data versions, facilitating the reproduction of complex environments with multiple components. Consequently, lakeFS, a data version control system,  seamlessly integrates with Git. This combination establishes a robust foundation for versioning both your code and data,  fostering a streamlined and reproducible development process.</p>"},{"location":"integrations/git/#use-cases","title":"Use Cases","text":""},{"location":"integrations/git/#develop-reproducible-ml-models","title":"Develop Reproducible ML Models","text":"<p>Maintain a comprehensive record of both model code versions and input data versions to ensure the reproducibility of ML  model results.</p> <p>The common way to develop reproducible ML models with lakeFS is to use the  lakectl local command. See Working with lakeFS Data Locally  to understand how to use lakectl local in conjunction with Git to develop reproducible ML models.    </p>"},{"location":"integrations/git/#develop-reproducible-etl-pipelines","title":"Develop Reproducible ETL Pipelines","text":"<p>Track code versions for each step in ETL pipelines along with the corresponding data versions of their inputs and outputs. This approach allows straight forward troubleshooting and reproduction of data errors. </p> <p>Check out this lakeFS sample  that demonstrates how Git, Airflow, and lakeFS can be integrated to result in reproducible ETL pipelines.   </p>"},{"location":"integrations/glue_hive_metastore/","title":"Using lakeFS with the Glue/Hive Metastore","text":"<p>The integration between lakeFS and Glue/Hive Metastore is handled through Data Catalog Exports.</p>"},{"location":"integrations/glue_hive_metastore/#integration-features","title":"Integration Features","text":"<ul> <li>Automated Export: Hook-based automation that exports table metadata when specific events occur</li> <li>Branch-aware Schemas: Query different branches as separate schemas in your query engine</li> <li>Multiple Format Support: Works with Hive tables, Delta Lake, and other formats</li> </ul>"},{"location":"integrations/glue_hive_metastore/#available-integration-guides","title":"Available Integration Guides","text":"<p>Choose the integration guide that matches your use case:</p>"},{"location":"integrations/glue_hive_metastore/#for-aws-glue-data-catalog-users","title":"For AWS Glue Data Catalog Users","text":"<p>If you're using AWS Glue as your metastore (which backs Amazon Athena), see the comprehensive Glue Data Catalog integration guide.</p>"},{"location":"integrations/glue_hive_metastore/#for-hive-metastore-users","title":"For Hive Metastore Users","text":"<p>If you're using Hive Metastore directly, refer to the Data Catalog Exports documentation which covers:</p> <ul> <li>Table descriptor configuration (<code>_lakefs_tables/&lt;table&gt;.yaml</code>)</li> <li>Symlink exporter for Hive-compatible formats</li> <li>Hook configuration for automated exports</li> </ul>"},{"location":"integrations/glue_metastore/","title":"Using lakeFS with the Glue Catalog","text":""},{"location":"integrations/glue_metastore/#overview","title":"Overview","text":"<p>The integration between Glue and lakeFS is based on Data Catalog Exports.</p> <p>This guide describes how to use lakeFS with the Glue Data Catalog. You'll be able to query your lakeFS data by specifying the repository, branch and commit in your SQL query. Currently, only read operations are supported on the tables. You will set up the automation required to work with lakeFS on top of the Glue Data Catalog, including:</p> <ol> <li>Create a table descriptor under <code>_lakefs_tables/&lt;your-table&gt;.yaml</code>. This will represent your table schema.</li> <li>Write an exporter script that will:</li> <li>Mirror your branch's state into Hive Symlink files readable by Athena.</li> <li>Export the table descriptors from your branch to the Glue Catalog.</li> <li>Set up lakeFS hooks to trigger the above script when specific events occur.</li> </ol>"},{"location":"integrations/glue_metastore/#example-using-athena-to-query-lakefs-data","title":"Example: Using Athena to query lakeFS data","text":""},{"location":"integrations/glue_metastore/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have: 1. An active lakeFS installation with S3 as the backing storage, and a repository in this installation. 2. A database in Glue Data Catalog (lakeFS does not create one). 3. AWS Credentials with permission to manage Glue, Athena Query and S3 access.</p>"},{"location":"integrations/glue_metastore/#add-table-descriptor","title":"Add table descriptor","text":"<p>Let's define a table, and commit it to lakeFS.  Save the YAML below as <code>animals.yaml</code> and upload it to lakeFS. </p> <pre><code>lakectl fs upload lakefs://catalogs/main/_lakefs_tables/animals.yaml -s ./animals.yaml &amp;&amp; \\\nlakectl commit lakefs://catalogs/main -m \"added table\"\n</code></pre> <pre><code>name: animals\ntype: hive\n# data location root in lakeFS\npath: tables/animals\n# partitions order\npartition_columns: ['type', 'weight']\nschema:\n  type: struct\n  # all the columns spec\n  fields:\n    - name: type\n      type: string\n      nullable: true\n      metadata:\n        comment: axolotl, cat, dog, fish etc\n    - name: weight\n      type: integer\n      nullable: false\n      metadata: {}\n    - name: name\n      type: string\n      nullable: false\n      metadata: {}\n</code></pre>"},{"location":"integrations/glue_metastore/#write-some-table-data","title":"Write some table data","text":"<p>Insert data into the table path, using your preferred method (e.g. Spark), and commit upon completion. This example uses CSV files, and the files added to lakeFS should look like this:</p> <p></p>"},{"location":"integrations/glue_metastore/#the-exporter-script","title":"The exporter script","text":"<p>Upload the following script to your main branch under <code>scripts/animals_exporter.lua</code> (or a path of your choice).</p> <p>Note</p> <p>For code references check symlink_exporter and glue_exporter docs.</p> <pre><code>local aws = require(\"aws\")\nlocal symlink_exporter = require(\"lakefs/catalogexport/symlink_exporter\")\nlocal glue_exporter = require(\"lakefs/catalogexport/glue_exporter\")\n-- settings \nlocal access_key = args.aws.aws_access_key_id\nlocal secret_key = args.aws.aws_secret_access_key\nlocal region = args.aws.aws_region\nlocal table_path = args.table_source -- table descriptor \nlocal db = args.catalog.db_name -- glue db\nlocal table_input = args.catalog.table_input -- table input (AWS input spec) for Glue\n-- export symlinks \nlocal s3 = aws.s3_client(access_key, secret_key, region)\nlocal result = symlink_exporter.export_s3(s3, table_path, action, {debug=true})\n-- register glue table\nlocal glue = aws.glue_client(access_key, secret_key, region)\nlocal res = glue_exporter.export_glue(glue, db, table_path, table_input, action, {debug=true})\n</code></pre>"},{"location":"integrations/glue_metastore/#configure-action-hooks","title":"Configure Action Hooks","text":"<p>Hooks serve as the mechanism that triggers the execution of the exporter. For more detailed information on how to configure exporter hooks, you can refer to Running an Exporter.</p> <p>Info</p> <p>The <code>args.catalog.table_input</code> argument in the Lua script is assumed to be passed from the action arguments, that way the same script can be reused for different tables. </p> <p>heck the example to construct the table input in the lua code.</p> Hook CSV Glue TableHook Parquet Glue TableMultiple Hooks / Inline script <p>Upload to <code>_lakefs_actions/animals_glue.yaml</code>: </p> <pre><code>name: Glue Exporter\non:\n  post-commit:\n    branches: [\"main\"]\nhooks:\n  - id: animals_table_glue_exporter\n    type: lua\n    properties:\n      script_path: \"scripts/animals_exporter.lua\"\n      args:\n        aws:\n          aws_access_key_id: \"&lt;AWS_ACCESS_KEY_ID&gt;\"\n          aws_secret_access_key: \"&lt;AWS_SECRET_ACCESS_KEY&gt;\"\n          aws_region: \"&lt;AWS_REGION&gt;\"\n        table_source: '_lakefs_tables/animals.yaml'\n        catalog:\n          db_name: \"my-glue-db\"\n          table_input:\n            StorageDescriptor: \n              InputFormat: \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\"\n              OutputFormat: \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\"\n              SerdeInfo:\n                SerializationLibrary: \"org.apache.hadoop.hive.serde2.OpenCSVSerde\"\n                Parameters:\n                  separatorChar: \",\"\n            Parameters: \n              classification: \"csv\"\n              \"skip.header.line.count\": \"1\"\n</code></pre> <p>When working with Parquet files, upload the following to <code>_lakefs_actions/animals_glue.yaml</code>:</p> <pre><code>name: Glue Exporter\non:\npost-commit:\n    branches: [\"main\"]\nhooks:\n- id: animals_table_glue_exporter\n    type: lua\n    properties:\n        script_path: \"scripts/animals_exporter.lua\"\n        args:\n        aws:\n            aws_access_key_id: \"&lt;AWS_ACCESS_KEY_ID&gt;\"\n            aws_secret_access_key: \"&lt;AWS_SECRET_ACCESS_KEY&gt;\"\n            aws_region: \"&lt;AWS_REGION&gt;\"\n        table_source: '_lakefs_tables/animals.yaml'\n        catalog:\n            db_name: \"my-glue-db\"\n            table_input:\n                StorageDescriptor:\n                InputFormat: \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\"\n                OutputFormat: \"org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\"\n                SerdeInfo:\n                    SerializationLibrary: \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n                Parameters:\n                classification: \"parquet\"\n                EXTERNAL: \"TRUE\"\n                \"parquet.compression\": \"SNAPPY\"\n</code></pre> <p>The following example demonstrates how to separate the symlink and glue exporter into building blocks running in separate hooks. It also shows how to run the lua script inline instead of a file, depending on user preference.</p> <pre><code>name: Animal Table Exporter\non:\npost-commit:\n    branches: [\"main\"]\nhooks:\n- id: symlink_exporter\n    type: lua\n    properties:\n    args:\n        aws:\n        aws_access_key_id: \"&lt;AWS_ACCESS_KEY_ID&gt;\"\n        aws_secret_access_key: \"&lt;AWS_SECRET_ACCESS_KEY&gt;\"\n        aws_region: \"&lt;AWS_REGION&gt;\"\n        table_source: '_lakefs_tables/animals.yaml'\n    script: |\n        local exporter = require(\"lakefs/catalogexport/symlink_exporter\")\n        local aws = require(\"aws\")\n        local table_path = args.table_source\n        local s3 = aws.s3_client(args.aws.aws_access_key_id, args.aws.aws_secret_access_key, args.aws.aws_region)\n        exporter.export_s3(s3, table_path, action, {debug=true})\n- id: glue_exporter\n    type: lua\n    properties:\n    args:\n        aws:\n        aws_access_key_id: \"&lt;AWS_ACCESS_KEY_ID&gt;\"\n        aws_secret_access_key: \"&lt;AWS_SECRET_ACCESS_KEY&gt;\"\n        aws_region: \"&lt;AWS_REGION&gt;\"\n        table_source: '_lakefs_tables/animals.yaml'\n        catalog:\n        db_name: \"my-glue-db\"\n        table_input: # add glue table input here \n    script: |\n        local aws = require(\"aws\")\n        local exporter = require(\"lakefs/catalogexport/glue_exporter\")\n        local glue = aws.glue_client(args.aws.aws_access_key_id, args.aws.aws_secret_access_key, args.aws.aws_region)\n        exporter.export_glue(glue, args.catalog.db_name, args.table_source, args.catalog.table_input, action, {debug=true})  \n</code></pre> <p>Adding the script and the action files to the repository and commit it. This is a post-commit action, meaning it will be executed after the commit operation has taken place. </p> <pre><code>lakectl fs upload lakefs://catalogs/main/scripts/animals_exporter.lua -s ./animals_exporter.lua\nlakectl fs upload lakefs://catalogs/main/_lakefs_actions/animals_glue.yaml -s ./animals_glue.yaml\nlakectl commit lakefs://catalogs/main -m \"trigger first export hook\"\n</code></pre> <p>Once the action has completed its execution, you can review the results in the action logs.</p> <p></p>"},{"location":"integrations/glue_metastore/#use-athena","title":"Use Athena","text":"<p>We can use the exported Glue table with any tool that supports Glue Catalog (or Hive compatible) such as Athena, Trino, Spark and others. To use Athena we can simply run <code>MSCK REPAIR TABLE</code> and then query the tables.</p> <p>In Athena, make sure that the correct database (<code>my-glue-db</code> in the example above) is configured, then run: </p> <pre><code>MSCK REPAIR TABLE `animals_catalogs_main_9255e5`; -- load partitions for the first time \nSELECT * FROM `animals_catalogs_main_9255e5` limit 50;\n</code></pre> <p></p>"},{"location":"integrations/glue_metastore/#cleanup","title":"Cleanup","text":"<p>Users can use additional hooks / actions to implement a custom cleanup logic to delete the symlink in S3 and Glue Tables. </p> <pre><code>glue.delete_table(db, '&lt;glue table name&gt;')\ns3.delete_recursive('bucket', 'path/to/symlinks/of/a/commit/')\n</code></pre>"},{"location":"integrations/hive/","title":"Using lakeFS with Apache Hive","text":"<p>The Apache Hive \u2122 data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive.</p>"},{"location":"integrations/hive/#configuration","title":"Configuration","text":"<p>To configure Hive to work with lakeFS, you need to set the lakeFS credentials in the corresponding S3 credential fields.</p> <p>lakeFS endpoint: <code>fs.s3a.endpoint</code> </p> <p>lakeFS access key: <code>fs.s3a.access.key</code></p> <p>lakeFS secret key: <code>fs.s3a.secret.key</code></p> <p>Note</p> <p>In the following examples, we set AWS credentials at runtime for clarity. In production, these properties should be set using one of Hadoop's standard ways of Authenticating with S3. </p> <p>For example, you can add the configurations to the file <code>hdfs-site.xml</code>:</p> <pre><code>&lt;configuration&gt;\n    ...\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;\n        &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;\n        &lt;value&gt;https://lakefs.example.com&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n       &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt;\n       &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Info</p> <p>In this example, we set <code>fs.s3a.path.style.access</code> to true to remove the need for additional DNS records for virtual hosting <code>fs.s3a.path.style.access</code> that was introduced in Hadoop 2.8.0</p>"},{"location":"integrations/hive/#examples","title":"Examples","text":""},{"location":"integrations/hive/#example-with-schema","title":"Example with schema","text":"<pre><code>CREATE  SCHEMA example LOCATION 's3a://example/main/' ;\nCREATE TABLE example.request_logs (\n    request_time timestamp,\n    url string,\n    ip string,\n    user_agent string\n);\n</code></pre>"},{"location":"integrations/hive/#example-with-an-external-table","title":"Example with an external table","text":"<pre><code>CREATE EXTERNAL TABLE request_logs (\n    request_time timestamp,\n    url string,\n    ip string,\n    user_agent string\n) LOCATION 's3a://example/main/request_logs' ;\n</code></pre>"},{"location":"integrations/huggingface_datasets/","title":"Versioning HuggingFace Datasets with lakeFS","text":"<p>HuggingFace \ud83e\udd17 Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.</p> <p>\ud83e\udd17 Datasets supports access to cloud storage providers through fsspec FileSystem implementations.</p> <p>lakefs-spec is a community implementation of an fsspec Filesystem that fully leverages lakeFS' capabilities. Let's start by installing it:</p>"},{"location":"integrations/huggingface_datasets/#installation","title":"Installation","text":"<pre><code>pip install lakefs-spec\n</code></pre>"},{"location":"integrations/huggingface_datasets/#configuration","title":"Configuration","text":"<p>If you've already configured the lakeFS python SDK and/or lakectl, you should have a <code>$HOME/.lakectl.yaml</code> file that contains your access credentials and endpoint for your lakeFS environment.</p> <p>Otherwise, install <code>lakectl</code> and run <code>lakectl config</code> to set up your access credentials.</p>"},{"location":"integrations/huggingface_datasets/#reading-a-dataset","title":"Reading a Dataset","text":"<p>To read a dataset, all we have to do is use a <code>lakefs://...</code> URI when calling <code>load_dataset</code>:</p> <pre><code>&gt;&gt;&gt; from datasets import load_dataset\n&gt;&gt;&gt; \n&gt;&gt;&gt; dataset = load_dataset('csv', data_files='lakefs://example-repository/my-branch/data/example.csv')\n</code></pre> <p>That's it! this should automatically load the lakefs-spec implementation that we've installed, which will use the <code>$HOME/.lakectl.yaml</code> file to read its credentials, so we don't need to pass additional configuration.</p>"},{"location":"integrations/huggingface_datasets/#savingloading","title":"Saving/Loading","text":"<p>Once we've loaded a Dataset, we can save it using the <code>save_to_disk</code> method as normal:</p> <pre><code>&gt;&gt;&gt; dataset.save_to_disk('lakefs://example-repository/my-branch/datasets/example/')\n</code></pre> <p>At this point, we might want to commit that change to lakeFS, and tag it, so we could share it with our colleagues.</p> <p>We can do it through the UI or lakectl, but let's do it with the lakeFS Python SDK:</p> <pre><code>&gt;&gt;&gt; import lakefs\n&gt;&gt;&gt;\n&gt;&gt;&gt; repo = lakefs.repository('example-repository')\n&gt;&gt;&gt; commit = repo.branch('my-branch').commit(\n...     'saved my first huggingface Dataset!',\n...     metadata={'using': '\ud83e\udd17'})\n&gt;&gt;&gt; repo.tag('alice_experiment1').create(commit)\n</code></pre> <p>Now, others on our team can load our exact dataset by using the tag we created:</p> <pre><code>&gt;&gt;&gt; from datasets import load_from_disk\n&gt;&gt;&gt;\n&gt;&gt;&gt; dataset = load_from_disk('lakefs://example-repository/alice_experiment1/datasets/example/')\n</code></pre>"},{"location":"integrations/iceberg-rest-catalog/","title":"Iceberg REST Catalog","text":"<p>Info</p> <p>Available in lakeFS Enterprise</p> <p></p> <p>lakeFS implements the Iceberg REST Catalog specification, allowing any Iceberg client to work with Iceberg tables managed by lakeFS. To read more, see Iceberg Integration \u2192</p>"},{"location":"integrations/iceberg/","title":"Using lakeFS with Apache Iceberg","text":""},{"location":"integrations/iceberg/#lakefs-iceberg-rest-catalog","title":"lakeFS Iceberg REST Catalog","text":"<p>Info</p> <p>Available in lakeFS Enterprise</p> <p>Tip</p> <p>lakeFS Iceberg REST Catalog is currently in private preview for lakeFS Enterprise customers. Contact us to get started!</p>"},{"location":"integrations/iceberg/#what-is-lakefs-iceberg-rest-catalog","title":"What is lakeFS Iceberg REST Catalog?","text":"<p>lakeFS Iceberg REST Catalog allow you to use lakeFS as a spec-compliant Apache Iceberg REST catalog,  allowing Iceberg clients to manage and access tables using a standard REST API. </p> <p></p> <p>Using lakeFS Iceberg REST Catalog, you can use lakeFS a drop-in replacement for other Iceberg catalogs like AWS Glue, Nessie, Hive Metastore - or the lakeFS HadoopCatalog (see below)</p> <p>With lakeFS Iceberg REST Catalog, you can:</p> <ul> <li>Manage Iceberg tables with full version control capabilities.</li> <li>Use standard Iceberg clients and tools without modification.</li> <li>Leverage lakeFS's branching and merging features for managing table's lifecycle.</li> <li>Maintain data consistency across different environments.</li> </ul>"},{"location":"integrations/iceberg/#use-cases","title":"Use Cases","text":"<ol> <li>Version-Controlled Data Development:<ul> <li>Create feature branches for table schema changes or data migrations</li> <li>Test modifications in isolation, across multiple tables</li> <li>Merge changes safely with conflict detection</li> </ul> </li> <li>Multi-Environment Management:<ul> <li>Use branches to represent different environments (dev, staging, prod)</li> <li>Promote changes between environments through merges, with automated testing</li> <li>Maintain consistent table schemas across environments</li> </ul> </li> <li>Collaborative Data Development:<ul> <li>Multiple teams can work on different table features simultaneously</li> <li>Maintain data quality through pre-merge validations</li> <li>Collaborate using pull requests on changes to data and schema</li> </ul> </li> <li>Manage and Govern Access to data:<ul> <li>Use the detailed built-in commit log capturing who, what and how data is changed</li> <li>Manage access using fine grained access control to users and groups using RBAC policies</li> <li>Rollback changes atomically and safely to reduce time-to-recover and increase system stability</li> </ul> </li> </ol>"},{"location":"integrations/iceberg/#configuration","title":"Configuration","text":"<p>The Iceberg REST catalog API is exposed at <code>/iceberg/api</code> in your lakeFS server. </p> <p>To use it:</p> <ol> <li>Enable the feature (contact us for details).</li> <li>Configure your Iceberg clients to use the lakeFS REST catalog endpoint.</li> <li>Use your lakeFS access key and secret for authentication.</li> </ol>"},{"location":"integrations/iceberg/#catalog-initialization-example-using-pyiceberg","title":"Catalog Initialization Example (using <code>pyiceberg</code>)","text":"<pre><code>from pyiceberg.catalog.rest import RestCatalog\n\ncatalog = RestCatalog(name = \"my_catalog\", **{\n    'prefix': 'lakefs',\n    'uri': f'{lakefs_endpoint}/iceberg/api',\n    'oauth2-server-uri': f'{lakefs_endpoint}/iceberg/api/v1/oauth/tokens',\n    'credential': f'{lakefs_client_key}:{lakefs_client_secret}',\n})\n</code></pre>"},{"location":"integrations/iceberg/#example-client-code","title":"Example Client code","text":"Python (PyIceberg)TrinoSpark <pre><code>import lakefs\nfrom pyiceberg.catalog import load_catalog\n\n# Initialize the catalog\ncatalog = RestCatalog(name = \"my_catalog\", **{\n    'prefix': 'lakefs',\n    'uri': 'https://lakefs.example.com/iceberg/api',\n    'oauth2-server-uri': 'https://lakefs.example.com/iceberg/api/iceberg/api/v1/oauth/tokens',\n    'credential': f'AKIAlakefs12345EXAMPLE:abc/lakefs/1234567bPxRfiCYEXAMPLEKEY',\n})\n\n# List namespaces in a branch\ncatalog.list_namespaces(('repo', 'main'))\n\n# Query a table\ncatalog.list_tables('repo.main.inventory')\ntable = catalog.load_table('repo.main.inventory.books')\narrow_df = table.scan().to_arrow()\n</code></pre> <pre><code>-- List tables in the iceberg catalog\nUSE \"repo.main.inventory\"; -- &lt;repository&gt;.&lt;branch or reference&gt;.&lt;namespace&gt;\nSHOW TABLES;\n\n-- Query a table\nSELECT * FROM books LIMIT 100;\n\n-- Switch to a different branch\nUSE \"repo.new_branch.inventory\";\nSELECT * FROM books;\n</code></pre> <pre><code>// Configure Spark to use the lakeFS REST catalog\nspark.sql(\"USE my_repo.main.inventory\")\n\n// List available tables\nspark.sql(\"SHOW TABLES\").show()\n\n// Query data with branch isolation\nspark.sql(\"SELECT * FROM books\").show()\n\n// Switch to a feature branch\nspark.sql(\"USE my_repo.new_branch.inventory\")\nspark.sql(\"SELECT * FROM books\").show()\n</code></pre>"},{"location":"integrations/iceberg/#namespaces-and-tables","title":"Namespaces and Tables","text":""},{"location":"integrations/iceberg/#namespace-operations","title":"Namespace Operations","text":"<p>The Iceberg Catalog supports Iceberg namespace operations:</p> <ul> <li>Create namespaces</li> <li>List namespaces</li> <li>Drop namespaces</li> <li>List tables within namespaces</li> </ul>"},{"location":"integrations/iceberg/#namespace-usage","title":"Namespace Usage","text":"<p>Namespaces in the Iceberg Catalog follow the pattern <code>\"&lt;repository&gt;.&lt;branch&gt;.&lt;namespace&gt;(.&lt;namespace&gt;...)\"</code> where:</p> <ul> <li><code>&lt;repository&gt;</code> must be a valid lakeFS repository name.</li> <li><code>&lt;branch&gt;</code> must be a valid lakeFS branch name.</li> <li><code>&lt;namespace&gt;</code> components can be nested using unit separator (e.g., <code>inventory.books</code>).</li> </ul> <p>Examples:</p> <ul> <li><code>my-repo.main.inventory</code></li> <li><code>my-repo.feature-branch.inventory.books</code></li> </ul> <p>The repository and branch components must already exist in lakeFS before using them in the Iceberg catalog.</p>"},{"location":"integrations/iceberg/#relative-namespace-support","title":"Relative Namespace support","text":"<p>Some Apache Iceberg clients do not support nested namespaces.</p> <p>To support those, the lakeFS REST Catalog allows specifying relative namespaces: passing a partial namespace as part of the catalog URL endpoint (commonly, <code>&lt;repository&gt;.&lt;branch&gt;</code>). </p> <p>By doing so, all namespaces passed by the user will be relative to the namespaces passed in the URL.</p> Example: DuckDB <p>DuckDB allows limited nesting in the form of <code>&lt;database&gt;.&lt;schema&gt;</code>. When using the Iceberg REST Catalog integration, the database is replaced by the name given to the catalog. We can use relative namespaces to allow scoping the catalog connecting to a specific repository and branch: <pre><code>LOAD iceberg;\nLOAD httpfs;\n\nCREATE SECRET lakefs_credentials (\n    TYPE ICEBERG,\n    CLIENT_ID '...',\n    CLIENT_SECRET '...',\n    OAUTH2_SERVER_URI 'https://lakefs.example.com/iceberg/api/v1/oauth/tokens'\n);\n\nATTACH 'lakefs' AS main_branch (\n    TYPE iceberg,\n    SECRET lakefs_credentials,\n    -- notice the \"/relative_to/.../\" part:\n    ENDPOINT 'https://lakefs.example.com/iceberg/relative_to/my-repo.main/api'\n);\n\nUSE main_branch.inventory;\nSELECT * FROM books;\n</code></pre></p> <p>See the DuckDB documentation for a full reference on how to setup an Iceberg catalog integration.</p>"},{"location":"integrations/iceberg/#namespace-restrictions","title":"Namespace Restrictions","text":"<ul> <li>Repository and branch names must follow lakeFS naming conventions.</li> <li>Namespace components cannot contain special characters except dots (.) for nesting.</li> <li>The total namespace path length must be less than 255 characters.</li> <li>Namespaces are case-sensitive.</li> <li>Empty namespace components are not allowed.</li> </ul>"},{"location":"integrations/iceberg/#table-operations","title":"Table Operations","text":"<p>The Iceberg Catalog supports all standard Iceberg table operations:</p> <ul> <li>Create tables with schemas and partitioning.</li> <li>Update table schemas and partitioning.</li> <li>Commit changes to tables.</li> <li>Delete tables.</li> <li>List tables in namespaces.</li> </ul>"},{"location":"integrations/iceberg/#version-control-features","title":"Version Control Features","text":"<p>The Iceberg Catalog integrates with lakeFS's version control system. Table and namespace changes made through the  Iceberg API are staged on the target lakeFS reference, and may be committed to lakeFS to record these staged changes.  This provides a complete history of table modifications and enables lakeFS branching and merging workflows.</p>"},{"location":"integrations/iceberg/#catalog-changes-in-lakefs","title":"Catalog Changes in lakeFS","text":"<p>Iceberg changes (create/drop/commit to table, create/drop namespace) are reflected as staging changes on the relevant lakeFS (table/namespace) object. These changes aren't committed to lakeFS, and should be explicitly committed by using lakeFS tools.</p> <p>Note that several Iceberg operations can be grouped across multiple tables into a single lakeFS commit.</p>"},{"location":"integrations/iceberg/#branching-and-merging","title":"Branching and Merging","text":"<p>Create a new branch to work on table changes:</p> <pre><code># Create a lakeFS branch using lakeFS Python SDK\nbranch = lakefs.repository('repo').branch('new_branch').create(source_reference='main')\n\n# The table is now accessible in the new branch\nnew_table = catalog.load_table(f'repo.{branch.id}.inventory.books')\n</code></pre> <p>Merge changes between branches:</p> <pre><code># Merge the branch using lakeFS Python SDK\nbranch.merge_into('main')\n\n# Changes are now visible in main\nmain_table = catalog.load_table('repo.main.inventory.books')\n</code></pre> <p>Info</p> <p>lakeFS handles table changes as file operations during merges.</p> <p>This means that when merging branches with table changes, lakeFS treats the table metadata files as regular files.</p> <p>No special merge logic is applied to handle conflicting table changes, and if there are conflicting changes to the same table in different branches,  the merge will fail with a conflict that needs to be resolved manually.</p>"},{"location":"integrations/iceberg/#authentication","title":"Authentication","text":"<p>lakeFS provides an OAuth2 token endpoint at <code>/catalog/iceberg/v1/oauth/tokens</code> that clients need to configure.  To authenticate, clients must provide their lakeFS access key and secret in the format <code>access_key:secret</code> as the credential.</p>"},{"location":"integrations/iceberg/#authorization","title":"Authorization","text":"<p>The authorization requirements are managed at the lakeFS level, meaning:</p> <ul> <li>Users need appropriate lakeFS permissions to access repositories and branches</li> <li>Table operations require lakeFS permissions on the underlying objects</li> <li>The same lakeFS RBAC policies apply to Iceberg catalog operations</li> </ul>"},{"location":"integrations/iceberg/#limitations","title":"Limitations","text":"<ol> <li>Table Maintenance:<ul> <li>See Table Maintenance section for details</li> </ul> </li> <li>Advanced Features:<ul> <li>Views (all view operations are unsupported)</li> <li>Transactional DML (<code>stage-create</code>)</li> <li>Server-side query planning</li> <li>Table renaming</li> <li>Updating table's location (using Commit)</li> <li>Table statistics (<code>set-statistics</code> and <code>remove-statistics</code> operations are currently a no-op)</li> </ul> </li> <li>lakeFS Iceberg REST Catalog is currently tested to work with Amazon S3 and Google Cloud Storage. Other storage backends, such as Azure or Local storage are currently not supported, but will be in future releases.</li> <li>Currently only Iceberg <code>v2</code> table format is supported</li> </ol>"},{"location":"integrations/iceberg/#table-maintenance","title":"Table Maintenance","text":""},{"location":"integrations/iceberg/#compact-data-files","title":"Compact Data Files","text":"<p>In the lakeFS catalog, data integrity is maintained after data file compaction operations (such as RewriteDataFiles), since data files are not deleted by such operations (only new snapshot and manifest files are created).</p> <p>However, to avoid unnecessary merge conflicts, we recommend marking compaction operations using <code>snapshotProperty()</code>,  so that lakeFS can automatically resolve conflicts when merging branches with compaction commits.</p> <p>Requirements:</p> <ul> <li>Use the Spark Java API (not SQL procedures) with Iceberg v1.5 or newer</li> <li>Mark compaction operations with the following property and value using <code>snapshotProperty()</code>:</li> </ul> <pre><code>SparkActions.get(spark)\n    .rewriteDataFiles(table)\n    .snapshotProperty(\"lakefs.compaction.operation-id\", \"rewrite-data-files\")\n    .execute();\n</code></pre> <p>Conflict Resolution:</p> <p>When merging branches, lakeFS automatically resolves conflicts if both branches have compaction commits for the same table and at most one branch has non-compaction data changes. This allows safe merging of compacted branches without losing work.</p> <p>Info</p> <p>Expired or deleted snapshots will very likely lead to merge conflicts, as lakeFS cannot determine which snapshots to keep.</p> <p>Tip</p> <p>Frequently merge compacted branches to minimize merge conflicts.</p> <p>Tip</p> <p>Data changes are ignored in non-\"main\" Iceberg branches (\"Refs\"),  so it's advised to avoid branching in Iceberg when branching in lakeFS.</p> <p>For disabling the automatic conflict resolution,  set the <code>iceberg.ignore_compaction_commits</code> configuration flag to <code>false</code> (it defaults to <code>true</code>).</p>"},{"location":"integrations/iceberg/#unsupported-operations","title":"Unsupported Operations","text":"<p>The following table maintenance operations are not supported in the current version:</p> <ul> <li>Drop table with purge</li> <li>Rewrite manifests</li> <li>Expire snapshots</li> <li>Remove old metadata files</li> <li>Delete orphan files</li> </ul> <p>Danger</p> <p>To prevent data loss, clients should disable their own cleanup operations by:</p> <ul> <li>Disabling orphan file deletion.</li> <li>Setting <code>remove-dangling-deletes</code> to false when rewriting.</li> <li>Disabling snapshot expiration.</li> <li>Setting a very high value for <code>min-snapshots-to-keep</code> parameter.</li> </ul>"},{"location":"integrations/iceberg/#roadmap","title":"Roadmap","text":"<p>The following features are planned for future releases:</p> <ol> <li>Table Import:<ul> <li>Support for importing existing Iceberg tables from other catalogs</li> <li>Bulk import capabilities for large-scale migrations</li> </ul> </li> <li>Azure Storage Support</li> <li>Advanced Features:<ul> <li>Views API support</li> <li>Table transactions</li> </ul> </li> <li>Advanced versioning capabilities<ul> <li>merge non-conflicting table updates</li> </ul> </li> </ol>"},{"location":"integrations/iceberg/#how-it-works","title":"How it works","text":"<p>Under the hood, the lakeFS Iceberg REST Catalog keeps track of each table's metadata file. This is typically referred to as the table pointer.</p> <p>This pointer is stored inside the repository's storage namespace. </p> <p>When a request is made, the catalog would examine the table's fully qualified namespace: <code>&lt;repository&gt;.&lt;reference&gt;.&lt;namespace&gt;.&lt;table_name&gt;</code> to read that special pointer file from the given reference specified, and returns the underlying object store location of the metadata file to the client. When a table is created or updated, lakeFS would make sure to generate a new metadata file inside the storage namespace, and register that metadata file as the current pointer for the requested branch. These changes are staged in lakeFS, and may be committed to the branch.</p> <p>This approach leverages Iceberg's existing metadata and the immutability of its snapshots: a commit in lakeFS captures a metadata file, which in turn captures manifest lists, manifest files and all related data files.</p> <p>Besides simply avoiding \"double booking\" where both Iceberg and lakeFS would need to keep track of which files belong to which version, it also greatly improves the scalability and compatibility of the catalog with the existing Iceberg tool ecosystem.</p>"},{"location":"integrations/iceberg/#example-reading-an-iceberg-table","title":"Example: Reading an Iceberg Table","text":"<p>Here's a simplified example of what reading from an Iceberg table would look like:</p> <pre><code>sequenceDiagram\n    Actor Iceberg Client\n    participant lakeFS Catalog API\n    participant lakeFS\n    participant Object Store\n\n    Iceberg Client-&gt;&gt;lakeFS Catalog API: get table metadata(\"repo.branch.table\")\n    lakeFS Catalog API-&gt;&gt;lakeFS: get('repo', 'branch', 'table')\n    lakeFS-&gt;&gt;lakeFS Catalog API: physical_address\n    lakeFS Catalog API-&gt;&gt;Iceberg Client: object location (\"s3://.../metadata.json\")\n    Iceberg Client-&gt;&gt;Object Store: GetObject\n    Object Store-&gt;&gt;Iceberg Client: table data</code></pre>"},{"location":"integrations/iceberg/#example-writing-an-iceberg-table","title":"Example: Writing an Iceberg Table","text":"<p>Here's a simplified example of what writing to an Iceberg table would look like:</p> <pre><code>sequenceDiagram\n    Actor Iceberg Client\n    participant lakeFS Catalog API\n    participant lakeFS\n    participant Object Store\n    Iceberg Client-&gt;&gt;Object Store: write table data\n    Iceberg Client-&gt;&gt;lakeFS Catalog API: Iceberg commit\n    lakeFS Catalog API-&gt;&gt;lakeFS: stage('new table pointer')\n    lakeFS-&gt;&gt;Object Store: PutObject(\"metdata.json\")\n    lakeFS Catalog API-&gt;&gt;Iceberg Client: Iceberg commit done\n    Iceberg Client-&gt;&gt;lakeFS: lakeFS commit('repo','branch', message)</code></pre>"},{"location":"integrations/iceberg/#related-resources","title":"Related Resources","text":"<p>Further Reading</p> <ul> <li>Iceberg REST Catalog API Specification</li> <li>Iceberg Official Documentation</li> <li>lakeFS Enterprise Features</li> </ul>"},{"location":"integrations/iceberg/#catalog-sync","title":"Catalog Sync","text":"<p>Catalog Sync allows you to synchronize Iceberg tables between the lakeFS catalog and external catalogs such as AWS Glue Data Catalog or other Iceberg REST-compatible catalogs. This enables workflows where some users or tools need to access data through other external catalogs.</p>"},{"location":"integrations/iceberg/#use-cases_1","title":"Use Cases","text":"<p>Collaboration with External Tools: Share data with users who rely on tools that only support specific catalogs. For example, data engineers can work with tables in lakeFS while data analysts query the same data through AWS Glue using Athena, all while maintaining isolation and version control.</p> <pre><code>sequenceDiagram\n    actor Alice (Data Engineer)\n    actor Bob (Data Analyst)\n    participant lakeFS\n    participant AWS Glue\n    Alice (Data Engineer)-&gt;&gt;lakeFS: Update tables on branch 'dev'\n    Alice (Data Engineer)-&gt;&gt;lakeFS: Push tables to Glue\n    lakeFS-&gt;&gt;AWS Glue: Register tables in 'dev' database\n    Alice (Data Engineer)-&gt;&gt;Bob (Data Analyst): Please review: glue/dev\n    Bob (Data Analyst)-&gt;&gt;AWS Glue: SELECT * FROM dev.table (via Athena)\n    Bob (Data Analyst)-&gt;&gt;Alice (Data Engineer): Approved!\n    Alice (Data Engineer)-&gt;&gt;lakeFS: Merge 'dev' into 'main'</code></pre> <p>Isolated Pipelines: Run data pipelines using tools that require external catalogs while maintaining isolation through lakeFS branches. Create a branch, push tables to an external catalog, run your pipeline, pull the changes back, and merge into main.</p> <pre><code>sequenceDiagram\n    participant Orchestrator\n    participant lakeFS\n    participant External Catalog\n    participant Pipeline Tool\n    Orchestrator-&gt;&gt;lakeFS: Create branch 'etl-2024-01-15'\n    Orchestrator-&gt;&gt;lakeFS: Push tables to external catalog\n    lakeFS-&gt;&gt;External Catalog: Register tables\n    Orchestrator-&gt;&gt;Pipeline Tool: Run ETL pipeline\n    Pipeline Tool-&gt;&gt;External Catalog: Read/write tables\n    Orchestrator-&gt;&gt;lakeFS: Pull updated tables\n    lakeFS-&gt;&gt;External Catalog: Read updated metadata\n    Orchestrator-&gt;&gt;lakeFS: Merge 'etl-2024-01-15' into 'main'</code></pre>"},{"location":"integrations/iceberg/#configuration_1","title":"Configuration","text":"<p>Remote catalogs are configured in your lakeFS configuration file. Each remote catalog requires a unique identifier and type-specific connection properties.</p> <p>Note</p> <p>In case you need help configuring a remote catalog, contact support.</p>"},{"location":"integrations/iceberg/#aws-glue-data-catalog","title":"AWS Glue Data Catalog","text":"<p>Configure an AWS Glue catalog by specifying the region and AWS credentials:</p> <pre><code>iceberg_catalog:\n  remotes:\n    - id: aws_glue_us_east_1\n      type: glue\n      glue:\n        region: us-east-1\n        access_key_id: &lt;your-glue-key&gt;\n        secret_access_key: &lt;your-glue-secret&gt;\n</code></pre>"},{"location":"integrations/iceberg/#iceberg-rest-catalog","title":"Iceberg REST Catalog","text":"<p>Configure a generic Iceberg REST catalog with basic authentication:</p> <pre><code>iceberg_catalog:\n  remotes:\n    - id: remote_catalog\n      type: rest\n      rest:\n        uri: https://catalog.example.com/iceberg/api\n        credential: &lt;client-id&gt;:&lt;client-secret&gt;\n</code></pre> <p>Or with OAuth2 client credentials flow:</p> <pre><code>iceberg_catalog:\n  remotes:\n    - id: remote_catalog\n      type: rest\n      rest:\n        uri: https://catalog.example.com/iceberg/api\n        credential: &lt;client-id&gt;:&lt;client-secret&gt;\n        oauth_server_uri: https://auth.example.com/oauth/tokens\n        oauth_scope: catalog:read catalog:write\n</code></pre>"},{"location":"integrations/iceberg/#push-to-remote","title":"Push to remote","text":"<p>Push operations register a table from lakeFS into a remote catalog. The table's metadata and data remain in lakeFS-managed storage, which are used as the pushed table's location.</p> <p>API Endpoint: <code>POST /iceberg/remotes/{catalog-id}/push</code></p> <p>Parameters: - <code>source</code>: The lakeFS table location (repository, branch/reference, namespace, table name) - <code>destination</code>: The remote catalog location (namespace, table name) - <code>force_update</code>: (optional, default: <code>false</code>) Override the table if it already exists in the remote catalog - <code>create_namespace</code>: (optional, default: <code>false</code>) Create the namespace in the remote catalog if it doesn't exist</p> <p>Example:</p> <pre><code>curl -X POST \"https://lakefs.example.com/iceberg/remotes/aws_glue_us_east_1/push\" \\\n  -H \"Authorization: Bearer $LAKEFS_ACCESS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"source\": {\n      \"repository_id\": \"my-repo\",\n      \"reference_id\": \"main\",\n      \"namespace\": [\"default\", \"features\"],\n      \"table\": \"image_properties\"\n    },\n    \"destination\": {\n      \"namespace\": [\"main_features\"],\n      \"table\": \"image_properties\"\n    },\n    \"create_namespace\": true,\n    \"force_update\": true\n  }'\n</code></pre> <p>This example pushes the table <code>my-repo.main.default.features.image_properties</code> from lakeFS to the AWS Glue catalog as <code>main_features.image_properties</code>. It creates the remote namespace if needed (since <code>create_namespace: true</code>), and overwrites any existing table or possible recent updates committed to it (since <code>force_update: true</code>).</p>"},{"location":"integrations/iceberg/#pull-from-remote","title":"Pull from remote","text":"<p>Pull operations update a lakeFS table with changes from a remote catalog. This is useful after external tools have modified a table previously pushed from lakeFS.</p> <p>API Endpoint: <code>POST /iceberg/remotes/{catalog-id}/pull</code></p> <p>Parameters: - <code>source</code>: The remote catalog location (namespace, table name) - <code>destination</code>: The lakeFS table location (repository, branch/reference, namespace, table name) - <code>force_update</code>: (optional, default: <code>false</code>) Override the table in lakeFS if metadata conflicts exist - <code>create_namespace</code>: (optional, default: <code>false</code>) Create the namespace in lakeFS if it doesn't exist</p> <p>Example:</p> <pre><code>curl -X POST \"https://lakefs.example.com/iceberg/remotes/aws_glue_us_east_1/pull\" \\\n  -H \"Authorization: Bearer $LAKEFS_ACCESS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"source\": {\n      \"namespace\": [\"main_features\"],\n      \"table\": \"image_properties\"\n    },\n    \"destination\": {\n      \"repository_id\": \"my-repo\",\n      \"reference_id\": \"main\",\n      \"namespace\": [\"default\", \"features\"],\n      \"table\": \"image_properties\"\n    }\n    \"create_namespace\": true,\n    \"force_update\": true\n  }'\n</code></pre> <p>This example pulls changes from the Glue table <code>main_features.image_properties</code> back into the lakeFS table <code>my-repo.main.default.features.image_properties</code>. It creates the namespace in lakeFS if needed (since <code>create_namespace: true</code>), and overwrites any existing table or possible recent updates committed to it (since <code>force_update: true</code>).</p>"},{"location":"integrations/iceberg/#important-notes","title":"Important Notes","text":"<ol> <li> <p>Storage Location: Pulled tables' <code>metadata.json</code> file must reside in a storage location to which lakeFS has read access to, or the <code>pull</code> operation will fail.</p> </li> <li> <p>Atomicity: Push and pull operations are not atomic. If an operation fails partway through, manual intervention may be required    (contact support in such a case if needed).</p> </li> <li> <p>Authentication: Ensure the credentials configured for remote catalogs have appropriate permissions:</p> <ul> <li>For AWS Glue: <code>glue:CreateTable</code>, <code>glue:UpdateTable</code>, <code>glue:GetTable</code>, <code>glue:CreateDatabase</code> (if <code>create_namespace</code> is used)</li> <li>For REST catalogs: Appropriate OAuth scopes for table and namespace operations</li> </ul> </li> <li> <p>Namespace Format: Namespaces are represented as arrays of strings to support nested namespaces (e.g., <code>[\"accounting\", \"tax\"]</code> represents <code>accounting.tax</code>).</p> </li> </ol>"},{"location":"integrations/iceberg/#deprecated-iceberg-hadoopcatalog","title":"Deprecated: Iceberg HadoopCatalog","text":"<p>Warning</p> <p><code>HadoopCatalog</code> and other filesystem-based catalogs are currently not recommended by the Apache Iceberg community and come with several limitations around concurrency and tooling.</p> <p>As such, the <code>HadoopCatalog</code> described in this section is now deprecated and will not receive further updates</p> Setup MavenPySpark <p>Use the following Maven dependency to install the lakeFS custom catalog:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.lakefs&lt;/groupId&gt;\n&lt;artifactId&gt;lakefs-iceberg&lt;/artifactId&gt;\n&lt;version&gt;0.1.4&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Include the <code>lakefs-iceberg</code> jar in your package list along with Iceberg. For example: </p> <pre><code>.config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.0,io.lakefs:lakefs-iceberg:0.1.4\")\n</code></pre> Configuration PySparkSpark Shell <p>Set up the Spark SQL catalog:  <pre><code>.config(\"spark.sql.catalog.lakefs\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n.config(\"spark.sql.catalog.lakefs.catalog-impl\", \"io.lakefs.iceberg.LakeFSCatalog\") \\\n.config(\"spark.sql.catalog.lakefs.warehouse\", f\"lakefs://{repo_name}\") \\ \n.config(\"spark.sql.catalog.lakefs.cache-enabled\", \"false\")\n</code></pre></p> <p>Configure the S3A Hadoop FileSystem with your lakeFS connection details. Note that these are your lakeFS endpoint and credentials, not your S3 ones.</p> <pre><code>.config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n.config(\"spark.hadoop.fs.s3a.endpoint\", \"https://example-org.us-east-1.lakefscloud.io\") \\\n.config(\"spark.hadoop.fs.s3a.access.key\", \"AKIAIO5FODNN7EXAMPLE\") \\\n.config(\"spark.hadoop.fs.s3a.secret.key\", \"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\") \\\n.config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n</code></pre> <pre><code>spark-shell --conf spark.sql.catalog.lakefs=\"org.apache.iceberg.spark.SparkCatalog\" \\\n    --conf spark.sql.catalog.lakefs.catalog-impl=\"io.lakefs.iceberg.LakeFSCatalog\" \\\n    --conf spark.sql.catalog.lakefs.warehouse=\"lakefs://example-repo\" \\\n    --conf spark.sql.catalog.lakefs.cache-enabled=\"false\" \\\n    --conf spark.hadoop.fs.s3.impl=\"org.apache.hadoop.fs.s3a.S3AFileSystem\" \\\n    --conf spark.hadoop.fs.s3a.endpoint=\"https://example-org.us-east-1.lakefscloud.io\" \\\n    --conf spark.hadoop.fs.s3a.access.key=\"AKIAIO5FODNN7EXAMPLE\" \\\n    --conf spark.hadoop.fs.s3a.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\\n    --conf spark.hadoop.fs.s3a.path.style.access=\"true\"\n</code></pre> Using Iceberg tables with HadoopCatalog Create a table <p>To create a table on your main branch, use the following syntax:</p> <pre><code>CREATE TABLE lakefs.main.db1.table1 (id int, data string);\n</code></pre> Insert data into the table <pre><code>INSERT INTO lakefs.main.db1.table1 VALUES (1, 'data1');\nINSERT INTO lakefs.main.db1.table1 VALUES (2, 'data2');\n</code></pre> Create a branch <p>We can now commit the creation of the table to the main branch:</p> <pre><code>lakectl commit lakefs://example-repo/main -m \"my first iceberg commit\"\n</code></pre> <p>Then, create a branch:</p> <pre><code>lakectl branch create lakefs://example-repo/dev -s lakefs://example-repo/main\n</code></pre> Make changes on the branch <p>We can now make changes on the branch:</p> <pre><code>INSERT INTO lakefs.dev.db1.table1 VALUES (3, 'data3');\n</code></pre> Query the table <p>If we query the table on the branch, we will see the data we inserted:</p> <pre><code>SELECT * FROM lakefs.dev.db1.table1;\n</code></pre> <p>Results in:</p> <pre><code>+----+------+\n| id | data |\n+----+------+\n| 1  | data1|\n| 2  | data2|\n| 3  | data3|\n+----+------+\n</code></pre> <p>However, if we query the table on the main branch, we will not see the new changes:</p> <pre><code>SELECT * FROM lakefs.main.db1.table1;\n</code></pre> <p>Results in:</p> <pre><code>+----+------+\n| id | data |\n+----+------+\n| 1  | data1|\n| 2  | data2|\n+----+------+\n</code></pre> Migrating an existing Iceberg table to the Hadoop Catalog <p>This is done through an incremental copy from the original table into lakeFS. </p> <ol> <li>Create a new lakeFS repository <code>lakectl repo create lakefs://example-repo &lt;base storage path&gt;</code></li> <li> <p>Initiate a spark session that can interact with the source iceberg table and the target lakeFS catalog.      Here's an example of Hadoop and S3 session and lakeFS catalog with per-bucket config: </p> <p><pre><code>SparkConf conf = new SparkConf();\nconf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\");\n\n// set hadoop on S3 config (source tables we want to copy) for spark\nconf.set(\"spark.sql.catalog.hadoop_prod\", \"org.apache.iceberg.spark.SparkCatalog\");\nconf.set(\"spark.sql.catalog.hadoop_prod.type\", \"hadoop\");\nconf.set(\"spark.sql.catalog.hadoop_prod.warehouse\", \"s3a://my-bucket/warehouse/hadoop/\");\nconf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\");\nconf.set(\"spark.hadoop.fs.s3a.bucket.my-bucket.access.key\", \"&lt;AWS_ACCESS_KEY&gt;\");\nconf.set(\"spark.hadoop.fs.s3a.bucket.my-bucket.secret.key\", \"&lt;AWS_SECRET_KEY&gt;\");\n\n// set lakeFS config (target catalog and repository)\nconf.set(\"spark.sql.catalog.lakefs\", \"org.apache.iceberg.spark.SparkCatalog\");\nconf.set(\"spark.sql.catalog.lakefs.catalog-impl\", \"io.lakefs.iceberg.LakeFSCatalog\");\nconf.set(\"spark.sql.catalog.lakefs.warehouse\", \"lakefs://example-repo\");\nconf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\");\nconf.set(\"spark.hadoop.fs.s3a.bucket.example-repo.access.key\", \"&lt;LAKEFS_ACCESS_KEY&gt;\");\nconf.set(\"spark.hadoop.fs.s3a.bucket.example-repo.secret.key\", \"&lt;LAKEFS_SECRET_KEY&gt;\");\nconf.set(\"spark.hadoop.fs.s3a.bucket.example-repo.endpoint\"  , \"&lt;LAKEFS_ENDPOINT&gt;\");\n</code></pre> 3. Create Schema in lakeFS and copy the data</p> <p>Example of copy with spark-sql: </p> <pre><code>-- Create Iceberg Schema in lakeFS\nCREATE SCHEMA IF NOT EXISTS &lt;lakefs-catalog&gt;.&lt;branch&gt;.&lt;db&gt;\n-- Create new iceberg table in lakeFS from the source table (pre-lakeFS)\nCREATE TABLE IF NOT EXISTS &lt;lakefs-catalog&gt;.&lt;branch&gt;.&lt;db&gt; USING iceberg AS SELECT * FROM &lt;iceberg-original-table&gt;\n</code></pre> </li> </ol>"},{"location":"integrations/kafka/","title":"Using lakeFS with Apache Kafka","text":"<p>Apache Kafka provides a unified, high-throughput, low-latency platform for handling real-time data feeds.</p> <p>Different distributions of Kafka offer different methods for exporting data to S3 called Kafka Sink Connectors.</p> <p>The most commonly used Connector for S3 is Confluent's S3 Sink Connector.</p> <p>Add the following to <code>connector.properties</code> file for lakeFS support:</p> <pre><code># Your lakeFS repository\ns3.bucket.name=example-repo\n\n# Your lakeFS S3 endpoint and credentials\nstore.url=https://lakefs.example.com\naws.access.key.id=AKIAIOSFODNN7EXAMPLE\naws.secret.access.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n# main being the branch we want to write to\ntopics.dir=main/topics \n</code></pre> <p>Example</p> <p>For usage examples, see the lakeFS Kafka sample repo. </p>"},{"location":"integrations/kubeflow/","title":"Using lakeFS with Kubeflow pipelines","text":"<p>Kubeflow is a project dedicated to making deployments of ML workflows on Kubernetes simple, portable, and scalable. A Kubeflow pipeline is a portable and scalable definition of an ML workflow composed of steps. Each step in the pipeline is an instance of a component represented as an instance of ContainerOp.</p>"},{"location":"integrations/kubeflow/#add-pipeline-steps-for-lakefs-operations","title":"Add pipeline steps for lakeFS operations","text":"<p>To integrate lakeFS into your Kubeflow pipeline, you need to create Kubeflow components that perform lakeFS operations. Currently, there are two methods to create lakeFS <code>ContainerOps</code>:</p> <ol> <li>Implement a function-based ContainerOp that uses the lakeFS Python API to invoke lakeFS operations.</li> <li>Implement a ContainerOp that uses the <code>lakectl</code> CLI docker image to invoke lakeFS operations.</li> </ol>"},{"location":"integrations/kubeflow/#function-based-containerops","title":"Function-based ContainerOps","text":"<p>To implement a function-based component that invokes lakeFS operations, you should use the Python OpenAPI client lakeFS provides. See the example below that demonstrates how to make the client's package available to your ContainerOp.</p>"},{"location":"integrations/kubeflow/#example-operations","title":"Example operations","text":"<p>Create a new branch: A function-based ContainerOp that creates a branch called <code>example-branch</code> based on the <code>main</code> branch of <code>example-repo</code>.</p> <pre><code>from kfp import components\n\ndef create_branch(repo_name, branch_name, source_branch):\n    import lakefs\n    from lakefs.client import Client\n    client = Client(\n        host=\"https://lakefs.example.com\",\n        username=\"AKIAIOSFODNN7EXAMPLE\",\n        password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n    )\n    lakefs.repository(repo_name, client=client).branch(branch_name).create(source_reference=source_branch)\n\n# Convert the function to a lakeFS pipeline step.\ncreate_branch_op = components.func_to_container_op(\n    func=create_branch,\n    packages_to_install=['lakefs'])\n</code></pre> <p>You can invoke any lakeFS operation supported by lakeFS OpenAPI. For example, you could implement a commit and merge function-based ContainerOps. Check out the Python documentation and the full API reference.</p>"},{"location":"integrations/kubeflow/#non-function-based-containerops","title":"Non-function-based ContainerOps","text":"<p>To implement a non-function based ContainerOp, you should use the <code>treeverse/lakectl</code> docker image. With this image, you can run lakectl commands to execute the desired lakeFS operation.</p> <p>For <code>lakectl</code> to work with Kubeflow, you will need to pass your lakeFS configurations as environment variables named:</p> <ul> <li><code>LAKECTL_CREDENTIALS_ACCESS_KEY_ID: AKIAIOSFODNN7EXAMPLE</code></li> <li><code>LAKECTL_SECRET_ACCESS_KEY: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</code></li> <li><code>LAKECTL_SERVER_ENDPOINT_URL: https://lakefs.example.com</code></li> </ul>"},{"location":"integrations/kubeflow/#example-operations_1","title":"Example operations","text":"<ol> <li>Commit changes to a branch: A ContainerOp that commits uncommitted changes to <code>example-branch</code> on <code>example-repo</code>.     <pre><code>from kubernetes.client.models import V1EnvVar\n\ndef commit_op():\n    return dsl.ContainerOp(\n    name='commit',\n    image='treeverse/lakectl',\n    arguments=['commit', 'lakefs://example-repo/example-branch', '-m', 'commit message']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com'))\n</code></pre></li> <li>Merge two lakeFS branches: A ContainerOp that merges <code>example-branch</code> into the <code>main</code> branch of <code>example-repo</code>.     <pre><code>def merge_op():\n    return dsl.ContainerOp(\n    name='merge',\n    image='treeverse/lakectl',\n    arguments=['merge', 'lakefs://example-repo/example-branch', 'lakefs://example-repo/main']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com'))\n</code></pre></li> </ol> <p>You can invoke any lakeFS operation supported by <code>lakectl</code> by implementing it as a ContainerOp. Check out the complete CLI reference for the list of supported operations.</p> <p>Note</p> <p>The lakeFS Kubeflow integration that uses <code>lakectl</code> is supported on lakeFS version &gt;= v0.43.0.</p>"},{"location":"integrations/kubeflow/#add-the-lakefs-steps-to-your-pipeline","title":"Add the lakeFS steps to your pipeline","text":"<p>Add the steps created in the previous step to your pipeline before compiling it.</p>"},{"location":"integrations/kubeflow/#example-pipeline","title":"Example pipeline","text":"<p>A pipeline that implements a simple ETL that has steps for branch creation and commits.</p> <pre><code>def lakectl_pipeline():\n    create_branch_task = create_branch_op('example-repo', 'example-branch', 'main') # A function-based component\n    extract_task = example_extract_op()\n    commit_task = commit_op()\n    transform_task = example_transform_op()\n    commit_task = commit_op()\n    load_task = example_load_op()\n</code></pre> <p>Info</p> <p>It's recommended to store credentials as Kubernetes secrets and pass them as environment variables to Kubeflow operations using V1EnvVarSource.</p>"},{"location":"integrations/lancedb/","title":"Using lakeFS with LanceDB","text":"<p>LanceDB is a vector database that allows you to store and query vector data.</p> <p>LanceDB works directly on an object store, so you can use it to store and query vector data in lakeFS.</p>"},{"location":"integrations/lancedb/#configuring-lancedb-to-work-with-lakefs","title":"Configuring LanceDB to work with lakeFS","text":"<p>To configure LanceDB to work with lakeFS, configure it to use the lakeFS S3 Gateway:</p> <pre><code>import lancedb  # pip install lancedb\n\ndb = lancedb.connect(\n    # structure: s3://&lt;repository ID&gt;/&lt;branch&gt;/&lt;path&gt;\n    uri=\"s3://example-repo/example-branch/path/to/lancedb\",\n    storage_options={\n        # Your lakeFS S3 Gateway\n        \"endpoint\": \"https://example.lakefs.io\",\n        # Access key and secret of a lakeFS user with permissions \n        #  to read and write data from that path\n        \"access_key_id\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"secret_access_key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n    }\n)\n\n# table \"vectors\" on the branch \"example-branch\" in the repository \"example-repo\"\ntable = db.open_table('vectors')\n\n# update and query some data!\ntable.add([{'id': '1', 'embedding': generate_embedding('some data')}])\ndf = db.open_table('vectors').search(generate_embedding('some other data')).limit(10).to_pandas()\n</code></pre> <p>Tip</p> <p>For more information on configuring and using LanceDB, see the LanceDB documentation.</p>"},{"location":"integrations/lancedb/#use-cases","title":"Use Cases","text":"<p>Running LanceDB on top of lakeFS has a few major benefits:</p>"},{"location":"integrations/lancedb/#multimodal-data-storage","title":"Multimodal data storage","text":"<p>In many cases, LanceDB stores embeddings of data that exists elsewhere: documents, images, text files etc. These \"raw\" data files are processed to extract embeddings, which are then stored in LanceDB - but they are also stored in their raw form for retrieval, and in some cases metadata about them is stored in other formats for use in a data warehouse or data lake.</p> <p>By co-locating these embeddings together with the other modalities, you can perform more complex queries and analysis without giving up on consistency: a commit will capture both the vector embeddings, raw data, and metadata as one atomic unit.</p> <p></p>"},{"location":"integrations/lancedb/#differential-processing-of-new-data","title":"Differential processing of new data","text":"<p>lakeFS provides a highly performant and scalable way understand how data changes over time. For example, say we store raw image data in <code>images/</code> - we can update the raw data by adding, removing or updating images in the <code>images/</code> directory - and lakeFS will capture the changes as a commit.</p> <p>This allows you to perform differential processing of new data: If we have an <code>images</code> table in LanceDB we can keep track of the latest commit represented in that table. As new data arrives, we can update our embeddings with the latest commit by diffing the previous commit and the new one, resulting in a minimal set of embeddings to add, remove or update:</p> <p></p>"},{"location":"integrations/lancedb/#ensuring-high-quality-data-with-write-audit-publish-hooks","title":"Ensuring high quality data with Write, Audit, Publish hooks","text":"<p>Using lakeFS hooks, you can ensure the vector embeddings meet a certain threshold for quality before they are made available for inference. These quality checks can be triggered automatically before new data is merged into a <code>main</code> or <code>production</code> branch. These tests could be:</p> <ul> <li>Coverage: How many of the images in the dataset have been processed and have embeddings? How many embeddings point to images no longer in the dataset?</li> <li>Governance: Are the embeddings consistent with the data policy? Do they contain any PII? </li> <li>Accuracy: Are the embeddings accurate? Do they match the expected output?</li> <li>Drift: Are metrics like centroid shift and norm drift within acceptable limits?</li> </ul> <p>If any of these tests fail, the commit will be rejected and the data will not be made available for inference. This ensures that the data is of high quality and that it is consistent with the data policy.</p> <p></p>"},{"location":"integrations/lancedb/#traceable-reproducible-inference","title":"Traceable &amp; Reproducible inference","text":"<p>Once deployed on lakeFS, querying the vector embeddings has to be done by specifying the branch, tag or commit ID of the data you want to query. Capturing the commit ID of the data you want to query allows you to reproduce the exact same results at any point in time. A common approach is to capture this commit ID in inference logs - allowing you to reproduce the exact same results as the user or agent that originally made the query.</p> <p>While this sounds simple, vector databases often change quite frequently over time, making it hard to answer questions like \"why did this customer complain about the chatbot being rude?\" or \"why did this product recommendation not work for this user?\".</p> <p>By tying that commit ID to the query, we can even go further and see the raw data as it existed at that point in time, complete with a commit log of who introduced that change, when and why.</p> <p></p>"},{"location":"integrations/matlab/","title":"MATLAB Integration","text":"<p>Integrate lakeFS with MATLAB for data version control in MATLAB data science, data engineering, and machine learning workflows.</p>"},{"location":"integrations/matlab/#overview","title":"Overview","text":"<p>The lakeFS MATLAB integration provides native MATLAB functions to interact with lakeFS repositories, enabling:</p> <ul> <li>Version control for data: Track datasets, models, and results across experiments</li> <li>Reproducible research: Link code to specific data versions with commit IDs</li> <li>Isolated experimentation: Create branches for parallel experiments without data duplication</li> <li>Direct data access: Mount lakeFS repositories as local directories using Everest</li> </ul> <p>This integration uses two MATLAB helper classes: - <code>lakefs.m</code>: Core lakeFS operations (branches, commits, tags, metadata) - everest.m: File system mounting for direct data access</p>"},{"location":"integrations/matlab/#prerequisites","title":"Prerequisites","text":"<p>Before using lakeFS with MATLAB, ensure you have:</p> <ol> <li>lakeFS Server: Running instance (local or cloud)</li> <li>Quick Start Guide</li> <li> <p>Cloud Setup</p> </li> <li> <p>lakectl CLI: Installed and configured</p> </li> <li>lakectl Installation</li> <li> <p>Configuration file at <code>~/.lakectl.yaml</code></p> </li> <li> <p>Everest (optional, for mounting): File system interface</p> </li> <li>Everest Documentation</li> <li> <p>Install: <code>brew install treeverse/brew/everest</code> (macOS)</p> </li> <li> <p>MATLAB: Version R2023a or later recommended</p> </li> </ol>"},{"location":"integrations/matlab/#installation","title":"Installation","text":""},{"location":"integrations/matlab/#download-helper-files","title":"Download Helper Files","text":"<p>Download the MATLAB helper classes: - <code>lakefs.m</code> - Core lakeFS operations - <code>everest.m</code> - File system mounting</p> <p>Place both files in your MATLAB project directory or add to your MATLAB path:</p> <pre><code>% Add helpers to MATLAB path\naddpath('/path/to/helpers');\n</code></pre>"},{"location":"integrations/matlab/#configure-lakefs-connection","title":"Configure lakeFS Connection","text":"<p>Create your lakeFS configuration by copying and editing the template:</p> <p><code>lakefs_template.m</code>: <pre><code>classdef lakefs_template\n    % LAKEFS_TEMPLATE - Configuration template for lakeFS connection\n    %\n    % SETUP:\n    % 1. Copy this file to lakefs.m in your project directory\n    % 2. Fill in your credentials below\n    % 3. Keep lakefs.m out of version control (.gitignore)\n\n    properties(Constant)\n        % === LOCAL CONFIGURATION ===\n        LOCAL_ENDPOINT = 'http://localhost:8000'\n        LOCAL_ACCESS_KEY = 'YOUR_ACCESS_KEY_HERE'\n        LOCAL_SECRET_KEY = 'YOUR_SECRET_KEY_HERE'\n\n        % === CLOUD CONFIGURATION ===\n        CLOUD_ENDPOINT = 'https://your-org.us-east-1.lakefscloud.io'\n        CLOUD_ACCESS_KEY = 'YOUR_CLOUD_ACCESS_KEY_HERE'\n        CLOUD_SECRET_KEY = 'YOUR_CLOUD_SECRET_KEY_HERE'\n\n        % === CURRENT CONFIGURATION (will be set by configure()) ===\n        % These will be populated when you call lakefs.configure()\n    end\n\n    properties(Access=private)\n        current_endpoint\n        current_access_key\n        current_secret_key\n    end\n\n    methods(Static)\n        function configure(environment)\n            % CONFIGURE - Set active lakeFS environment\n            % \n            % Syntax:\n            %   lakefs.configure('local')   % Use local lakeFS\n            %   lakefs.configure('cloud')   % Use cloud lakeFS\n        end\n    end\nend\n</code></pre></p> <p>Configuration steps:</p> <ol> <li>Copy <code>lakefs_template.m</code> to <code>lakefs.m</code></li> <li>Edit credentials in <code>lakefs.m</code></li> <li>Add <code>lakefs.m</code> to <code>.gitignore</code> to keep credentials private</li> <li>Set active environment in your scripts:</li> </ol> <pre><code>% Use local lakeFS instance\nlakefs.configure('local');\n\n% Or use cloud lakeFS instance\nlakefs.configure('cloud');\n</code></pre>"},{"location":"integrations/matlab/#basic-usage","title":"Basic Usage","text":""},{"location":"integrations/matlab/#repository-operations","title":"Repository Operations","text":"<p>List branches: <pre><code>% List all branches in a repository\nbranches = lakefs.list_branches('my-repo');\ndisp(branches.branch);\n</code></pre></p> <p>List tags: <pre><code>% List all tags\ntags = lakefs.list_tags('my-repo');\ndisp(tags.tag);\n</code></pre></p> <p>View commit history: <pre><code>% Get commit log for a branch\ncommits = lakefs.log('my-repo', 'main', 'amount', 5);\nfor i = 1:height(commits)\n    fprintf('%s: %s\\n', commits.id{i}(1:12), commits.message{i});\nend\n</code></pre></p>"},{"location":"integrations/matlab/#branch-management","title":"Branch Management","text":"<p>Create a branch: <pre><code>% Create experiment branch from main\nlakefs.create_branch('my-repo', 'experiment-1', 'main');\n</code></pre></p> <p>Compare branches: <pre><code>% Show differences between branches\ndiff = lakefs.diff('my-repo', 'main', 'experiment-1');\nfprintf('Changed objects: %d\\n', height(diff));\n</code></pre></p>"},{"location":"integrations/matlab/#data-operations","title":"Data Operations","text":"<p>Upload data: <pre><code>% Upload local file to lakeFS branch\nlakefs.upload('my-repo', 'experiment-1', ...\n    'data/results.mat', ...      % Destination in lakeFS\n    'local/output/results.mat');  % Local source file\n</code></pre></p> <p>Download data: <pre><code>% Download file from lakeFS\nlakefs.download('my-repo', 'main', ...\n    'models/trained_model.mat', ...  % Source in lakeFS\n    'local/models/model.mat');        % Local destination\n</code></pre></p> <p>Commit changes: <pre><code>% Commit with metadata\nmetadata = struct();\nmetadata.accuracy = '0.945';\nmetadata.training_time = '120.5';\nmetadata.dataset = 'v1.2';\n\nlakefs.commit('my-repo', 'experiment-1', ...\n    'Trained model with improved accuracy', ...\n    'metadata', metadata);\n</code></pre></p>"},{"location":"integrations/matlab/#mounting-everest","title":"Mounting (Everest)","text":"<p>For direct file system access, use Everest to mount lakeFS paths:</p> <pre><code>% Mount a branch\neverest.mount('lakefs://my-repo/main/', 'local_data/');\n\n% Read files directly\ndata = load('local_data/results.mat');\n\n% Unmount when done\neverest.umount('local_data/');\n</code></pre> <p>Note: Mounting enables standard MATLAB file I/O functions to work with lakeFS data without explicit upload/download operations.</p>"},{"location":"integrations/matlab/#core-api-reference","title":"Core API Reference","text":""},{"location":"integrations/matlab/#lakefsm-repository-operations","title":"lakefs.m - Repository Operations","text":""},{"location":"integrations/matlab/#configuration","title":"Configuration","text":"<p><pre><code>lakefs.configure(environment)\n</code></pre> Description: Set active lakeFS environment Parameters:  - <code>environment</code> (string): <code>'local'</code> or <code>'cloud'</code></p>"},{"location":"integrations/matlab/#branch-operations","title":"Branch Operations","text":"<p><pre><code>branches = lakefs.list_branches(repo)\n</code></pre> Description: List all branches in repository Parameters:  - <code>repo</code> (string): Repository name Returns: Table with branch names and commit IDs</p> <p><pre><code>lakefs.create_branch(repo, branch_name, source_ref)\n</code></pre> Description: Create new branch from source reference Parameters:  - <code>repo</code> (string): Repository name - <code>branch_name</code> (string): New branch name - <code>source_ref</code> (string): Source branch or commit ID</p>"},{"location":"integrations/matlab/#commit-operations","title":"Commit Operations","text":"<p><pre><code>commits = lakefs.log(repo, ref, varargin)\n</code></pre> Description: Get commit history Parameters:  - <code>repo</code> (string): Repository name - <code>ref</code> (string): Branch or commit reference - Optional: <code>'amount', N</code> - Limit number of commits returned Returns: Table with commit IDs, messages, timestamps, metadata</p> <p><pre><code>lakefs.commit(repo, branch, message, varargin)\n</code></pre> Description: Commit changes to branch Parameters:  - <code>repo</code> (string): Repository name - <code>branch</code> (string): Branch name - <code>message</code> (string): Commit message - Optional: <code>'metadata', struct</code> - Attach metadata to commit</p>"},{"location":"integrations/matlab/#data-transfer","title":"Data Transfer","text":"<p><pre><code>lakefs.upload(repo, branch, lakefs_path, local_path)\n</code></pre> Description: Upload local file to lakeFS Parameters:  - <code>repo</code> (string): Repository name - <code>branch</code> (string): Branch name - <code>lakefs_path</code> (string): Destination path in lakeFS - <code>local_path</code> (string): Source file on local system</p> <p><pre><code>lakefs.download(repo, ref, lakefs_path, local_path)\n</code></pre> Description: Download file from lakeFS Parameters:  - <code>repo</code> (string): Repository name - <code>ref</code> (string): Branch, tag, or commit ID - <code>lakefs_path</code> (string): Source path in lakeFS - <code>local_path</code> (string): Destination on local system</p>"},{"location":"integrations/matlab/#comparison","title":"Comparison","text":"<p><pre><code>diff = lakefs.diff(repo, left_ref, right_ref)\n</code></pre> Description: Compare two references Parameters:  - <code>repo</code> (string): Repository name - <code>left_ref</code> (string): First reference (branch/tag/commit) - <code>right_ref</code> (string): Second reference Returns: Table showing changed, added, removed objects</p>"},{"location":"integrations/matlab/#tags","title":"Tags","text":"<p><pre><code>tags = lakefs.list_tags(repo)\n</code></pre> Description: List all tags Parameters:  - <code>repo</code> (string): Repository name Returns: Table with tag names and commit IDs</p>"},{"location":"integrations/matlab/#everestm-file-system-operations","title":"everest.m - File System Operations","text":""},{"location":"integrations/matlab/#mounting","title":"Mounting","text":"<p><pre><code>everest.mount(lakefs_uri, mount_dir, varargin)\n</code></pre> Description: Mount lakeFS path as local directory Parameters:  - <code>lakefs_uri</code> (string): Full lakeFS URI (e.g., <code>'lakefs://repo/branch/path/'</code>) - <code>mount_dir</code> (string): Local mount point - Optional: <code>'presign', true</code> - Use presigned URLs (default: true)</p> <p><pre><code>everest.umount(mount_dir)\n</code></pre> Description: Unmount directory Parameters:  - <code>mount_dir</code> (string): Local mount point to unmount</p> <p><pre><code>mounts = everest.list_mounts()\n</code></pre> Description: Show active mounts Returns: Cell array of currently mounted directories</p> <p><pre><code>everest.verify()\n</code></pre> Description: Check Everest installation and accessibility</p>"},{"location":"integrations/matlab/#example-workflows","title":"Example Workflows","text":""},{"location":"integrations/matlab/#experiment-tracking","title":"Experiment Tracking","text":"<pre><code>% Configure environment\nlakefs.configure('local');\n\n% Create experiment branch\nlakefs.create_branch('ml-project', 'experiment-3', 'main');\n\n% Train model (your code here)\nmodel = train_model(data);\nsave('outputs/model_v3.mat', 'model');\n\n% Upload results\nlakefs.upload('ml-project', 'experiment-3', ...\n    'models/model_v3.mat', 'outputs/model_v3.mat');\n\n% Commit with metrics\nmetadata = struct();\nmetadata.accuracy = num2str(model.accuracy);\nmetadata.loss = num2str(model.loss);\nmetadata.epochs = num2str(model.epochs);\n\nlakefs.commit('ml-project', 'experiment-3', ...\n    'Experiment 3: Increased learning rate', ...\n    'metadata', metadata);\n\n% View experiment history\ncommits = lakefs.log('ml-project', 'experiment-3', 'amount', 1);\nfprintf('Experiment accuracy: %s\\n', commits.metadata{1}.accuracy);\n</code></pre>"},{"location":"integrations/matlab/#data-version-comparison","title":"Data Version Comparison","text":"<pre><code>% Compare two data versions\ndiff = lakefs.diff('research-data', 'v1.0', 'v2.0');\n\n% Show what changed\nfprintf('Data changes between versions:\\n');\nfor i = 1:height(diff)\n    fprintf('  %s: %s\\n', diff.type{i}, diff.path{i});\nend\n\n% Download specific version\nlakefs.download('research-data', 'v1.0', ...\n    'datasets/training_data.mat', 'data/v1_training.mat');\n\nlakefs.download('research-data', 'v2.0', ...\n    'datasets/training_data.mat', 'data/v2_training.mat');\n\n% Compare in MATLAB\nv1 = load('data/v1_training.mat');\nv2 = load('data/v2_training.mat');\nfprintf('V1 samples: %d\\n', size(v1.data, 1));\nfprintf('V2 samples: %d\\n', size(v2.data, 1));\n</code></pre>"},{"location":"integrations/matlab/#reproducible-analysis","title":"Reproducible Analysis","text":"<pre><code>% Record exact data version used\ncommits = lakefs.log('sensor-data', 'main', 'amount', 1);\ndata_commit = commits.id{1};\n\n% Mount data\neverest.mount('lakefs://sensor-data/main/', 'analysis_data/');\n\n% Run analysis\ndata = load('analysis_data/sensor_readings.mat');\nresults = analyze_sensors(data);\n\n% Unmount\neverest.umount('analysis_data/');\n\n% Store results with data lineage\nlakefs.upload('sensor-data', 'analysis-results', ...\n    'results/analysis_output.mat', 'results/output.mat');\n\nmetadata = struct();\nmetadata.data_commit = data_commit;  % Track source data version\nmetadata.analysis_date = datestr(now);\nmetadata.mean_value = num2str(mean(results.values));\n\nlakefs.commit('sensor-data', 'analysis-results', ...\n    'Sensor analysis results', 'metadata', metadata);\n\n% Later: Reproduce exact analysis\nfprintf('Analysis used data from commit: %s\\n', data_commit);\n</code></pre>"},{"location":"integrations/matlab/#advanced-topics","title":"Advanced Topics","text":""},{"location":"integrations/matlab/#environment-switching","title":"Environment Switching","text":"<p>Switch between local and cloud environments in the same session:</p> <pre><code>% Work with local instance\nlakefs.configure('local');\nlocal_branches = lakefs.list_branches('dev-repo');\n\n% Switch to cloud\nlakefs.configure('cloud');\ncloud_branches = lakefs.list_branches('prod-repo');\n</code></pre>"},{"location":"integrations/matlab/#metadata-best-practices","title":"Metadata Best Practices","text":"<p>Structure metadata for maximum value:</p> <pre><code>metadata = struct();\n\n% Experiment details\nmetadata.experiment_id = 'exp-2024-001';\nmetadata.hypothesis = 'increased_batch_size';\n\n% Quantitative results\nmetadata.accuracy = sprintf('%.4f', results.accuracy);\nmetadata.training_time_sec = num2str(results.time);\n\n% Computational environment\nmetadata.matlab_version = version;\nmetadata.gpu_used = 'NVIDIA RTX 4090';\n\n% Data lineage\nmetadata.training_data_commit = training_commit_id;\nmetadata.validation_data_commit = val_commit_id;\n\nlakefs.commit(repo, branch, message, 'metadata', metadata);\n</code></pre>"},{"location":"integrations/matlab/#working-with-large-datasets","title":"Working with Large Datasets","text":"<p>For large datasets, prefer mounting over upload/download:</p> <pre><code>% Mount for direct access (no copying)\neverest.mount('lakefs://large-data/main/datasets/', 'data/');\n\n% Process data in chunks\ndatastore = imageDatastore('data/images', ...\n    IncludeSubfolders=true, ...\n    LabelSource=\"foldernames\");\n\n% Process without loading all into memory\nwhile hasdata(datastore)\n    img = read(datastore);\n    process_image(img);\nend\n\n% Unmount when done\neverest.umount('data/');\n</code></pre>"},{"location":"integrations/matlab/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integrations/matlab/#connection-issues","title":"Connection Issues","text":"<p>Error: Cannot connect to lakeFS</p> <p>Verify configuration: <pre><code>% Check lakectl is configured\n!lakectl repo list\n\n% Test lakefs.m configuration\nlakefs.configure('local');  % or 'cloud'\n</code></pre></p>"},{"location":"integrations/matlab/#mount-issues","title":"Mount Issues","text":"<p>Error: Mount failed</p> <p>Verify Everest installation: <pre><code>everest.verify();\n\n% Check if Everest is in PATH\n!which everest\n</code></pre></p> <p>Mount not showing files</p> <p>Wait for mount to complete: <pre><code>everest.mount('lakefs://repo/branch/', 'data/');\npause(2);  % Give mount time to initialize\ndir('data/')  % Should now show files\n</code></pre></p>"},{"location":"integrations/matlab/#path-issues","title":"Path Issues","text":"<p>Error: File not found in lakeFS</p> <p>Verify path format: <pre><code>% Correct: Relative path from branch root\nlakefs.upload('repo', 'branch', 'data/file.mat', 'local.mat');\n\n% Incorrect: Leading slash\nlakefs.upload('repo', 'branch', '/data/file.mat', 'local.mat');\n</code></pre></p>"},{"location":"integrations/matlab/#additional-resources","title":"Additional Resources","text":"<ul> <li>lakeFS Documentation</li> <li>lakectl CLI Reference</li> <li>Everest Mount Reference</li> <li>lakeFS Samples Repository</li> <li>lakeFS Community Slack</li> </ul>"},{"location":"integrations/matlab/#version-compatibility","title":"Version Compatibility","text":"Component Minimum Version Recommended MATLAB R2020a R2023a+ lakeFS 0.100.0 Latest lakectl 0.100.0 Latest Everest 0.1.0 Latest <p>Note: MATLAB R2023a introduced <code>name=value</code> syntax for function arguments. Earlier versions use <code>'name', value</code> pairs. The helper files are compatible with both syntaxes.</p>"},{"location":"integrations/mlflow/","title":"Using MLflow with lakeFS","text":"<p>MLflow is a comprehensive tool designed to manage the machine learning lifecycle, assisting practitioners and teams in handling the complexities of ML processes. It focuses on the full lifecycle of machine learning projects, ensuring that each phase is manageable, traceable, and reproducible.</p> <p>MLflow comprises multiple core components, and lakeFS seamlessly integrates with the MLflow Tracking component. MLflow tracking enables experiment tracking that accounts for both inputs and outputs, allowing for visualization and comparison of experiment results.</p>"},{"location":"integrations/mlflow/#benefits-of-integrating-mlflow-with-lakefs","title":"Benefits of integrating MLflow with lakeFS","text":"<p>Integrating MLflow with lakeFS offers several advantages that enhance the machine learning workflow:</p> <ol> <li>Experiment Reproducibility: By leveraging MLflow's input logging capabilities alongside lakeFS's data versioning, you can precisely track the specific dataset version used in each experiment run. This ensures that experiments remain reproducible over time, even as datasets evolve.</li> <li>Parallel Experiments with Zero Data Copy: lakeFS enables efficient branching without duplicating data. This allows for multiple experiments to be conducted in parallel, with each branch providing an isolated environment for dataset modifications. Changes in one branch do not affect others, promoting safe collaboration among team members. Once an experiment is complete, the branch can be seamlessly merged back into the main dataset, incorporating new insights.</li> </ol>"},{"location":"integrations/mlflow/#how-to-use-mlflow-with-lakefs","title":"How to use MLflow with lakeFS","text":"<p>To harness the combined capabilities of MLflow and lakeFS for safe experimentation and accurate result reproduction, consider the workflow below and review the practical examples provided on the next section.  </p>"},{"location":"integrations/mlflow/#recommended-workflow","title":"Recommended workflow","text":"<ol> <li>Create a branch for each experiment: Start each experiment by creating a dedicated lakeFS branch for it. This approach  allows you to safely make changes to your input dataset without duplicating it. You will later load data from this branch  to your MLflow experiment runs. </li> <li>Read datasets from the experiment branch: Conduct your experiments by reading data directly from the dedicated  branch. We recommend to read the dataset from the head commit of the branch to ensure precise version tracking.</li> <li>Create an MLflow Dataset pointing to lakeFS: Use MLflow's Dataset ensuring that the dataset source points to lakeFS. </li> <li>Log your input: Use MLflow's log_input  function to log the versioned dataset stored in lakeFS.    </li> <li>Commit dataset changes: Machine learning development is inherently iterative. When you make changes to your input dataset, commit them to the experiment branch in lakeFS with a meaningful commit message. During an experiment run, load the dataset version corresponding to the branch's head commit and track this reference to facilitate future result reproduction.</li> <li>Merge experiment results: After concluding your experimentation, merge the branch used for the selected experiment  run back into the main branch.</li> </ol> <p>Branch per experiment Vs. Branch per experiment run</p> <p>While it's possible to create a lakeFS branch for each experiment run, given that lakeFS branches are both quick and  cost-effective to create, it's often more efficient to create a branch per experiment. By reading directly from the head commit of the experiment branch, you can distinguish between dataset versions without creating excessive branches. This practice maintains branch hygiene within lakeFS.</p>"},{"location":"integrations/mlflow/#example-using-pandas","title":"Example: Using Pandas","text":"<pre><code>import lakefs \nimport mlflow\nimport pandas as pd\n\nrepo = lakefs.Repository(\"my-repo\")\nrepo_id = repo.id\n\nexp_branch = repo.branch(\"experiment-1\").create(source_reference=\"main\", exist_ok=True)\nbranch_id = exp_branch.id\nhead_commit_id = exp_branch.head.id\n\ntable_path = \"famous_people.csv\"\n\ndataset_source_url = f\"s3://{repo_id}/{head_commit_id}/{table_path}\"\n\n# Use Pandas to read from lakeFS, at its most updated version to which the head commit id is pointing\nraw_data = pd.read_csv(dataset_source_url, delimiter=\";\", storage_options={\n        \"key\": \"AKIAIOSFOLKFSSAMPLES\",\n        \"secret\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"client_kwargs\": {\"endpoint_url\": \"http://localhost:8000\"}\n    })\n\n# Create an instance of a PandasDataset\ndataset = mlflow.data.from_pandas(\n    raw_data, source=dataset_source_url, name=\"famous_people\"\n)\n\n# View some of the recorded Dataset information\nprint(f\"Dataset name: {dataset.name}\")\nprint(f\"Dataset source URI: {dataset.source.uri}\")\n\n# Use mlflow input logging to track the dataset versioned by lakeFS\nwith mlflow.start_run() as run:\n    mlflow.log_input(dataset, context=\"training\")\n    mlflow.set_tag(\"lakefs_repo\", repo_id)\n    mlflow.set_tag(\"lakefs_branch\", branch_id) \n    mlflow.set_tag(\"lakefs_commit\", head_commit_id) \n\n# Inspect run's dataset\nlogged_run = mlflow.get_run(run.info.run_id) # \n\n# Retrieve the Dataset object\nlogged_dataset = logged_run.inputs.dataset_inputs[0].dataset\n\n# View some of the recorded Dataset information\nprint(f\"Logged dataset name: {logged_dataset.name}\")\nprint(f\"Logged dataset source URI: {logged_dataset.source}\")\n</code></pre> <p>Output <pre><code>Dataset name: famous_people\nDataset source URI: s3://my-repo/3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3/fp.csv\nLogged dataset name: famous_people\nLogged dataset source URI: {\"uri\": \"s3://my-repo/3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3/fp.csv\"}\n</code></pre></p>"},{"location":"integrations/mlflow/#example-using-spark","title":"Example: Using Spark","text":"<p>The example below configures Spark to access lakeFS' S3-compatible API and load a Delta Lake tables to the experiment. </p> <pre><code>import lakefs \nimport mlflow\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"lakeFS / Mlflow\") \\\n    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\ \n    .config(\"spark.hadoop.fs.s3a.endpoint\", 'http://localhost:8000') \\ \n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\ \n    .config(\"spark.hadoop.fs.s3a.access.key\", 'AKIAlakefs12345EXAMPLE') \\ \n    .config(\"spark.hadoop.fs.s3a.secret.key\", 'abc/lakefs/1234567bPxRfiCYEXAMPLEKEY') \\ \n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") \\ \n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ \n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ \n    .getOrCreate()\n\nrepo = lakefs.Repository(\"my-repo\")\nrepo_id = repo.id\n\nexp_branch = repo.branch(\"experiment-1\").create(source_reference=\"main\", exist_ok=True)\nbranch_id = exp_branch.id\nhead_commit_id = exp_branch.head.id\n\ntable_path = \"gold/train_v2/\"\n\ndataset_source_url = f\"s3://{repo_id}/{head_commit_id}/{table_path}\"\n\n# Load delta lake table from lakeFS, at its most updated version to which the head commit id is pointing\ndataset = mlflow.data.load_delta(path=dataset_source_url, name=\"boat-images\")\n\n# View some of the recorded Dataset information\nprint(f\"Dataset name: {dataset.name}\")\nprint(f\"Dataset source URI: {dataset.source.path}\")\n\n# Use mlflow input logging to track the dataset versioned by lakeFS\nwith mlflow.start_run() as run:\n    mlflow.log_input(dataset, context=\"training\")\n    mlflow.set_tag(\"lakefs_repo\", repo_id)\n    mlflow.set_tag(\"lakefs_branch\", branch_id) # Log the branch id, to have a friendly lakeFS reference to search the input dataset in\n    mlflow.set_tag(\"lakefs_commit\", head_commit_id)\n\n# Inspect run's dataset\nlogged_run = mlflow.get_run(run.info.run_id) # \n\n# Retrieve the Dataset object\nlogged_dataset = logged_run.inputs.dataset_inputs[0].dataset\n\n# View some of the recorded Dataset information\nprint(f\"Logged dataset name: {logged_dataset.name}\")\nprint(f\"Logged dataset source URI: {logged_dataset.source}\")\n</code></pre> <p>Output:</p> <pre><code>Dataset name: boat-images\nDataset source URI: s3://my-repo/3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3/gold/train_v2/\nLogged dataset name: boat-images\nLogged dataset source URI: {\"path\": \"s3://my-repo/3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3/gold/train_v2/\"}\n</code></pre>"},{"location":"integrations/mlflow/#reproduce-experiment-results","title":"Reproduce experiment results","text":"<p>To reproduce the results of a specific experiment run in MLflow, it's essential to retrieve the exact dataset and associated metadata used during that run. While the MLflow Tracking UI provides a general overview, detailed dataset information  and its source are best accessed programmatically.</p> <ol> <li> <p>Obtain the Run ID: Navigate to the MLflow UI and copy the Run ID of the experiment you're interested in.</p> <p></p> </li> <li> <p>Extract Dataset Information Using MLflow's Python SDK:</p> <pre><code>import mlflow\n\n# Inspect run's dataset and tags\nrun_id = \"c0f8fbb1b63748abaa0a6479115e272c\"\nrun = mlflow.get_run(run_id) \n\n# Retrieve the Dataset object\nlogged_dataset = run.inputs.dataset_inputs[0].dataset\n\n# View some of the recorded Dataset information\nprint(f\"Run ID: {run_id} Dataset name: {logged_dataset.name}\")\nprint(f\"Run ID: {run_id} Dataset source URI: {logged_dataset.source}\")\n\n# Retrieve run's tags \nlogged_tags = run.data.tags\nprint(f\"Run ID: {run_id} tags: {logged_tags}\")\n</code></pre> <p>Output</p> <pre><code>Run ID: c0f8fbb1b63748abaa0a6479115e272c Dataset name: boat-images\nRun ID: c0f8fbb1b63748abaa0a6479115e272c Dataset source URI: {\"path\": \"s3://my-repo/3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3/gold/train_v2/\"}\nRun ID: c0f8fbb1b63748abaa0a6479115e272c tags: {'lakefs_branch': 'experiment-1', 'lakefs_repo': 'my-repo', 'lakefs_commit': '3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3'}\n</code></pre> </li> </ol> <p>Notes</p> <ul> <li>The Dataset Source URI provides the location of the dataset at the exact version used in the run. </li> <li>Run tags, such as 'lakefs_branch' and 'lakefs_repo', offer additional context about the dataset's origin within lakeFS.</li> </ul>"},{"location":"integrations/mlflow/#compare-runs-input","title":"Compare runs input","text":"<p>To determine whether two distinct MLflow runs utilized the same input dataset, you can compare specific attributes of  their logged Dataset objects. The source attribute, which contains the versioned dataset's URI, is a common choice for  this comparison. Here's an example:</p> <pre><code>import mlflow\n\nfirst_run_id = \"4c0464d665944dc5bb90587d455948b8\"\nfirst_run = mlflow.get_run(first_run_id)\n\n# Retrieve the Dataset object\nfirst_dataset = first_run.inputs.dataset_inputs[0].dataset\nfirst_dataset_src = first_dataset.source\n\nsec_run_id = \"12b91e073a8b40df97ea8d570534de31\"\nsec_run = mlflow.get_run(sec_run_id)\n\n# Retrieve the Dataset object\nsec_dataset = sec_run.inputs.dataset_inputs[0].dataset\nsec_dataset_src = sec_dataset.source\n\nassert first_dataset_src == sec_dataset_src, \"Dataset sources are not equal.\"\n\nprint(f\"First dataset src: {first_dataset_src}\")\nprint(f\"Second dataset src: {sec_dataset_src}\")\n</code></pre> <p>Output</p> <pre><code>First dataset src: {\"uri\": \"s3://mlflow-tracking/f16682c0186a81adf72edaad23c3f59ed2ff3afddad4fef987b4919f5e82003a/gold/train_v2/\"}\nSecond dataset src: {\"uri\": \"s3://mlflow-tracking/3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3/gold/train_v2/\"}\n</code></pre> <p>In this example, the source attribute of each Dataset object is compared to determine if the input datasets are identical.  If they differ, you can further inspect them. With the dataset source URI in hand, you can use lakeFS to gain more insights about the changes made to your dataset:</p> <ul> <li>Inspect the lakeFS Commit ID: By examining the commit ID within the URI, you can retrieve detailed information about  the commit, including the author and the purpose of the changes.</li> <li>Use lakeFS Diff: lakeFS offers a diff function that allows you to compare different versions of your data. </li> </ul> <p>By leveraging these tools, you can effectively track and understand the evolution of your datasets across different MLflow runs.</p>"},{"location":"integrations/presto_trino/","title":"Using lakeFS with Trino / Presto","text":"<p>Trino and Presto are distributed SQL query engines designed to query large data sets distributed over one or more heterogeneous data sources.</p>"},{"location":"integrations/presto_trino/#iceberg-rest-catalog","title":"Iceberg REST Catalog","text":"<p>lakeFS Iceberg REST Catalog allow you to use lakeFS as a spec-compliant Apache Iceberg REST catalog,  allowing Trino/Presto to manage and access tables using a standard REST API. </p> <p></p> <p>This is the recommended way to use lakeFS with Trino/Presto, as it allows lakeFS to stay completely outside the data path: data itself is read and written by Trino/Presto executors, directly to the underlying object store. Metadata is managed by Iceberg at the table level, while lakeFS keeps track of new snapshots to provide versioning and isolation.</p> <p>Read more about using the Iceberg REST Catalog.</p>"},{"location":"integrations/presto_trino/#configuration","title":"Configuration","text":"<p>To use the Iceberg REST Catalog, you need to configure Trino/Presto to use the Iceberg REST catalog endpoint:</p> <p>Tip</p> <p>To learn more about the Iceberg REST Catalog, see the Iceberg REST Catalog documentation.</p> <pre><code># example: /etc/trino/catalog/lakefs.properties\nconnector.name=iceberg\n\n# REST Catalog connection\niceberg.catalog.type=rest\niceberg.rest-catalog.uri=https://lakefs.example.com/iceberg/api\niceberg.rest-catalog.nested-namespace-enabled=true\n\n# REST Catalog authentication\niceberg.rest-catalog.security=OAUTH2\niceberg.rest-catalog.oauth2.credential=${ENV:LAKEFS_CREDENTIALS}\niceberg.rest-catalog.oauth2.server-uri=https://lakefs.example.com/iceberg/api/v1/oauth/tokens\n\n# Object storage access to underlying tables (modify this to match your storage provider)\nfs.hadoop.enabled=false\nfs.native-s3.enabled=true\ns3.region=us-east-1\ns3.aws-access-key=${ENV:AWS_ACCESS_KEY_ID}\ns3.aws-secret-key=${ENV:AWS_SECRET_ACCESS_KEY}\n</code></pre>"},{"location":"integrations/presto_trino/#usage","title":"Usage","text":"<p>Once configured, you can use the Iceberg REST Catalog to query and update Iceberg tables.</p> <pre><code>USE \"repo.main.inventory\";\nSHOW TABLES;\nSELECT * FROM books LIMIT 100;\n</code></pre> <pre><code>USE \"repo.new_branch.inventory\";\nSHOW TABLES;\nSELECT * FROM books LIMIT 100;\n</code></pre>"},{"location":"integrations/presto_trino/#using-prestotrino-with-the-s3-gateway","title":"Using Presto/Trino with the S3 Gateway","text":"<p>Using the S3 Gateway allows reading and writing data to lakeFS from Presto/Trino, in any format supported by Presto/Trino (i.e. not just Iceberg tables). </p> <p>While flexible, this approach requires lakeFS to be involved in the data path, which can be less efficient than the Iceberg REST Catalog approach, since lakeFS has to proxy all data operations through the lakeFS server. This is particularly true for large data sets where network bandwidth might incur some overhead.</p>"},{"location":"integrations/presto_trino/#configuration_1","title":"Configuration","text":"<p>Credentials</p> <p>In the following examples, we set AWS credentials at runtime for clarity. In production, these properties should be set using one of Hadoop's standard ways of Authenticating with S3. </p> <p>Create <code>/etc/catalog/hive.properties</code> with the following contents to mount the <code>hive-hadoop2</code> connector as the Hive catalog, replacing <code>example.net:9083</code> with the correct host and port for your Hive Metastore Thrift service:</p> <pre><code>connector.name=hive-hadoop2\nhive.metastore.uri=thrift://example.net:9083\n</code></pre> <p>Add the lakeFS configurations to <code>/etc/catalog/hive.properties</code> in the corresponding S3 configuration properties:</p> <pre><code>hive.s3.aws-access-key=AKIAIOSFODNN7EXAMPLE\nhive.s3.aws-secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nhive.s3.endpoint=https://lakefs.example.com\nhive.s3.path-style-access=true\n</code></pre>"},{"location":"integrations/presto_trino/#configure-hive","title":"Configure Hive","text":"<p>Presto/Trino uses Hive Metastore Service (HMS) or a compatible implementation of the Hive Metastore such as AWS Glue Data Catalog to write data to S3. In case you are using Hive Metastore, you will need to configure Hive as well.</p> <p>In file <code>hive-site.xml</code> add to the configuration:</p> <pre><code>&lt;configuration&gt;\n    ...\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt;&lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;\n        &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;\n        &lt;value&gt;https://lakefs.example.com&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"integrations/python-boto/","title":"Using Boto3 with lakeFS S3 Gateway","text":"<p>lakeFS exposes an S3-compatible API through its S3 Gateway, allowing you to use Boto3 (AWS SDK for Python) directly with lakeFS. This integration is perfect for existing S3 workflows and applications.</p> <p>Info</p> <p>To use Boto with lakeFS alongside S3, check out Boto S3 Router. It will route requests to either S3 or lakeFS according to the provided bucket name.</p>"},{"location":"integrations/python-boto/#when-to-use","title":"When to Use","text":"<p>Use Boto with lakeFS when you:</p> <ul> <li>Have existing S3 workflows you want to use with lakeFS</li> <li>Need S3-compatible operations (put, get, list, delete)</li> <li>Work with legacy S3 applications</li> <li>Want to migrate from S3 without code changes</li> </ul> <p>For versioning-focused workflows, use the High-Level SDK or lakefs-spec.</p>"},{"location":"integrations/python-boto/#installation","title":"Installation","text":"<p>Install Boto3 using pip:</p> <pre><code>pip install boto3\n</code></pre> <p>Or upgrade to the latest version:</p> <pre><code>pip install --upgrade boto3\n</code></pre>"},{"location":"integrations/python-boto/#basic-setup","title":"Basic Setup","text":""},{"location":"integrations/python-boto/#initializing-boto3-client","title":"Initializing Boto3 Client","text":"<pre><code>import boto3\n\n# Create S3 client pointing to lakeFS\ns3 = boto3.client(\n    's3',\n    endpoint_url='https://example.lakefs.io',\n    aws_access_key_id='your-access-key',\n    aws_secret_access_key='your-secret-key',\n    region_name='us-east-1'\n)\n\nprint(\"Client initialized\")\n</code></pre>"},{"location":"integrations/python-boto/#checksum-configuration","title":"Checksum Configuration","text":"<p>In newer versions of Boto3 when using HTTPS, you might encounter an <code>AccessDenied</code> error with lakeFS logs showing <code>encoding/hex: invalid byte: U+0053 'S'</code>. This is due to checksum configuration.</p>"},{"location":"integrations/python-boto/#configuring-checksum-settings","title":"Configuring Checksum Settings","text":"<pre><code>import boto3\nfrom botocore.config import Config\n\n# Configure checksum settings\nconfig = Config(\n    request_checksum_calculation='when_required',\n    response_checksum_validation='when_required'\n)\n\ns3 = boto3.client(\n    's3',\n    endpoint_url='https://lakefs.example.io',\n    aws_access_key_id='your-access-key',\n    aws_secret_access_key='your-secret-key',\n    config=config\n)\n\nprint(\"Client with checksum configuration initialized\")\n</code></pre>"},{"location":"integrations/python-boto/#basic-operations","title":"Basic Operations","text":""},{"location":"integrations/python-boto/#uploading-objects","title":"Uploading Objects","text":"<pre><code>import boto3\n\ns3 = boto3.client(\n    's3',\n    endpoint_url='https://example.lakefs.io',\n    aws_access_key_id='your-access-key',\n    aws_secret_access_key='your-secret-key'\n)\n\n# Upload from bytes\ndata = b\"Hello, lakeFS!\"\ns3.put_object(\n    Bucket='my-repo',\n    Key='main/data/hello.txt',\n    Body=data\n)\n\n# Upload from file\nwith open('local_file.csv', 'rb') as f:\n    s3.put_object(\n        Bucket='my-repo',\n        Key='main/data/imported.csv',\n        Body=f\n    )\n\n# Upload with metadata\ns3.put_object(\n    Bucket='my-repo',\n    Key='main/data/data.csv',\n    Body=b'id,name\\n1,Alice\\n2,Bob',\n    Metadata={\n        'owner': 'data-team',\n        'version': '1.0'\n    }\n)\n\nprint(\"Upload complete\")\n</code></pre>"},{"location":"integrations/python-boto/#downloading-objects","title":"Downloading Objects","text":"<pre><code>import boto3\n\ns3 = boto3.client(\n    's3',\n    endpoint_url='https://example.lakefs.io',\n    aws_access_key_id='your-access-key',\n    aws_secret_access_key='your-secret-key'\n)\n\n# Download entire object\nresponse = s3.get_object(\n    Bucket='my-repo',\n    Key='main/data/data.csv'\n)\ndata = response['Body'].read()\nprint(f\"Downloaded {len(data)} bytes\")\n\n# Download to file\ns3.download_file(\n    Bucket='my-repo',\n    Key='main/data/large_file.parquet',\n    Filename='local_file.parquet'\n)\n\n# Stream download (for large files)\nresponse = s3.get_object(Bucket='my-repo', Key='main/data/large.csv')\nfor chunk in iter(lambda: response['Body'].read(1024), b''):\n    process_chunk(chunk)\n</code></pre>"},{"location":"integrations/python-boto/#listing-objects","title":"Listing Objects","text":"<pre><code>import boto3\n\ns3 = boto3.client(\n    's3',\n    endpoint_url='https://example.lakefs.io',\n    aws_access_key_id='your-access-key',\n    aws_secret_access_key='your-secret-key'\n)\n\n# List objects in branch\nresponse = s3.list_objects_v2(\n    Bucket='my-repo',\n    Prefix='main/data/'\n)\n\nfor obj in response.get('Contents', []):\n    print(f\"{obj['Key']} ({obj['Size']} bytes)\")\n\n# List objects at commit\nresponse = s3.list_objects_v2(\n    Bucket='my-repo',\n    Prefix='abc123def456/data/'\n)\n\nfor obj in response.get('Contents', []):\n    print(f\"{obj['Key']}\")\n</code></pre>"},{"location":"integrations/python-boto/#getting-object-metadata","title":"Getting Object Metadata","text":"<pre><code>import boto3\n\ns3 = boto3.client(\n    's3',\n    endpoint_url='https://example.lakefs.io',\n    aws_access_key_id='your-access-key',\n    aws_secret_access_key='your-secret-key'\n)\n\n# Head object\nresponse = s3.head_object(\n    Bucket='my-repo',\n    Key='main/data/file.csv'\n)\n\nprint(f\"Content Type: {response.get('ContentType')}\")\nprint(f\"Content Length: {response.get('ContentLength')}\")\nprint(f\"Last Modified: {response.get('LastModified')}\")\nprint(f\"Metadata: {response.get('Metadata')}\")\n</code></pre>"},{"location":"integrations/python-boto/#deleting-objects","title":"Deleting Objects","text":"<pre><code>import boto3\n\ns3 = boto3.client(\n    's3',\n    endpoint_url='https://example.lakefs.io',\n    aws_access_key_id='your-access-key',\n    aws_secret_access_key='your-secret-key'\n)\n\n# Delete single object\ns3.delete_object(\n    Bucket='my-repo',\n    Key='main/data/temp.txt'\n)\n\n# Delete multiple objects\ns3.delete_objects(\n    Bucket='my-repo',\n    Delete={\n        'Objects': [\n            {'Key': 'main/data/file1.txt'},\n            {'Key': 'main/data/file2.txt'},\n            {'Key': 'main/data/file3.txt'}\n        ]\n    }\n)\n\nprint(\"Delete complete\")\n</code></pre>"},{"location":"integrations/python-boto/#real-world-workflows","title":"Real-World Workflows","text":""},{"location":"integrations/python-boto/#etl-with-s3-like-operations","title":"ETL with S3-Like Operations","text":"<pre><code>import boto3\nimport csv\nimport io\n\ndef etl_pipeline():\n    s3 = boto3.client(\n        's3',\n        endpoint_url='https://example.lakefs.io',\n        aws_access_key_id='your-access-key',\n        aws_secret_access_key='your-secret-key'\n    )\n\n    # Extract: Read from source\n    response = s3.get_object(Bucket='my-repo', Key='main/raw/input.csv')\n    input_data = response['Body'].read().decode()\n\n    # Transform: Process data\n    reader = csv.DictReader(io.StringIO(input_data))\n    rows = list(reader)\n\n    # Clean: Remove duplicates\n    unique_rows = {row['id']: row for row in rows}.values()\n\n    # Load: Write processed data\n    output = io.StringIO()\n    writer = csv.DictWriter(output, fieldnames=['id', 'name', 'value'])\n    writer.writeheader()\n    writer.writerows(unique_rows)\n\n    s3.put_object(\n        Bucket='my-repo',\n        Key='main/processed/output.csv',\n        Body=output.getvalue()\n    )\n\n    print(f\"ETL complete: {len(unique_rows)} unique records\")\n\netl_pipeline()\n</code></pre>"},{"location":"integrations/python-boto/#backup-and-sync","title":"Backup and Sync","text":"<pre><code>import boto3\nimport os\nfrom pathlib import Path\n\ndef backup_to_lakeFS(local_dir, repo, branch, prefix):\n    \"\"\"Backup local directory to lakeFS\"\"\"\n    s3 = boto3.client(\n        's3',\n        endpoint_url='https://example.lakefs.io',\n        aws_access_key_id='your-access-key',\n        aws_secret_access_key='your-secret-key'\n    )\n\n    count = 0\n    for local_file in Path(local_dir).rglob('*'):\n        if local_file.is_file():\n            # Calculate remote path\n            rel_path = local_file.relative_to(local_dir)\n            remote_path = f\"{branch}/{prefix}/{rel_path}\".replace(\"\\\\\", \"/\")\n\n            # Upload\n            with open(local_file, 'rb') as f:\n                s3.put_object(\n                    Bucket=repo,\n                    Key=remote_path,\n                    Body=f\n                )\n            count += 1\n\n            if count % 100 == 0:\n                print(f\"Backed up {count} files...\")\n\n    print(f\"Backup complete: {count} files uploaded\")\n\n# Usage:\n# backup_to_lakeFS(\"/path/to/local/data\", \"my-repo\", \"main\", \"backups/2024-01\")\n</code></pre>"},{"location":"integrations/python-boto/#copy-from-s3-to-lakefs","title":"Copy from S3 to lakeFS","text":"<pre><code>import boto3\n\ndef migrate_s3_to_lakefs(s3_bucket, prefix, repo, branch):\n    \"\"\"Migrate data from S3 to lakeFS\"\"\"\n    # Connect to S3\n    s3_source = boto3.client(\n        's3',\n        region_name='us-east-1'\n    )\n\n    # Connect to lakeFS\n    s3_dest = boto3.client(\n        's3',\n        endpoint_url='https://example.lakefs.io',\n        aws_access_key_id='your-access-key',\n        aws_secret_access_key='your-secret-key'\n    )\n\n    # List objects\n    paginator = s3_source.get_paginator('list_objects_v2')\n    pages = paginator.paginate(Bucket=s3_bucket, Prefix=prefix)\n\n    count = 0\n    for page in pages:\n        for obj in page.get('Contents', []):\n            # Download from S3\n            response = s3_source.get_object(\n                Bucket=s3_bucket,\n                Key=obj['Key']\n            )\n            data = response['Body'].read()\n\n            # Upload to lakeFS\n            s3_dest.put_object(\n                Bucket=repo,\n                Key=f\"{branch}/{obj['Key']}\",\n                Body=data\n            )\n\n            count += 1\n            if count % 100 == 0:\n                print(f\"Migrated {count} objects...\")\n\n    print(f\"Migration complete: {count} objects\")\n\n# Usage:\n# migrate_s3_to_lakefs(\"my-s3-bucket\", \"data/\", \"my-repo\", \"main\")\n</code></pre>"},{"location":"integrations/python-boto/#version-specific-access","title":"Version-Specific Access","text":"<pre><code>import boto3\n\ndef read_from_commit(repo, commit_id, key):\n    \"\"\"Read object from specific commit\"\"\"\n    s3 = boto3.client(\n        's3',\n        endpoint_url='https://example.lakefs.io',\n        aws_access_key_id='your-access-key',\n        aws_secret_access_key='your-secret-key'\n    )\n\n    # Use commit ID as prefix\n    response = s3.get_object(\n        Bucket=repo,\n        Key=f\"{commit_id}/{key}\"\n    )\n\n    return response['Body'].read()\n\n# Usage:\n# data = read_from_commit(\"my-repo\", \"abc123def456\", \"data/file.csv\")\n</code></pre>"},{"location":"integrations/python-boto/#error-handling","title":"Error Handling","text":"<pre><code>import boto3\nfrom botocore.exceptions import ClientError\n\ns3 = boto3.client(\n    's3',\n    endpoint_url='https://example.lakefs.io',\n    aws_access_key_id='your-access-key',\n    aws_secret_access_key='your-secret-key'\n)\n\ntry:\n    s3.put_object(\n        Bucket='my-repo',\n        Key='main/data/file.txt',\n        Body=b'data'\n    )\nexcept ClientError as e:\n    error_code = e.response['Error']['Code']\n    if error_code == 'AccessDenied':\n        print(\"Access denied - check credentials or permissions\")\n    elif error_code == 'NoSuchBucket':\n        print(\"Bucket not found - check repository name\")\n    else:\n        print(f\"Error: {error_code}\")\n</code></pre>"},{"location":"integrations/python-boto/#further-resources","title":"Further Resources","text":"<ul> <li>lakeFS S3 Gateway - S3 Gateway API documentation</li> <li>Boto3 Documentation - Official Boto3 reference</li> </ul>"},{"location":"integrations/python-data-operations/","title":"Working with Objects &amp; Data Operations","text":"<p>This guide covers object operations in lakeFS, including uploading, downloading, batch operations, and metadata management.</p>"},{"location":"integrations/python-data-operations/#basic-object-operations","title":"Basic Object Operations","text":""},{"location":"integrations/python-data-operations/#uploading-objects","title":"Uploading Objects","text":"<p>Upload data to lakeFS:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\n# Upload text data\nbranch.object(\"data/simple.txt\").upload(\n    data=b\"Hello, lakeFS!\"\n)\n\n# Upload with content type\nbranch.object(\"data/data.json\").upload(\n    data=b'{\"key\": \"value\"}',\n    content_type=\"application/json\"\n)\n\n# Upload larger data\ncsv_data = b\"id,name,value\\n1,Alice,100\\n2,Bob,200\\n3,Carol,300\"\nbranch.object(\"data/records.csv\").upload(data=csv_data)\n\nprint(\"Objects uploaded successfully\")\n</code></pre>"},{"location":"integrations/python-data-operations/#downloading-objects","title":"Downloading Objects","text":"<p>Read object data from lakeFS:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\n# Read as text\nwith branch.object(\"data/simple.txt\").reader(mode='r') as f:\n    content = f.read()\n    print(f\"Content: {content}\")\n\n# Read as binary\nwith branch.object(\"data/data.json\").reader(mode='rb') as f:\n    binary_content = f.read()\n    print(f\"Binary size: {len(binary_content)} bytes\")\n\n# Read CSV and process\nimport csv\nimport io\n\nwith branch.object(\"data/records.csv\").reader(mode='r') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        print(f\"  {row['name']}: {row['value']}\")\n</code></pre>"},{"location":"integrations/python-data-operations/#object-information-metadata","title":"Object Information &amp; Metadata","text":"<p>Get object details and metadata:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\nobj = branch.object(\"data/records.csv\")\n\n# Check if object exists\ntry:\n    if obj.exists():\n        print(\"Object exists\")\nexcept:\n    print(\"Object not found\")\n\n# Get object statistics\nstat = obj.stat()\nprint(f\"Size: {stat.size_bytes} bytes\")\nprint(f\"Modified: {stat.mtime}\")\nprint(f\"Checksum: {stat.checksum}\")\nprint(f\"Content Type: {stat.content_type}\")\nprint(f\"Path: {stat.path}\")\n</code></pre>"},{"location":"integrations/python-data-operations/#deleting-objects","title":"Deleting Objects","text":"<p>Remove objects from lakeFS:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\n# Delete a single object\nobj = branch.object(\"data/temp_file.txt\")\nobj.delete()\nprint(\"Object deleted\")\n\n# Handle non-existent objects gracefully\ntry:\n    obj.delete()\nexcept Exception as e:\n    print(f\"Delete failed: {e}\")\n</code></pre>"},{"location":"integrations/python-data-operations/#batch-operations","title":"Batch Operations","text":""},{"location":"integrations/python-data-operations/#batch-delete-multiple-objects","title":"Batch Delete Multiple Objects","text":"<p>Delete many objects efficiently:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\n# Delete multiple objects by path\npaths_to_delete = [\n    \"data/file1.csv\",\n    \"data/file2.csv\",\n    \"data/file3.csv\",\n    \"logs/temp.log\"\n]\n\ntry:\n    branch.delete_objects(paths_to_delete)\n    print(f\"Deleted {len(paths_to_delete)} objects\")\nexcept Exception as e:\n    print(f\"Batch delete failed: {e}\")\n</code></pre>"},{"location":"integrations/python-data-operations/#listing-and-filtering-objects","title":"Listing and Filtering Objects","text":""},{"location":"integrations/python-data-operations/#list-objects-by-prefix","title":"List Objects by Prefix","text":"<p>List all objects under a path:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\n# List all objects in data/ folder\nprint(\"Objects in data/:\")\nfor obj in branch.objects(prefix=\"data/\"):\n    print(f\"  {obj.path} ({obj.size_bytes} bytes)\")\n\n# Count total objects\ntotal_objects = 0\nfor _ in branch.objects(prefix=\"data/\"):\n    total_objects += 1\nprint(f\"Total objects: {total_objects}\")\n</code></pre>"},{"location":"integrations/python-data-operations/#list-with-delimiter-folder-view","title":"List with Delimiter (Folder View)","text":"<p>Use delimiter to see folder structure:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\n# List with folder delimiter\nprint(\"Folder structure (with /):\")\nfor item in branch.objects(prefix=\"\", delimiter=\"/\"):\n    if hasattr(item, 'path'):\n        # It's a file\n        print(f\"  FILE: {item.path}\")\n    else:\n        # It's a folder\n        print(f\"  FOLDER: {item.name}\")\n</code></pre>"},{"location":"integrations/python-data-operations/#working-with-object-metadata","title":"Working with Object Metadata","text":""},{"location":"integrations/python-data-operations/#set-custom-object-metadata","title":"Set Custom Object Metadata","text":"<p>Attach custom metadata to objects:</p> <pre><code>import lakefs\nimport json\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\n# Create object with metadata\nobj = branch.object(\"data/important.csv\")\nobj.upload(\n    data=b\"id,value\\n1,100\",\n    metadata={\n        \"owner\": \"data-team\",\n        \"sensitivity\": \"public\",\n        \"version\": \"1.0\"\n    }\n)\n\nprint(\"Object uploaded with metadata\")\n</code></pre>"},{"location":"integrations/python-data-operations/#read-object-metadata","title":"Read Object Metadata","text":"<p>Retrieve object metadata:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\nobj = branch.object(\"data/important.csv\")\nstat = obj.stat()\n\nprint(f\"Object: {stat.path}\")\nprint(f\"Size: {stat.size_bytes}\")\nprint(f\"Metadata: {stat.metadata}\")\n</code></pre>"},{"location":"integrations/python-data-operations/#real-world-workflows","title":"Real-World Workflows","text":""},{"location":"integrations/python-data-operations/#data-cleanup","title":"Data Cleanup","text":"<p>Remove old and temporary files:</p> <pre><code>import lakefs\nfrom datetime import datetime, timedelta\n\ndef cleanup_old_files(repo_name, branch_name, days_old=7):\n    \"\"\"Delete files older than specified days\"\"\"\n    repo = lakefs.repository(repo_name)\n    branch = repo.branch(branch_name)\n\n    cutoff_time = datetime.now().timestamp() - (days_old * 24 * 60 * 60)\n\n    old_files = []\n\n    for obj in branch.objects():\n        if hasattr(obj, 'mtime') and obj.mtime &lt; cutoff_time:\n            old_files.append(obj.path)\n\n    if old_files:\n        print(f\"Found {len(old_files)} files older than {days_old} days\")\n        branch.delete_objects(old_files)\n        print(f\"Deleted {len(old_files)} old files\")\n        return len(old_files)\n    else:\n        print(\"No old files to delete\")\n        return 0\n\n\n# Usage:\ndeleted_count = cleanup_old_files(\"archive-repo\", \"main\", days_old=30)\nprint(f\"Cleanup complete: {deleted_count} files removed\")\n</code></pre>"},{"location":"integrations/python-data-operations/#bulk-data-import","title":"Bulk Data Import","text":"<p>Import multiple files efficiently:</p> <pre><code>import lakefs\nimport os\n\ndef bulk_import_files(repo_name, branch_name, local_dir, lakeFS_prefix):\n    \"\"\"Import all files from local directory\"\"\"\n    repo = lakefs.repository(repo_name)\n    branch = repo.branch(branch_name)\n\n    imported = 0\n    errors = 0\n\n    # Walk local directory\n    for root, dirs, files in os.walk(local_dir):\n        for filename in files:\n            local_path = os.path.join(root, filename)\n\n            # Calculate lakeFS path\n            rel_path = os.path.relpath(local_path, local_dir)\n            lakeFS_path = f\"{lakeFS_prefix}/{rel_path}\".replace(\"\\\\\", \"/\")\n\n            try:\n                # Read and upload file\n                with open(local_path, 'rb') as f:\n                    data = f.read()\n\n                branch.object(lakeFS_path).upload(data=data)\n                print(f\"  Imported: {lakeFS_path}\")\n                imported += 1\n\n            except Exception as e:\n                print(f\"  Error importing {lakeFS_path}: {e}\")\n                errors += 1\n\n    return imported, errors\n\n\n# Usage (pseudo-code - adjust for your environment):\n# imported, errors = bulk_import_files(\n#     \"my-repo\",\n#     \"main\",\n#     \"/local/data/directory\",\n#     \"data/imports\"\n# )\n# print(f\"Imported: {imported}, Errors: {errors}\")\n</code></pre>"},{"location":"integrations/python-data-operations/#stream-processing","title":"Stream Processing","text":"<p>Process large files efficiently:</p> <pre><code>import lakefs\nimport io\n\ndef process_csv_stream(repo_name, branch_name, file_path, processor_func):\n    \"\"\"Process large CSV file line by line\"\"\"\n    repo = lakefs.repository(repo_name)\n    branch = repo.branch(branch_name)\n\n    processed = 0\n\n    with branch.object(file_path).reader(mode='r') as f:\n        for line in f:\n            processor_func(line.strip())\n            processed += 1\n\n    return processed\n\n\n# Usage:\ndef count_records(line):\n    pass  # Do something with each line\n\n\ncount = process_csv_stream(\n    \"data-repo\",\n    \"main\",\n    \"data/large_file.csv\",\n    count_records\n)\n</code></pre>"},{"location":"integrations/python-data-operations/#creating-a-complete-data-pipeline","title":"Creating a Complete Data Pipeline","text":"<p>Implement an end-to-end pipeline with data operations, transactions, and merging:</p> <pre><code>import lakefs\n\n# Get repository and create experiment branch\nrepo = lakefs.repository(\"analytics-repo\")\nbranch = repo.branch(\"processing-v2\").create(source_reference=\"main\")\n\ntry:\n    # Upload raw data\n    branch.object(\"raw/input.csv\").upload(data=raw_data)\n\n    # Perform transformations with transactions\n    with branch.transact(commit_message=\"Process raw data\") as tx:\n        # Read and transform\n        with tx.object(\"raw/input.csv\").reader() as f:\n            processed = transform(f.read())\n\n        # Write processed data\n        tx.object(\"processed/output.csv\").upload(data=processed)\n\n    # Review changes before merging\n    changes = list(branch.uncommitted())\n    print(f\"Changes: {len(changes)} objects\")\n\n    # Merge to main if satisfied\n    branch.merge_into(repo.branch(\"main\"))\n\nexcept Exception as e:\n    print(f\"Error in pipeline: {e}\")\n    branch.delete()  # Clean up on failure\n</code></pre> <p>This pattern ensures:</p> <ul> <li>Raw data is preserved in isolation</li> <li>Transformations are atomic (all-or-nothing)</li> <li>Changes are reviewable before integration</li> <li>Failed pipelines can be safely cleaned up</li> </ul>"},{"location":"integrations/python-data-operations/#error-handling","title":"Error Handling","text":""},{"location":"integrations/python-data-operations/#handling-object-errors","title":"Handling Object Errors","text":"<pre><code>import lakefs\nfrom lakefs.exceptions import NotFoundException, ObjectNotFoundException\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\n# Object not found\ntry:\n    obj = branch.object(\"non-existent.csv\")\n    obj.delete()\nexcept (NotFoundException, ObjectNotFoundException):\n    print(\"Object not found\")\n\n# Permission denied\ntry:\n    obj = branch.object(\"data/file.csv\")\n    obj.upload(data=b\"data\")\nexcept Exception as e:\n    print(f\"Upload failed: {e}\")\n</code></pre>"},{"location":"integrations/python-getting-started/","title":"Getting Started with High-Level SDK","text":"<p>The High-Level SDK provides a Pythonic interface to lakeFS. This page covers installation, initialization, and configuration of the SDK.</p>"},{"location":"integrations/python-getting-started/#installation","title":"Installation","text":"<p>Install the High-Level SDK using pip:</p> <pre><code>pip install lakefs\n</code></pre> <p>Or upgrade to the latest version:</p> <pre><code>pip install --upgrade lakefs\n</code></pre>"},{"location":"integrations/python-getting-started/#initialization","title":"Initialization","text":"<p>The High-Level SDK by default will try to collect authentication parameters from the environment and attempt to create a default client. When working in an environment where lakectl is configured, it is not necessary to instantiate a lakeFS client or provide it for creating the lakeFS objects.</p> <p>In case no authentication parameters exist, it is also possible to explicitly create a lakeFS client.</p>"},{"location":"integrations/python-getting-started/#basic-client-initialization","title":"Basic Client Initialization","text":"<p>Here's how to instantiate a client:</p> <pre><code>from lakefs.client import Client\n\nclt = Client(\n    host=\"http://localhost:8000\",\n    username=\"AKIAIOSFODNN7EXAMPLE\",\n    password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n)\n</code></pre> <p>Info</p> <p>See here for instructions on how to log in with Python using your AWS role. This is applicable for enterprise users.</p>"},{"location":"integrations/python-getting-started/#ssl-configuration","title":"SSL Configuration","text":"<p>You can use TLS with a CA that is not trusted on the host by configuring the client with a CA cert bundle file. It should contain concatenated CA certificates in PEM format:</p> <pre><code>clt = Client(\n    host=\"https://lakefs.example.io\",\n    username=\"AKIAIOSFODNN7EXAMPLE\",\n    password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n    # Customize the CA certificates used to verify the peer.\n    ssl_ca_cert=\"path/to/concatenated_CA_certificates.PEM\",\n)\n</code></pre> <p>For connecting to a secure endpoint without verification (for test environments):</p> <pre><code>clt = Client(\n    host=\"https://lakefs.example.io\",\n    username=\"AKIAIOSFODNN7EXAMPLE\",\n    password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n    verify_ssl=False,\n)\n</code></pre> <p>Warning</p> <p>This setting allows well-known \"man-in-the-middle\", impersonation, and credential stealing attacks. Never use this in any production setting.</p>"},{"location":"integrations/python-getting-started/#proxy-configuration","title":"Proxy Configuration","text":"<p>To enable communication via proxies, add a proxy configuration:</p> <pre><code>clt = Client(\n    host=\"https://lakefs.example.io\",\n    username=\"AKIAIOSFODNN7EXAMPLE\",\n    password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n    ssl_ca_cert=\"(if needed)\",\n    proxy=\"&lt;proxy server URL&gt;\",\n)\n</code></pre>"},{"location":"integrations/python-getting-started/#next-steps","title":"Next Steps","text":"<p>Once you have initialized the SDK, explore the different operations:</p> <ul> <li>Branches &amp; Merging - Work with branches for feature development and experimentation</li> <li>References, Commits &amp; Tags - Navigate commit history, work with commits, and manage tags</li> <li>Transactions - Perform atomic operations</li> <li>Data Operations - Upload, download, and manage objects</li> </ul>"},{"location":"integrations/python-lakefs-spec/","title":"Using lakefs-spec for File System Operations","text":"<p>The lakefs-spec project provides a filesystem-like API to lakeFS, built on top of fsspec. This integration is perfect for data science workflows, pandas integration, and scenarios where you want S3-like operations.</p> <p>Note</p> <p>lakefs-spec is a third-party package maintained by the lakeFS community. For issues and questions, refer to the lakefs-spec repository.</p>"},{"location":"integrations/python-lakefs-spec/#when-to-use","title":"When to Use","text":"<p>Use lakefs-spec when you:</p> <ul> <li>Need file system-like operations (open, read, write, delete)</li> <li>Work with data science tools (pandas, dask, polars)</li> <li>Want an S3-compatible interface without managing branches explicitly</li> <li>Need to integrate with fsspec-compatible libraries</li> <li>Prefer familiar file operations over versioning abstractions</li> </ul> <p>For versioning-focused workflows (branches, tags, commits), use the High-Level SDK instead.</p>"},{"location":"integrations/python-lakefs-spec/#installation","title":"Installation","text":"<p>Install lakefs-spec using pip:</p> <pre><code>pip install lakefs-spec\n</code></pre> <p>Or upgrade to the latest version:</p> <pre><code>pip install --upgrade lakefs-spec\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#basic-setup","title":"Basic Setup","text":""},{"location":"integrations/python-lakefs-spec/#initializing-the-file-system","title":"Initializing the File System","text":"<pre><code>from lakefs_spec import LakeFSFileSystem\n\n# Auto-discover credentials from ~/.lakectl.yaml\nfs = LakeFSFileSystem()\n\n# Or provide explicit credentials\nfs = LakeFSFileSystem(\n    host=\"http://localhost:8000\",\n    username=\"your-access-key\",\n    password=\"your-secret-key\"\n)\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#file-operations","title":"File Operations","text":""},{"location":"integrations/python-lakefs-spec/#writing-files","title":"Writing Files","text":"<pre><code>from pathlib import Path\nfrom lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\n# Write text file\nfs.pipe(\"my-repo/main/data/text.txt\", b\"Hello, lakeFS!\")\n\n# Write from local file\nlocal_file = Path(\"local_data.csv\")\nlocal_file.write_text(\"id,name\\n1,Alice\\n2,Bob\")\nfs.put(str(local_file), \"my-repo/main/data/imported.csv\")\n\n# Write using context manager\nwith fs.open(\"my-repo/main/data/output.txt\", \"w\") as f:\n    f.write(\"Data written to lakeFS\")\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#reading-files","title":"Reading Files","text":"<pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\n# Read entire file\ndata = fs.cat(\"my-repo/main/data/text.txt\")\nprint(data.decode())\n\n# Read using context manager\nwith fs.open(\"my-repo/main/data/input.txt\", \"r\") as f:\n    content = f.read()\n    print(content)\n\n# Read in chunks (for large files)\nwith fs.open(\"my-repo/main/data/large_file.csv\", \"rb\") as f:\n    chunk = f.read(1024)\n    while chunk:\n        process(chunk)\n        chunk = f.read(1024)\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#listing-files","title":"Listing Files","text":"<pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\n# List files at path\nfiles = fs.ls(\"my-repo/main/data/\")\nfor file in files:\n    print(file)\n\n# Find files with glob pattern\ncsv_files = fs.glob(\"my-repo/main/**/*.csv\")\nfor csv in csv_files:\n    print(csv)\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#deleting-files","title":"Deleting Files","text":"<pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\n# Delete single file\nfs.rm(\"my-repo/main/data/temp.txt\")\n\n# Delete directory recursively\nfs.rm(\"my-repo/main/data/temp_dir\", recursive=True)\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#pandas-integration","title":"Pandas Integration","text":""},{"location":"integrations/python-lakefs-spec/#reading-data-into-pandas","title":"Reading Data into Pandas","text":"<pre><code>import pandas as pd\nfrom lakefs_spec import LakeFSFileSystem\n\n# Read CSV directly from lakeFS\ndf = pd.read_csv(\"lakefs://my-repo/main/data/dataset.csv\")\nprint(df.head())\n\n# Read Parquet\ndf = pd.read_parquet(\"lakefs://my-repo/main/data/data.parquet\")\n\n# Read JSON\ndf = pd.read_json(\"lakefs://my-repo/main/data/data.json\")\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#writing-data-from-pandas","title":"Writing Data from Pandas","text":"<pre><code>import pandas as pd\n\n# Create sample data\ndf = pd.DataFrame({\n    \"id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\"],\n    \"value\": [100, 200, 300, 400, 500]\n})\n\n# Write to lakeFS as CSV\ndf.to_csv(\"lakefs://my-repo/main/output/data.csv\", index=False)\n\n# Write as Parquet\ndf.to_parquet(\"lakefs://my-repo/main/output/data.parquet\")\n\n# Write as JSON\ndf.to_json(\"lakefs://my-repo/main/output/data.json\")\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#data-science-workflow-example","title":"Data Science Workflow Example","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n# Read training data\ntrain_df = pd.read_csv(\"lakefs://ml-repo/main/datasets/train.csv\")\ntest_df = pd.read_csv(\"lakefs://ml-repo/main/datasets/test.csv\")\n\n# Process data\ntrain_df[\"normalized_value\"] = (train_df[\"value\"] - train_df[\"value\"].mean()) / train_df[\"value\"].std()\ntest_df[\"normalized_value\"] = (test_df[\"value\"] - test_df[\"value\"].mean()) / test_df[\"value\"].std()\n\n# Save processed data\ntrain_df.to_parquet(\"lakefs://ml-repo/main/processed/train.parquet\")\ntest_df.to_parquet(\"lakefs://ml-repo/main/processed/test.parquet\")\n\nprint(\"Processing complete!\")\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#transactions","title":"Transactions","text":""},{"location":"integrations/python-lakefs-spec/#atomic-operations-with-transactions","title":"Atomic Operations with Transactions","text":"<pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\n# Perform atomic operations\nwith fs.transaction(\"my-repo\", \"main\") as tx:\n    # All operations happen on ephemeral branch\n    fs.pipe(\n        f\"my-repo/{tx.branch.id}/data/file1.txt\",\n        b\"Content 1\"\n    )\n    fs.pipe(\n        f\"my-repo/{tx.branch.id}/data/file2.txt\",\n        b\"Content 2\"\n    )\n\n    # Commit when done\n    tx.commit(message=\"Add files atomically\")\n    print(f\"Committed: {tx.branch.id}\")\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#transaction-with-error-handling","title":"Transaction with Error Handling","text":"<pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\ntry:\n    with fs.transaction(\"my-repo\", \"main\") as tx:\n        # Perform operations\n        fs.pipe(f\"my-repo/{tx.branch.id}/file.txt\", b\"data\")\n\n        # Validate\n        stat = fs.stat(f\"my-repo/{tx.branch.id}/file.txt\")\n        if stat[\"size\"] &lt; 100:\n            raise ValueError(\"File too small\")\n\n        tx.commit(message=\"Validated and committed\")\n\nexcept Exception as e:\n    print(f\"Transaction failed: {e}\")\n    print(\"Changes rolled back automatically\")\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#tagging-after-transaction","title":"Tagging After Transaction","text":"<pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\nwith fs.transaction(\"ml-repo\", \"main\") as tx:\n    # Train and save model\n    fs.pipe(f\"ml-repo/{tx.branch.id}/models/model.pkl\", model_data)\n\n    # Save metrics\n    fs.pipe(f\"ml-repo/{tx.branch.id}/metrics.json\", metrics_data)\n\n    # Commit\n    tx.commit(message=\"Model v1.0\")\n\n    # Tag as release\n    tx.tag(\"v1.0.0\")\n    print(\"Model released as v1.0.0\")\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#real-world-examples","title":"Real-World Examples","text":""},{"location":"integrations/python-lakefs-spec/#etl-pipeline","title":"ETL Pipeline","text":"<pre><code>import pandas as pd\nfrom lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\n# Extract: Read from multiple sources\nraw_files = fs.glob(\"my-repo/main/raw/*.csv\")\ndfs = [pd.read_csv(f\"lakefs://{f}\") for f in raw_files]\ncombined = pd.concat(dfs)\n\n# Transform: Clean and process\ncombined = combined.dropna()\ncombined[\"timestamp\"] = pd.to_datetime(combined[\"timestamp\"])\ncombined[\"normalized\"] = (combined[\"value\"] - combined[\"value\"].mean()) / combined[\"value\"].std()\n\n# Load: Write processed data\ncombined.to_parquet(\"lakefs://my-repo/main/processed/data.parquet\")\nprint(f\"ETL complete: {len(combined)} rows processed\")\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#data-analysis-and-reporting","title":"Data Analysis and Reporting","text":"<pre><code>import pandas as pd\nimport json\nfrom lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\n# Read data for analysis\ndf = pd.read_parquet(\"lakefs://analytics-repo/main/data/raw_data.parquet\")\n\n# Perform analysis\nsummary = {\n    \"total_records\": len(df),\n    \"mean_value\": float(df[\"value\"].mean()),\n    \"median_value\": float(df[\"value\"].median()),\n    \"std_value\": float(df[\"value\"].std())\n}\n\n# Save report\nreport = json.dumps(summary, indent=2)\nfs.pipe(\"lakefs://analytics-repo/main/reports/summary.json\", report.encode())\n\nprint(\"Analysis report saved\")\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#model-versioning","title":"Model Versioning","text":"<pre><code>import pickle\nfrom datetime import datetime\nfrom lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\ndef save_model_version(repo, model, version, metrics):\n    \"\"\"Save model with version and metrics\"\"\"\n    timestamp = datetime.now().isoformat()\n\n    with fs.transaction(repo, \"main\") as tx:\n        branch_id = tx.branch.id\n\n        # Save model\n        model_bytes = pickle.dumps(model)\n        fs.pipe(\n            f\"{repo}/{branch_id}/models/{version}/model.pkl\",\n            model_bytes\n        )\n\n        # Save metrics\n        metrics_json = json.dumps({\n            \"version\": version,\n            \"timestamp\": timestamp,\n            **metrics\n        })\n        fs.pipe(\n            f\"{repo}/{branch_id}/models/{version}/metrics.json\",\n            metrics_json.encode()\n        )\n\n        # Commit\n        tx.commit(message=f\"Model {version}\")\n\n        # Tag for reference\n        tx.tag(f\"model-{version}\")\n        print(f\"Model {version} saved and tagged\")\n\n\n# Usage:\nmodel = train_model(training_data)\nsave_model_version(\n    \"ml-repo\",\n    model,\n    \"v2.1.0\",\n    {\"accuracy\": 0.95, \"f1\": 0.94}\n)\n</code></pre>"},{"location":"integrations/python-lakefs-spec/#further-resources","title":"Further Resources","text":"<ul> <li>lakefs-spec Project - Official project documentation</li> <li>fsspec Documentation - Filesystem spec reference</li> </ul>"},{"location":"integrations/python-refs/","title":"Working with References, Commits &amp; Tags","text":"<p>References, commits, and tags are fundamental to understanding and managing versions in lakeFS. This guide covers navigating commit history, working with references, creating immutable snapshots with tags, and using metadata for tracking and lineage.</p>"},{"location":"integrations/python-refs/#understanding-references","title":"Understanding References","text":""},{"location":"integrations/python-refs/#what-are-references","title":"What are References?","text":"<p>A reference is any pointer to a commit in lakeFS:</p> <ul> <li>Branch: A mutable reference (changes as new commits are made)</li> <li>Tag: An immutable reference (always points to the same commit)</li> <li>Commit ID: A specific commit's unique identifier</li> <li>Ref Expression: Advanced reference syntax like <code>main~2</code> (2 commits before main)</li> </ul>"},{"location":"integrations/python-refs/#creating-references","title":"Creating References","text":"<p>Get reference objects for any valid reference:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\n\n# Reference to branch head\nmain_ref = repo.ref(\"main\")\nprint(f\"Main reference: {main_ref.id}\")\n\n# Reference to specific commit\ncommit_ref = repo.ref(\"abc123def456\")\nprint(f\"Commit reference: {commit_ref.id}\")\n\n# Reference to tag\ntag_ref = repo.ref(\"v1.0.0\")\nprint(f\"Tag reference: {tag_ref.id}\")\n\n# Advanced reference expressions\ntwo_back = repo.ref(\"main~2\")  # Two commits before main\nprint(f\"Two commits back: {two_back.id}\")\n</code></pre>"},{"location":"integrations/python-refs/#getting-commit-information-from-references","title":"Getting Commit Information from References","text":"<pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nref = repo.ref(\"main\")\n\n# Get the underlying commit\ncommit = ref.get_commit()\n\nprint(f\"Commit ID: {commit.id}\")\nprint(f\"Message: {commit.message}\")\nprint(f\"Committer: {commit.committer}\")\nprint(f\"Created: {commit.creation_date}\")\nprint(f\"Parents: {commit.parents}\")\nprint(f\"Metadata: {commit.metadata}\")\n</code></pre>"},{"location":"integrations/python-refs/#understanding-commits","title":"Understanding Commits","text":""},{"location":"integrations/python-refs/#what-are-commits","title":"What are Commits?","text":"<p>Commits create immutable snapshots of changes on a branch. Each commit has a unique ID and optional metadata:</p> <pre><code>branch = lakefs.repository(\"my-repo\").branch(\"main\")\n\n# Create a commit\nref = branch.commit(\n    message=\"Add new dataset\",\n    metadata={\"author\": \"data-team\", \"version\": \"1.0\"}\n)\nprint(f\"Committed: {ref.id}\")\n</code></pre> <p>Commits are the fundamental building blocks of version control in lakeFS. They allow you to:</p> <ul> <li>Track changes over time with unique identifiers</li> <li>Capture metadata for auditing and tracking</li> <li>Create reproducible snapshots for data lineage</li> <li>Understand who made changes and when</li> </ul>"},{"location":"integrations/python-refs/#working-with-commits","title":"Working with Commits","text":""},{"location":"integrations/python-refs/#getting-commit-details","title":"Getting Commit Details","text":"<p>Retrieve detailed information about a specific commit:</p> <pre><code>import lakefs\nfrom datetime import datetime\n\nrepo = lakefs.repository(\"my-data-repo\")\n\ntry:\n    # Get commit by ID\n    commit_ref = repo.commit(\"abc123def456xyz\")\n    commit = commit_ref.get_commit()\n\n    print(f\"Commit Details:\")\n    print(f\"  ID: {commit.id}\")\n    print(f\"  Message: {commit.message}\")\n    print(f\"  Committer: {commit.committer}\")\n    print(f\"  Timestamp: {datetime.fromtimestamp(commit.creation_date)}\")\n    print(f\"  Parents: {', '.join(commit.parents) if commit.parents else 'None'}\")\n\n    # Check for merge commit\n    if len(commit.parents) &gt; 1:\n        print(f\"  Type: Merge commit (from {len(commit.parents)} parents)\")\n    else:\n        print(f\"  Type: Regular commit\")\n\nexcept Exception as e:\n    print(f\"Commit not found: {e}\")\n</code></pre>"},{"location":"integrations/python-refs/#accessing-commit-metadata","title":"Accessing Commit Metadata","text":"<p>Retrieve custom metadata attached to commits:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\n# Get the latest commit\ncommit = branch.get_commit()\n\nprint(f\"Commit: {commit.id[:8]}\")\nprint(f\"Message: {commit.message}\")\n\nif commit.metadata:\n    print(\"Metadata:\")\n    for key, value in commit.metadata.items():\n        print(f\"  {key}: {value}\")\nelse:\n    print(\"No metadata\")\n</code></pre>"},{"location":"integrations/python-refs/#creating-commits-with-metadata","title":"Creating Commits with Metadata","text":"<p>Create commits with custom metadata for tracking:</p> <pre><code>import lakefs\nimport json\nfrom datetime import datetime\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\n# Upload data\nbranch.object(\"data/dataset.csv\").upload(data=b\"id,value\\n1,100\\n2,200\")\n\n# Commit with rich metadata\ncommit_ref = branch.commit(\n    message=\"Add customer dataset v2\",\n    metadata={\n        \"author\": \"data-team\",\n        \"version\": \"2.0\",\n        \"dataset-type\": \"raw\",\n        \"source\": \"database-export\",\n        \"record-count\": \"10000\",\n        \"timestamp\": datetime.now().isoformat(),\n        \"data-owner\": \"analytics-team@company.com\"\n    }\n)\n\nprint(f\"Committed: {commit_ref.id}\")\nprint(f\"Metadata stored for tracking\")\n</code></pre>"},{"location":"integrations/python-refs/#navigating-commit-history","title":"Navigating Commit History","text":""},{"location":"integrations/python-refs/#list-commits-log","title":"List Commits (Log)","text":"<p>View the commit history of a branch:</p> <pre><code>import lakefs\nfrom datetime import datetime\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\nprint(\"Recent commits:\")\nfor i, commit in enumerate(branch.log(max_amount=10)):\n    timestamp = datetime.fromtimestamp(commit.creation_date)\n    print(f\"  {i+1}. {commit.id[:8]} - {commit.message[:40]} ({timestamp})\")\n</code></pre>"},{"location":"integrations/python-refs/#track-commits-by-metadata","title":"Track Commits by Metadata","text":"<p>Find commits based on custom metadata:</p> <pre><code>import lakefs\n\ndef find_commits_by_metadata(repo_name, branch_name, key, value):\n    \"\"\"Find commits with specific metadata\"\"\"\n    repo = lakefs.repository(repo_name)\n    branch = repo.branch(branch_name)\n\n    matching_commits = []\n\n    for commit in branch.log(max_amount=1000):\n        if commit.metadata and commit.metadata.get(key) == value:\n            matching_commits.append(commit)\n\n    return matching_commits\n\n\n# Usage:\ncommits = find_commits_by_metadata(\"analytics-repo\", \"main\", \"dataset-type\", \"clean\")\nprint(f\"Found {len(commits)} commits with dataset-type=clean\")\n\nfor commit in commits[:5]:\n    print(f\"  {commit.id[:8]} - {commit.message}\")\n</code></pre>"},{"location":"integrations/python-refs/#comparing-references-diffs","title":"Comparing References (Diffs)","text":""},{"location":"integrations/python-refs/#diff-between-two-references","title":"Diff Between Two References","text":"<p>See what changed between any two references:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nmain = repo.ref(\"main\")\ndev = repo.ref(\"develop\")\n\nprint(\"Changes from main to develop:\")\nfor change in main.diff(other_ref=dev):\n    print(f\"  {change.type:10} {change.path} ({change.size_bytes} bytes)\")\n\n# Count changes\nchanges = list(main.diff(other_ref=dev))\nprint(f\"\\nTotal changes: {len(changes)}\")\n</code></pre>"},{"location":"integrations/python-refs/#diff-with-filtering","title":"Diff with Filtering","text":"<p>Filter diff results by path or change type:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\ntag_v1 = repo.ref(\"v1.0.0\")\ntag_v2 = repo.ref(\"v2.0.0\")\n\n# Get all changes\nall_changes = list(tag_v1.diff(other_ref=tag_v2))\n\n# Filter by change type\nadded = [c for c in all_changes if c.type == \"added\"]\nremoved = [c for c in all_changes if c.type == \"removed\"]\nchanged = [c for c in all_changes if c.type == \"changed\"]\n\nprint(f\"Added: {len(added)}\")\nprint(f\"Removed: {len(removed)}\")\nprint(f\"Changed: {len(changed)}\")\n\n# Filter by path prefix\ndata_changes = [c for c in all_changes if c.path.startswith(\"data/\")]\nprint(f\"Changes in data/ folder: {len(data_changes)}\")\n</code></pre>"},{"location":"integrations/python-refs/#detailed-diff-with-size-analysis","title":"Detailed Diff with Size Analysis","text":"<p>Analyze what changed with size information:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nref1 = repo.ref(\"commit1\")\nref2 = repo.ref(\"commit2\")\n\nprint(\"Detailed changes:\")\nfor change in ref1.diff(other_ref=ref2):\n    size_info = f\" ({change.size_bytes} bytes)\" if change.size_bytes else \"\"\n    print(f\"  {change.type:10} {change.path}{size_info}\")\n</code></pre>"},{"location":"integrations/python-refs/#working-with-tags","title":"Working with Tags","text":"<p>Tags are immutable pointers to specific commits in lakeFS, making them perfect for marking releases, data versions, and important snapshots.</p>"},{"location":"integrations/python-refs/#what-are-tags","title":"What are Tags?","text":"<p>Tags mark specific commits as important (e.g., releases):</p> <pre><code>import lakefs\n\ntag = lakefs.repository(\"my-repo\").tag(\"v1.0.0\").create(\n    source_ref=\"main\"\n)\n</code></pre> <p>Tags are immutable pointers to commits that allow you to:</p> <ul> <li>Mark releases for versioning and distribution</li> <li>Create snapshots for reproducibility and archival</li> <li>Reference important points in your data history</li> <li>Track data lineage across versions</li> </ul> <p>Unlike branches, tags never change once created, making them perfect for stable reference points.</p>"},{"location":"integrations/python-refs/#creating-tags","title":"Creating Tags","text":""},{"location":"integrations/python-refs/#create-a-simple-tag","title":"Create a Simple Tag","text":"<p>Create a tag pointing to the current head of a branch:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\n\n# Create a tag from the main branch's head\ntag = repo.tag(\"v1.0.0\").create(source_ref=\"main\")\n\nprint(f\"Created tag: v1.0.0\")\nprint(f\"Points to commit: {tag.get_commit().id}\")\n</code></pre>"},{"location":"integrations/python-refs/#create-a-tag-from-a-specific-commit","title":"Create a Tag from a Specific Commit","text":"<p>Create a tag pointing to any commit:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nmain = repo.branch(\"main\")\n\n# Get a specific commit from history\ncommits = list(main.log(max_amount=10))\n\nif commits:\n    # Tag an older commit\n    commit_to_tag = commits[0]  # Most recent\n    tag = repo.tag(\"v1.0.0-rc1\").create(source_ref=commit_to_tag.id)\n\n    print(f\"Tagged commit: {commit_to_tag.id[:8]}\")\n    print(f\"Tag name: v1.0.0-rc1\")\n</code></pre>"},{"location":"integrations/python-refs/#create-a-tag-from-another-tag","title":"Create a Tag from Another Tag","text":"<p>Create a tag based on an existing tag:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\n\ntry:\n    # Create a new tag from an existing tag\n    existing_tag = repo.tag(\"v1.0.0\")\n    new_tag = repo.tag(\"stable\").create(source_ref=existing_tag)\n\n    print(f\"New tag 'stable' points to same commit as 'v1.0.0'\")\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"integrations/python-refs/#conditional-tag-creation","title":"Conditional Tag Creation","text":"<p>Create a tag only if it doesn't already exist:</p> <pre><code>import lakefs\nfrom lakefs.exceptions import ConflictException\n\nrepo = lakefs.repository(\"my-data-repo\")\ntag_name = \"v2.0.0\"\n\ntry:\n    # Create tag with exist_ok=False (will fail if exists)\n    tag = repo.tag(tag_name).create(source_ref=\"main\", exist_ok=False)\n    print(f\"Created new tag: {tag_name}\")\n\nexcept ConflictException:\n    print(f\"Tag already exists: {tag_name}\")\n    tag = repo.tag(tag_name)\n    print(f\"Using existing tag: {tag.get_commit().id}\")\n</code></pre>"},{"location":"integrations/python-refs/#listing-tags","title":"Listing Tags","text":"<p>List all tags in a repository:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\n\nprint(\"All tags in repository:\")\nfor tag in repo.tags():\n    commit = tag.get_commit()\n    print(f\"  {tag.id:20} -&gt; {commit.id[:8]}... ({commit.message})\")\n</code></pre>"},{"location":"integrations/python-refs/#get-tag-information","title":"Get Tag Information","text":"<p>Get detailed information about a specific tag:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\ntag = repo.tag(\"v1.0.0\")\n\ntry:\n    commit = tag.get_commit()\n\n    print(f\"Tag: {tag.id}\")\n    print(f\"Commit ID: {commit.id}\")\n    print(f\"Message: {commit.message}\")\n    print(f\"Committer: {commit.committer}\")\n    print(f\"Created: {commit.creation_date}\")\n    print(f\"Metadata: {commit.metadata}\")\n\nexcept Exception as e:\n    print(f\"Tag not found: {e}\")\n</code></pre>"},{"location":"integrations/python-refs/#accessing-data-from-tags","title":"Accessing Data from Tags","text":""},{"location":"integrations/python-refs/#list-objects-in-a-tagged-version","title":"List Objects in a Tagged Version","text":"<p>List all objects in a specific tagged version:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\ntag_ref = repo.ref(\"v1.0.0\")  # Use ref() for tag access\n\n# List all objects in this tag\nprint(f\"Objects in v1.0.0:\")\nfor obj in tag_ref.objects():\n    print(f\"  {obj.path}\")\n\n# List specific prefix\nprint(f\"\\nModels in v1.0.0:\")\nfor obj in tag_ref.objects(prefix=\"models/\"):\n    if hasattr(obj, 'path'):  # It's a file, not a folder\n        print(f\"  {obj.path} ({obj.size_bytes} bytes)\")\n</code></pre>"},{"location":"integrations/python-refs/#read-data-from-tagged-version","title":"Read Data from Tagged Version","text":"<p>Read object contents from a specific tag:</p> <pre><code>import lakefs\nimport csv\nimport io\n\nrepo = lakefs.repository(\"my-data-repo\")\ntag_ref = repo.ref(\"v1.0.0\")\n\n# Read a CSV file from the tag\ntry:\n    obj = tag_ref.object(\"data/dataset.csv\")\n\n    with obj.reader(mode='r') as f:\n        reader = csv.reader(f)\n        headers = next(reader)\n        print(f\"Headers: {headers}\")\n\n        for row in reader:\n            print(f\"  {row}\")\n\nexcept Exception as e:\n    print(f\"Error reading file: {e}\")\n</code></pre>"},{"location":"integrations/python-refs/#compare-data-across-tagged-versions","title":"Compare Data Across Tagged Versions","text":"<p>Compare what changed between two tagged versions:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\ntag_v1 = repo.ref(\"v1.0.0\")\ntag_v2 = repo.ref(\"v2.0.0\")\n\n# See what changed\nprint(\"Changes from v1.0.0 to v2.0.0:\")\nfor change in tag_v1.diff(other_ref=tag_v2):\n    print(f\"  {change.type:10} {change.path}\")\n\n# Count change types\nchanges = list(tag_v1.diff(other_ref=tag_v2))\nadded = len([c for c in changes if c.type == \"added\"])\nremoved = len([c for c in changes if c.type == \"removed\"])\nchanged = len([c for c in changes if c.type == \"changed\"])\n\nprint(f\"\\nSummary: +{added} -{removed} ~{changed}\")\n</code></pre>"},{"location":"integrations/python-refs/#deleting-tags","title":"Deleting Tags","text":""},{"location":"integrations/python-refs/#delete-a-single-tag","title":"Delete a Single Tag","text":"<p>Remove a tag that's no longer needed:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\n\ntry:\n    tag = repo.tag(\"old-release\")\n    tag.delete()\n    print(\"Tag deleted: old-release\")\n\nexcept Exception as e:\n    print(f\"Delete failed: {e}\")\n</code></pre>"},{"location":"integrations/python-refs/#commit-relationships","title":"Commit Relationships","text":""},{"location":"integrations/python-refs/#identify-merge-commits","title":"Identify Merge Commits","text":"<p>Find and analyze merge commits:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\nprint(\"Merge commits:\")\nfor i, commit in enumerate(branch.log(max_amount=50)):\n    if len(commit.parents) &gt; 1:\n        print(f\"  {commit.id[:8]} - Merged {len(commit.parents)} branches\")\n        print(f\"    Message: {commit.message}\")\n        print(f\"    Parents: {', '.join([p[:8] for p in commit.parents])}\")\n</code></pre>"},{"location":"integrations/python-refs/#trace-commit-ancestry","title":"Trace Commit Ancestry","text":"<p>Follow a commit back through its parents. This is for better understanding of commit, we will prefer to use <code>log</code> operation to trace back changes:</p> <pre><code>import lakefs\n\ndef trace_ancestry(repo_name, commit_id, depth=5):\n    \"\"\"Trace commit ancestry up to specified depth\"\"\"\n    repo = lakefs.repository(repo_name)\n    ancestry = []\n\n    current_id = commit_id\n\n    for level in range(depth):\n        try:\n            commit_ref = repo.commit(current_id)\n            commit = commit_ref.get_commit()\n\n            ancestry.append({\n                \"level\": level,\n                \"commit_id\": commit.id[:8],\n                \"message\": commit.message,\n                \"parents\": commit.parents\n            })\n\n            # Move to first parent\n            if commit.parents:\n                current_id = commit.parents[0]\n            else:\n                break\n\n        except Exception as e:\n            print(f\"Error at level {level}: {e}\")\n            break\n\n    return ancestry\n\n\n# Usage:\nancestry = trace_ancestry(\"my-repo\", \"abc123def456\", depth=5)\nprint(\"Commit Ancestry:\")\nfor entry in ancestry:\n    indent = \"  \" * entry[\"level\"]\n    print(f\"{indent}\u2514\u2500 {entry['commit_id']} - {entry['message']}\")\n</code></pre>"},{"location":"integrations/python-refs/#real-world-workflows","title":"Real-World Workflows","text":""},{"location":"integrations/python-refs/#ml-model-release-workflow","title":"ML Model Release Workflow","text":"<p>Release trained models with versioning:</p> <pre><code>import lakefs\nimport json\n\ndef release_ml_model(repo_name, model_version, model_metrics):\n    \"\"\"\n    Create a versioned release of an ML model\n    \"\"\"\n    repo = lakefs.repository(repo_name)\n\n    try:\n        # Create release tag\n        tag_name = f\"model-v{model_version}\"\n        tag = repo.tag(tag_name).create(source_ref=\"main\")\n\n        commit = tag.get_commit()\n\n        print(f\"ML Model Released: {tag_name}\")\n        print(f\"  Commit: {commit.id[:8]}\")\n\n        # Read model metadata from tagged version\n        tag_ref = repo.ref(tag_name)\n\n        try:\n            with tag_ref.object(\"models/metadata.json\").reader() as f:\n                metadata = json.load(f)\n                print(f\"  Model: {metadata.get('name')}\")\n                print(f\"  Framework: {metadata.get('framework')}\")\n                print(f\"  Version: {metadata.get('version')}\")\n        except:\n            print(\"  (No metadata file)\")\n\n        # Store release info\n        release_info = {\n            \"version\": model_version,\n            \"commit\": commit.id,\n            \"metrics\": model_metrics,\n            \"tag\": tag_name\n        }\n\n        return release_info\n\n    except Exception as e:\n        print(f\"Model release failed: {e}\")\n        return None\n\n\n# Usage:\nmetrics = {\n    \"accuracy\": 0.945,\n    \"precision\": 0.92,\n    \"recall\": 0.96,\n    \"f1\": 0.939\n}\n\nrelease_info = release_ml_model(\"ml-repo\", \"3.2.0\", metrics)\nif release_info:\n    print(f\"\\nModel released and can be retrieved from tag: {release_info['tag']}\")\n</code></pre>"},{"location":"integrations/python-refs/#production-deployment-workflow","title":"Production Deployment Workflow","text":"<p>Manage production data versions:</p> <pre><code>import lakefs\n\ndef promote_to_production(repo_name, from_tag, environment):\n    \"\"\"\n    Promote a tagged version to production by creating an environment tag\n    \"\"\"\n    repo = lakefs.repository(repo_name)\n\n    try:\n        # Create environment-specific tag\n        env_tag_name = f\"prod-{environment}\"\n\n        # Delete old environment tag if it exists\n        try:\n            old_tag = repo.tag(env_tag_name)\n            old_tag.delete()\n            print(f\"Removed old {env_tag_name} tag\")\n        except:\n            pass  # Tag didn't exist\n\n        # Create new environment tag pointing to the same commit as version tag\n        source_tag = repo.tag(from_tag)\n        env_tag = repo.tag(env_tag_name).create(source_ref=source_tag)\n\n        print(f\"Promoted to {environment}\")\n        print(f\"  Source: {from_tag}\")\n        print(f\"  Target: {env_tag_name}\")\n        print(f\"  Commit: {env_tag.get_commit().id[:8]}\")\n\n        return env_tag_name\n\n    except Exception as e:\n        print(f\"Promotion failed: {e}\")\n        return None\n\n\n# Usage:\nenv_tag = promote_to_production(\"prod-repo\", \"v2.1.0\", \"us-west-1\")\nif env_tag:\n    print(f\"Production data updated to use {env_tag}\")\n</code></pre>"},{"location":"integrations/python-refs/#error-handling","title":"Error Handling","text":""},{"location":"integrations/python-refs/#handling-common-reference-errors","title":"Handling Common Reference Errors","text":"<pre><code>import lakefs\nfrom lakefs.exceptions import NotFoundException\n\nrepo = lakefs.repository(\"my-data-repo\")\n\n# Reference doesn't exist\ntry:\n    ref = repo.ref(\"non-existent-ref\")\n    commit = ref.get_commit()\nexcept NotFoundException:\n    print(\"Reference not found\")\n\n# Commit doesn't exist\ntry:\n    commit_ref = repo.commit(\"nonexistent123\")\n    commit = commit_ref.get_commit()\nexcept NotFoundException:\n    print(\"Commit not found\")\n\n# Invalid reference expression\ntry:\n    ref = repo.ref(\"main~1000\")  # Try to get 1000 commits back\n    commit = ref.get_commit()\nexcept NotFoundException:\n    print(\"Reference expression invalid or out of range\")\n</code></pre>"},{"location":"integrations/python-refs/#handling-tag-errors","title":"Handling Tag Errors","text":"<pre><code>import lakefs\nfrom lakefs.exceptions import ConflictException, NotFoundException\n\nrepo = lakefs.repository(\"my-data-repo\")\n\n# Tag already exists\ntry:\n    tag = repo.tag(\"v1.0.0\").create(source_ref=\"main\", exist_ok=False)\nexcept ConflictException:\n    print(\"Tag already exists\")\n    tag = repo.tag(\"v1.0.0\")\n\n# Tag doesn't exist\ntry:\n    tag = repo.tag(\"non-existent-tag\")\n    commit = tag.get_commit()\nexcept NotFoundException:\n    print(\"Tag not found\")\n</code></pre>"},{"location":"integrations/python-sdk/","title":"Using the Generated lakeFS SDK","text":"<p>The generated <code>lakefs-sdk</code> provides direct access to the lakeFS API based on the OpenAPI specification. This low-level SDK gives you complete control over API interactions and is useful when you need capabilities not covered by the High-Level SDK or when you prefer working directly with the API.</p>"},{"location":"integrations/python-sdk/#when-to-use","title":"When to Use","text":"<p>Use the generated SDK when you:</p> <ul> <li>Need low-level API access with full control over requests</li> <li>Want to use features not available in the High-Level SDK</li> <li>Prefer direct API interaction patterns</li> <li>Need to access all API endpoints programmatically</li> </ul> <p>Info</p> <p>For most common lakeFS operations (branches, tags, commits, objects), the High-Level SDK is recommended as it provides a more Pythonic interface.</p>"},{"location":"integrations/python-sdk/#installation","title":"Installation","text":"<p>Install the generated SDK using pip:</p> <pre><code>pip install lakefs-sdk\n</code></pre> <p>Or upgrade to the latest version:</p> <pre><code>pip install --upgrade lakefs-sdk\n</code></pre>"},{"location":"integrations/python-sdk/#basic-setup","title":"Basic Setup","text":""},{"location":"integrations/python-sdk/#initializing-the-sdk","title":"Initializing the SDK","text":"<pre><code>import lakefs_sdk\nfrom lakefs_sdk.rest import ApiException\n\n# Configure the API client\nconfiguration = lakefs_sdk.Configuration(\n    host=\"https://example.lakefs.io/api/v1\",\n    username=\"your-access-key\",\n    password=\"your-secret-key\"\n)\n\n# Create an API client\napi_client = lakefs_sdk.ApiClient(configuration)\n</code></pre>"},{"location":"integrations/python-sdk/#authentication-options","title":"Authentication Options","text":"<p>The SDK supports multiple authentication methods:</p> <pre><code>import lakefs_sdk\nimport os\n\n# HTTP Basic Authentication\nconfiguration = lakefs_sdk.Configuration(\n    host=\"https://example.lakefs.io/api/v1\",\n    username=os.environ[\"LAKEFS_ACCESS_KEY\"],\n    password=os.environ[\"LAKEFS_SECRET_KEY\"]\n)\n\n# JWT Bearer Token\nconfiguration = lakefs_sdk.Configuration(\n    host=\"https://example.lakefs.io/api/v1\",\n    access_token=os.environ[\"LAKEFS_TOKEN\"]\n)\n\napi_client = lakefs_sdk.ApiClient(configuration)\n</code></pre>"},{"location":"integrations/python-sdk/#simple-examples","title":"Simple Examples","text":""},{"location":"integrations/python-sdk/#list-repositories","title":"List Repositories","text":"<pre><code>import lakefs_sdk\nfrom lakefs_sdk.apis.repositories_api import RepositoriesApi\n\nconfiguration = lakefs_sdk.Configuration(\n    host=\"https://example.lakefs.io/api/v1\",\n    username=\"your-access-key\",\n    password=\"your-secret-key\"\n)\n\nwith lakefs_sdk.ApiClient(configuration) as api_client:\n    api_instance = RepositoriesApi(api_client)\n\n    try:\n        response = api_instance.list_repositories()\n        for repo in response.results:\n            print(f\"Repository: {repo.id}\")\n    except lakefs_sdk.ApiException as e:\n        print(f\"Error: {e}\")\n</code></pre>"},{"location":"integrations/python-sdk/#get-repository-details","title":"Get Repository Details","text":"<pre><code>import lakefs_sdk\nfrom lakefs_sdk.apis.repositories_api import RepositoriesApi\n\nconfiguration = lakefs_sdk.Configuration(\n    host=\"https://example.lakefs.io/api/v1\",\n    username=\"your-access-key\",\n    password=\"your-secret-key\"\n)\n\nwith lakefs_sdk.ApiClient(configuration) as api_client:\n    api_instance = RepositoriesApi(api_client)\n    repository = \"my-repo\"\n\n    try:\n        repo = api_instance.get_repository(repository)\n        print(f\"Repository: {repo.id}\")\n        print(f\"Storage: {repo.storage_namespace}\")\n        print(f\"Created: {repo.creation_date}\")\n    except lakefs_sdk.ApiException as e:\n        print(f\"Error: {e}\")\n</code></pre>"},{"location":"integrations/python-sdk/#list-branches","title":"List Branches","text":"<pre><code>import lakefs_sdk\nfrom lakefs_sdk.apis.branches_api import BranchesApi\n\nconfiguration = lakefs_sdk.Configuration(\n    host=\"https://example.lakefs.io/api/v1\",\n    username=\"your-access-key\",\n    password=\"your-secret-key\"\n)\n\nwith lakefs_sdk.ApiClient(configuration) as api_client:\n    api_instance = BranchesApi(api_client)\n    repository = \"my-repo\"\n\n    try:\n        response = api_instance.list_branches(repository)\n        for branch in response.results:\n            print(f\"Branch: {branch.id}\")\n    except lakefs_sdk.ApiException as e:\n        print(f\"Error: {e}\")\n</code></pre>"},{"location":"integrations/python-sdk/#create-a-commit","title":"Create a Commit","text":"<pre><code>import lakefs_sdk\nfrom lakefs_sdk.apis.commits_api import CommitsApi\nfrom lakefs_sdk.models.commit_creation import CommitCreation\n\nconfiguration = lakefs_sdk.Configuration(\n    host=\"https://example.lakefs.io/api/v1\",\n    username=\"your-access-key\",\n    password=\"your-secret-key\"\n)\n\nwith lakefs_sdk.ApiClient(configuration) as api_client:\n    api_instance = CommitsApi(api_client)\n    repository = \"my-repo\"\n    branch = \"main\"\n\n    commit_creation = CommitCreation(\n        message=\"My commit message\",\n        metadata={\"author\": \"data-team\"}\n    )\n\n    try:\n        commit = api_instance.commit(repository, branch, commit_creation)\n        print(f\"Committed: {commit.id}\")\n    except lakefs_sdk.ApiException as e:\n        print(f\"Error: {e}\")\n</code></pre>"},{"location":"integrations/python-sdk/#error-handling","title":"Error Handling","text":"<p>Handle API errors using <code>ApiException</code>:</p> <pre><code>import lakefs_sdk\nfrom lakefs_sdk.apis.repositories_api import RepositoriesApi\nfrom lakefs_sdk.rest import ApiException\n\nconfiguration = lakefs_sdk.Configuration(\n    host=\"https://example.lakefs.io/api/v1\",\n    username=\"your-access-key\",\n    password=\"your-secret-key\"\n)\n\nwith lakefs_sdk.ApiClient(configuration) as api_client:\n    api_instance = RepositoriesApi(api_client)\n\n    try:\n        response = api_instance.list_repositories()\n    except ApiException as e:\n        if e.status == 401:\n            print(\"Unauthorized - check credentials\")\n        elif e.status == 404:\n            print(\"Resource not found\")\n        else:\n            print(f\"API Error: {e.reason}\")\n</code></pre>"},{"location":"integrations/python-sdk/#full-api-reference","title":"Full API Reference","text":"<p>For complete API documentation and method signatures, see:</p> <ul> <li>Full SDK API Reference - Complete documentation for all API classes and methods</li> </ul>"},{"location":"integrations/python-transactions/","title":"Working with Transactions","text":"<p>Transactions enable you to perform multiple operations atomically on lakeFS, similar to database transactions. This guide covers creating and managing transactions for reliable data operations.</p>"},{"location":"integrations/python-transactions/#understanding-transactions","title":"Understanding Transactions","text":""},{"location":"integrations/python-transactions/#what-are-transactions","title":"What are Transactions?","text":"<p>A transaction in lakeFS:</p> <ol> <li>Creates an ephemeral (temporary) branch from a source branch</li> <li>Performs all operations on that ephemeral branch</li> <li>Atomically merges the branch back upon successful completion</li> <li>Automatically cleans up the ephemeral branch</li> <li>Rolls back if any error occurs</li> </ol> <p>This ensures either all operations succeed or none of them do.</p>"},{"location":"integrations/python-transactions/#creating-transactions","title":"Creating Transactions","text":""},{"location":"integrations/python-transactions/#basic-transaction","title":"Basic Transaction","text":"<p>Perform multiple operations atomically:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\ntry:\n    with branch.transact(commit_message=\"Add datasets\") as tx:\n        # All operations happen on ephemeral branch\n        tx.object(\"data/file1.csv\").upload(data=b\"id,value\\n1,100\\n2,200\")\n        tx.object(\"data/file2.csv\").upload(data=b\"id,name\\n1,Alice\\n2,Bob\")\n\n        print(\"Upload successful - changes will be committed atomically\")\n\n    # At this point, transaction is complete and merged\n    print(\"Transaction committed to main\")\n\nexcept Exception as e:\n    # If we get here, changes were rolled back\n    print(f\"Transaction failed and rolled back: {e}\")\n</code></pre>"},{"location":"integrations/python-transactions/#transaction-with-metadata","title":"Transaction with Metadata","text":"<p>Include metadata in the commit created by the transaction:</p> <pre><code>import lakefs\nfrom datetime import datetime\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\ntry:\n    with branch.transact(\n        commit_message=\"Import customer data\",\n        commit_metadata={\n            \"import-date\": datetime.now().isoformat(),\n            \"source\": \"database-export\",\n            \"record-count\": \"10000\"\n        }\n    ) as tx:\n        # Perform operations\n        tx.object(\"data/customers.csv\").upload(data=b\"id,name,email\\n1,Alice,alice@example.com\")\n\n        print(\"Data imported\")\n\n    # Transaction complete with metadata\n    print(\"Transaction committed with tracking metadata\")\n\nexcept Exception as e:\n    print(f\"Import failed: {e}\")\n</code></pre>"},{"location":"integrations/python-transactions/#transaction-with-tagging","title":"Transaction with Tagging","text":"<p>Create a tag after successful transaction completion:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\ntry:\n    with branch.transact(\n        commit_message=\"Production data release v1.5\",\n        tag=\"v1.5.0\"  # Tag is created if transaction succeeds\n    ) as tx:\n        # Make changes\n        tx.object(\"VERSION\").upload(data=b\"1.5.0\")\n        tx.object(\"data/prod.csv\").upload(data=b\"updated data\")\n\n        print(\"Production release in progress\")\n\n    print(\"Release tagged and deployed\")\n\nexcept Exception as e:\n    print(f\"Release failed: {e}\")\n    print(\"Changes rolled back, no tag created\")\n</code></pre>"},{"location":"integrations/python-transactions/#working-with-transaction-context","title":"Working with Transaction Context","text":""},{"location":"integrations/python-transactions/#performing-multiple-operations","title":"Performing Multiple Operations","text":"<p>Execute multiple operations within a transaction:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"analytics-repo\")\nbranch = repo.branch(\"develop\")\n\ntry:\n    with branch.transact(commit_message=\"Data preparation pipeline\") as tx:\n        # Step 1: Upload raw data\n        tx.object(\"raw/input.csv\").upload(data=b\"raw input data\")\n\n        # Step 2: Upload processing script\n        tx.object(\"scripts/transform.py\").upload(\n            data=b\"#!/usr/bin/env python\\n# Transformation logic\"\n        )\n\n        # Step 3: Upload intermediate results\n        tx.object(\"processed/output.csv\").upload(data=b\"processed output\")\n\n        # Step 4: Upload metadata\n        tx.object(\".metadata/pipeline_version.txt\").upload(data=b\"1.0\")\n\n        print(\"All pipeline stages added atomically\")\n\n    print(\"Pipeline committed successfully\")\n\nexcept Exception as e:\n    print(f\"Pipeline failed: {e}\")\n</code></pre>"},{"location":"integrations/python-transactions/#reading-and-modifying-data","title":"Reading and Modifying Data","text":"<p>Read objects and modify them within a transaction:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\ntry:\n    with branch.transact(commit_message=\"Data validation update\") as tx:\n        # Read existing data\n        try:\n            with tx.object(\"data/config.txt\").reader(mode='r') as f:\n                config_data = f.read()\n            print(f\"Current config: {config_data}\")\n        except:\n            config_data = \"\"\n\n        # Modify data\n        updated_config = config_data + \"\\nvalidation_enabled: true\"\n\n        # Write updated data\n        tx.object(\"data/config.txt\").upload(data=updated_config.encode())\n\n        print(\"Config updated\")\n\n    print(\"Configuration changes committed\")\n\nexcept Exception as e:\n    print(f\"Update failed: {e}\")\n</code></pre>"},{"location":"integrations/python-transactions/#iterating-and-modifying-objects","title":"Iterating and Modifying Objects","text":"<p>Process multiple objects within a transaction:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\ntry:\n    with branch.transact(commit_message=\"Bulk update data versions\") as tx:\n        # List objects and modify each one\n        processed_count = 0\n\n        for obj in tx.objects(prefix=\"data/\"):\n            # Read object\n            try:\n                with obj.reader(mode='r') as f:\n                    content = f.read()\n\n                # Add version marker\n                versioned_content = f\"# version: 2.0\\n{content}\"\n\n                # Write back\n                tx.object(obj.path).upload(data=versioned_content.encode())\n                processed_count += 1\n\n            except Exception as e:\n                print(f\"Error processing {obj.path}: {e}\")\n                raise  # Transaction will rollback\n\n        print(f\"Processed {processed_count} objects\")\n\n    print(f\"Bulk update committed: {processed_count} objects updated\")\n\nexcept Exception as e:\n    print(f\"Bulk update failed and rolled back: {e}\")\n</code></pre>"},{"location":"integrations/python-transactions/#transaction-error-handling","title":"Transaction Error Handling","text":""},{"location":"integrations/python-transactions/#basic-error-handling-in-transactions","title":"Basic Error Handling in Transactions","text":"<pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\ntry:\n    with branch.transact(commit_message=\"Data operations\") as tx:\n        # First operation\n        tx.object(\"data/file1.csv\").upload(data=b\"data1\")\n\n        # Second operation that might fail\n        try:\n            with tx.object(\"data/large_file.csv\").reader() as f:\n                large_content = f.read()\n\n            # Process large content\n            if len(large_content) &gt; 1000000:\n                raise ValueError(\"File too large\")\n\n        except ValueError as e:\n            print(f\"Validation failed: {e}\")\n            raise  # This will rollback the entire transaction\n\n        # Third operation\n        tx.object(\"data/file2.csv\").upload(data=b\"data2\")\n\nexcept Exception as e:\n    print(f\"Transaction rolled back: {e}\")\n</code></pre>"},{"location":"integrations/python-transactions/#transaction-with-cleanup-on-error","title":"Transaction with Cleanup on Error","text":"<pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\ncleanup_needed = False\n\ntry:\n    with branch.transact(\n        commit_message=\"Complex operation\",\n        delete_branch_on_error=True  # Default is True\n    ) as tx:\n        # Perform operations\n        cleanup_needed = True\n\n        tx.object(\"step1/file.csv\").upload(data=b\"step 1\")\n\n        # Simulate error\n        if True:  # In real code, some condition\n            raise Exception(\"Something went wrong in step 1\")\n\n        tx.object(\"step2/file.csv\").upload(data=b\"step 2\")\n        cleanup_needed = False\n\nexcept Exception as e:\n    if cleanup_needed:\n        print(f\"Operation failed, ephemeral branch automatically cleaned up\")\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"integrations/python-transactions/#transaction-with-conditional-rollback","title":"Transaction with Conditional Rollback","text":"<pre><code>import lakefs\n\ndef validate_data(data):\n    \"\"\"Validate data meets requirements\"\"\"\n    if len(data) == 0:\n        raise ValueError(\"Empty data not allowed\")\n    if b\"invalid\" in data:\n        raise ValueError(\"Data contains invalid markers\")\n    return True\n\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\ntry:\n    with branch.transact(commit_message=\"Data import with validation\") as tx:\n        # Upload data\n        data = b\"id,value\\n1,100\\n2,200\"\n\n        # Validate before committing\n        validate_data(data)\n\n        # If validation passes, commit\n        tx.object(\"data/validated.csv\").upload(data=data)\n\n        print(\"Data validation passed, changes committed\")\n\nexcept ValueError as e:\n    print(f\"Validation error - transaction rolled back: {e}\")\nexcept Exception as e:\n    print(f\"Transaction error: {e}\")\n</code></pre>"},{"location":"integrations/python-transactions/#real-world-workflows","title":"Real-World Workflows","text":""},{"location":"integrations/python-transactions/#data-quality-validation-workflow","title":"Data Quality Validation Workflow","text":"<p>Implement data quality checks before committing changes:</p> <pre><code>import lakefs\nimport csv\nimport io\n\ndef check_data_quality(repo_name, data_path, quality_rules):\n    \"\"\"\n    Check data quality before committing\n    Returns True if passes all checks, False otherwise\n    \"\"\"\n    repo = lakefs.repository(repo_name)\n    branch = repo.branch(\"main\")\n\n    try:\n        with branch.transact(commit_message=\"Quality checked data\") as tx:\n            # Read data\n            with tx.object(data_path).reader(mode='r') as f:\n                data = f.read()\n\n            # Run quality checks\n            errors = []\n\n            # Check 1: Not empty\n            if len(data) == 0:\n                errors.append(\"Data is empty\")\n\n            # Check 2: Valid CSV format\n            try:\n                reader = csv.reader(io.StringIO(data.decode()))\n                rows = list(reader)\n                if len(rows) &lt; 2:\n                    errors.append(\"Data has no rows\")\n            except Exception as e:\n                errors.append(f\"Invalid CSV format: {e}\")\n\n            # Check 3: Custom rules\n            for rule in quality_rules:\n                if not rule(data):\n                    errors.append(f\"Failed custom rule: {rule.__name__}\")\n\n            # If checks fail, raise error (transaction will rollback)\n            if errors:\n                error_msg = \"; \".join(errors)\n                raise ValueError(f\"Quality checks failed: {error_msg}\")\n\n            # If all pass, add quality marker\n            tx.object(f\"{data_path}.quality_passed\").upload(data=b\"true\")\n\n            print(f\"Data quality checks passed for {data_path}\")\n            return True\n\n    except Exception as e:\n        print(f\"Quality check failed: {e}\")\n        return False\n\n\n# Usage:\ndef rule_has_headers(data):\n    \"\"\"Custom rule: data must have headers\"\"\"\n    lines = data.decode().strip().split('\\n')\n    return len(lines) &gt; 0 and ',' in lines[0]\n\nsuccess = check_data_quality(\n    \"analytics-repo\",\n    \"data/incoming.csv\",\n    [rule_has_headers]\n)\n\nif success:\n    print(\"Data passed quality gates and is now committed\")\n</code></pre>"},{"location":"integrations/python-transactions/#database-synchronization-workflow","title":"Database Synchronization Workflow","text":"<p>Keep data synchronized with atomicity guarantees:</p> <pre><code>import lakefs\nimport json\nfrom datetime import datetime\n\ndef sync_database_export(repo_name, table_name, export_data, export_metadata):\n    \"\"\"\n    Atomically sync a database table export:\n    1. Store the export data\n    2. Update metadata\n    3. Update sync timestamp\n    4. All-or-nothing\n    \"\"\"\n    repo = lakefs.repository(repo_name)\n    branch = repo.branch(\"main\")\n\n    try:\n        with branch.transact(\n            commit_message=f\"Sync: {table_name}\",\n            commit_metadata={\n                \"sync-type\": \"database-export\",\n                \"table\": table_name,\n                \"sync-time\": datetime.now().isoformat()\n            }\n        ) as tx:\n            # Step 1: Store the data\n            data_path = f\"data/{table_name}.csv\"\n            tx.object(data_path).upload(data=export_data)\n\n            # Step 2: Store metadata\n            metadata = {\n                \"table\": table_name,\n                \"row_count\": export_metadata.get(\"row_count\", 0),\n                \"columns\": export_metadata.get(\"columns\", []),\n                \"sync_timestamp\": datetime.now().isoformat(),\n                \"source_database\": export_metadata.get(\"source\", \"unknown\")\n            }\n            metadata_path = f\".metadata/{table_name}_metadata.json\"\n            tx.object(metadata_path).upload(\n                data=json.dumps(metadata, indent=2).encode()\n            )\n\n            # Step 3: Update sync status\n            status = {\n                \"table\": table_name,\n                \"last_sync\": datetime.now().isoformat(),\n                \"status\": \"success\"\n            }\n            status_path = f\".sync/{table_name}_status.json\"\n            tx.object(status_path).upload(\n                data=json.dumps(status).encode()\n            )\n\n            print(f\"Synchronized {table_name}\")\n\n        print(f\"Sync committed atomically\")\n        return True\n\n    except Exception as e:\n        print(f\"Sync failed - rolling back: {e}\")\n        return False\n\n\n# Usage:\nsuccess = sync_database_export(\n    \"warehouse-repo\",\n    \"customers\",\n    b\"id,name,email\\n1,Alice,alice@example.com\\n2,Bob,bob@example.com\",\n    {\n        \"row_count\": 2,\n        \"columns\": [\"id\", \"name\", \"email\"],\n        \"source\": \"production_db\"\n    }\n)\n</code></pre>"},{"location":"integrations/python-transactions/#etl-pipeline-with-checkpoints","title":"ETL Pipeline with Checkpoints","text":"<p>Implement ETL with atomic checkpoints:</p> <pre><code>import lakefs\n\ndef etl_pipeline_step(repo_name, branch_name, step_name, step_logic):\n    \"\"\"\n    Run an ETL step with atomic checkpointing\n    \"\"\"\n    repo = lakefs.repository(repo_name)\n    branch = repo.branch(branch_name)\n\n    try:\n        with branch.transact(\n            commit_message=f\"ETL: {step_name}\",\n            commit_metadata={\"etl-step\": step_name}\n        ) as tx:\n            # Run the step logic\n            result = step_logic(tx)\n\n            # Create checkpoint\n            checkpoint = {\n                \"step\": step_name,\n                \"status\": \"completed\",\n                \"records_processed\": result.get(\"count\", 0)\n            }\n\n            tx.object(f\".checkpoints/{step_name}.json\").upload(\n                data=json.dumps(checkpoint).encode()\n            )\n\n            print(f\"Step '{step_name}' completed with checkpoint\")\n            return True\n\n    except Exception as e:\n        print(f\"Step '{step_name}' failed - checkpoint rolled back: {e}\")\n        return False\n\n\n# Define ETL steps\ndef extract_step(tx):\n    \"\"\"Extract data\"\"\"\n    tx.object(\"etl/01_raw/data.csv\").upload(data=b\"extracted data\")\n    return {\"count\": 1}\n\ndef transform_step(tx):\n    \"\"\"Transform data\"\"\"\n    tx.object(\"etl/02_transformed/data.csv\").upload(data=b\"transformed data\")\n    return {\"count\": 1}\n\ndef load_step(tx):\n    \"\"\"Load data\"\"\"\n    tx.object(\"etl/03_loaded/data.csv\").upload(data=b\"loaded data\")\n    return {\"count\": 1}\n\n\n# Execute pipeline\nimport json\n\nsteps = [\n    (\"extract\", extract_step),\n    (\"transform\", transform_step),\n    (\"load\", load_step)\n]\n\nfor step_name, step_func in steps:\n    success = etl_pipeline_step(\"data-repo\", \"main\", step_name, step_func)\n    if not success:\n        print(f\"ETL failed at {step_name}\")\n        break\nelse:\n    print(\"ETL pipeline completed successfully\")\n</code></pre>"},{"location":"integrations/python-transactions/#schema-evolution-with-validation","title":"Schema Evolution with Validation","text":"<p>Safely evolve data schema with validation:</p> <pre><code>import lakefs\nimport csv\nimport io\n\ndef evolve_schema(repo_name, table_name, old_schema, new_schema, migration_logic):\n    \"\"\"\n    Evolve a table schema with validation\n    Either all rows get migrated or none do\n    \"\"\"\n    repo = lakefs.repository(repo_name)\n    branch = repo.branch(\"main\")\n\n    try:\n        with branch.transact(\n            commit_message=f\"Schema evolution: {table_name}\",\n            commit_metadata={\n                \"schema-version\": new_schema.get(\"version\"),\n                \"migration-type\": \"schema-evolution\"\n            }\n        ) as tx:\n            # Read current data\n            try:\n                with tx.object(f\"data/{table_name}.csv\").reader(mode='r') as f:\n                    current_data = f.read()\n            except:\n                raise ValueError(f\"Table {table_name} not found\")\n\n            # Parse CSV\n            reader = csv.DictReader(io.StringIO(current_data.decode()))\n            rows = list(reader)\n\n            # Migrate each row\n            migrated_rows = []\n            for row in rows:\n                migrated_row = migration_logic(row, old_schema, new_schema)\n                migrated_rows.append(migrated_row)\n\n            # Write migrated data\n            output = io.StringIO()\n            writer = csv.DictWriter(\n                output,\n                fieldnames=new_schema.get(\"columns\", [])\n            )\n            writer.writeheader()\n            writer.writerows(migrated_rows)\n\n            tx.object(f\"data/{table_name}.csv\").upload(\n                data=output.getvalue().encode()\n            )\n\n            # Store schema version\n            tx.object(f\".schema/{table_name}.json\").upload(\n                data=json.dumps(new_schema).encode()\n            )\n\n            print(f\"Schema evolved for {table_name}\")\n            return True\n\n    except Exception as e:\n        print(f\"Schema evolution failed - rolled back: {e}\")\n        return False\n\n\n# Usage:\nold_schema = {\n    \"version\": 1,\n    \"columns\": [\"id\", \"name\", \"email\"]\n}\n\nnew_schema = {\n    \"version\": 2,\n    \"columns\": [\"id\", \"name\", \"email\", \"phone\"]\n}\n\ndef add_phone_column(row, old, new):\n    row[\"phone\"] = \"\"  # Default empty phone\n    return row\n\nsuccess = evolve_schema(\n    \"crm-repo\",\n    \"contacts\",\n    old_schema,\n    new_schema,\n    add_phone_column\n)\n</code></pre>"},{"location":"integrations/python-transactions/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"integrations/python-transactions/#conditional-transaction-execution","title":"Conditional Transaction Execution","text":"<pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\n# Check if we need to make changes\nchanges_needed = True  # In real code, some condition\n\nif changes_needed:\n    try:\n        with branch.transact(commit_message=\"Conditional update\") as tx:\n            tx.object(\"data/file.csv\").upload(data=b\"new data\")\n            print(\"Update committed\")\n    except Exception as e:\n        print(f\"Update failed: {e}\")\nelse:\n    print(\"No changes needed, skipping transaction\")\n</code></pre>"},{"location":"integrations/python-transactions/#transaction-retry-logic","title":"Transaction Retry Logic","text":"<pre><code>import lakefs\nimport time\n\ndef transact_with_retry(repo_name, branch_name, max_retries=3):\n    \"\"\"Retry transaction on failure\"\"\"\n    repo = lakefs.repository(repo_name)\n    branch = repo.branch(branch_name)\n\n    for attempt in range(max_retries):\n        try:\n            with branch.transact(commit_message=f\"Attempt {attempt + 1}\") as tx:\n                tx.object(\"data/file.csv\").upload(data=b\"data\")\n                print(\"Transaction succeeded\")\n                return True\n\n        except Exception as e:\n            print(f\"Attempt {attempt + 1} failed: {e}\")\n            if attempt &lt; max_retries - 1:\n                wait_time = 2 ** attempt  # Exponential backoff\n                print(f\"Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                print(f\"Failed after {max_retries} attempts\")\n                return False\n</code></pre>"},{"location":"integrations/python-transactions/#error-handling","title":"Error Handling","text":"<pre><code>import lakefs\nfrom lakefs.exceptions import NotFoundException, ForbiddenException, ServerException\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"main\")\n\ntry:\n    with branch.transact(commit_message=\"Safe operation\") as tx:\n        tx.object(\"data/file.csv\").upload(data=b\"data\")\n\nexcept NotFoundException:\n    print(\"Branch or repository not found\")\nexcept ForbiddenException:\n    print(\"Permission denied - cannot write to branch\")\nexcept ServerException as e:\n    print(f\"Server error - transaction rolled back: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"integrations/python-versioning-branches/","title":"Working with Branches &amp; Merging","text":"<p>Branches are the foundation of version control in lakeFS. They enable parallel development, safe experimentation, and collaborative data management. This guide covers all branch operations using the Python SDK.</p>"},{"location":"integrations/python-versioning-branches/#understanding-repositories-and-branches","title":"Understanding Repositories and Branches","text":"Repositories <p>A repository is a versioned storage namespace that holds all your data and version history. Create a repository by specifying a storage location:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-repo\").create(\n    storage_namespace=\"s3://my-bucket/data\"\n)\n</code></pre> Branches <p>Branches enable parallel development and experimentation. Each branch maintains its own version of the data:</p> <pre><code>branch = lakefs.repository(\"my-repo\").branch(\"main\")\nexperiment_branch = lakefs.repository(\"my-repo\").branch(\"experiment1\").create(\n    source_reference=\"main\"\n)\n</code></pre>"},{"location":"integrations/python-versioning-branches/#creating-and-listing-branches","title":"Creating and Listing Branches","text":"Creating a Branch <p>Create a new branch from any reference (branch, tag, or commit):</p> <pre><code>import lakefs\n\n# Initialize repository\nrepo = lakefs.repository(\"my-data-repo\")\n\n# Create a branch from main\nexperiment_branch = repo.branch(\"experiment-1\").create(source_reference=\"main\")\n\nprint(f\"Created branch: {experiment_branch.id}\")\n\n# Check what commit this branch points to\ncommit = experiment_branch.get_commit()\nprint(f\"Branch head: {commit.id}\")\n</code></pre> <p>Using an explicit client:</p> <pre><code>import lakefs\nfrom lakefs.client import Client\n\n# Create client\nclient = Client(\n    host=\"http://localhost:8000\",\n    username=\"your-access-key\",\n    password=\"your-secret-key\"\n)\n\n# Create branch with explicit client\nbranch = lakefs.repository(\"my-repo\", client=client).branch(\"dev\").create(\n    source_reference=\"main\"\n)\n</code></pre> Listing Branches <p>List all branches in a repository:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\n\nprint(\"All branches:\")\nfor branch in repo.branches():\n    commit = branch.get_commit()\n    print(f\"  {branch.id:20} -&gt; {commit.id[:8]}... ({commit.message})\")\n</code></pre> Checking Branch Head <p>Get the commit a branch is currently pointing to:</p> <pre><code>import lakefs\n\nbranch = lakefs.repository(\"my-data-repo\").branch(\"main\")\n\n# Get the head commit\nhead_commit = branch.get_commit()\nprint(f\"Current head: {head_commit.id}\")\nprint(f\"Committed by: {head_commit.committer}\")\nprint(f\"Message: {head_commit.message}\")\n\n# Or use the head property\nhead_ref = branch.head\nprint(f\"Head reference ID: {head_ref.id}\")\n</code></pre>"},{"location":"integrations/python-versioning-branches/#working-with-branch-content","title":"Working with Branch Content","text":"Viewing Uncommitted Changes <p>See what's changed on a branch since the last commit:</p> <pre><code>import lakefs\n\nbranch = lakefs.repository(\"my-data-repo\").branch(\"feature-branch\")\n\n# List uncommitted changes\nprint(\"Uncommitted changes:\")\nfor change in branch.uncommitted():\n    print(f\"  {change.type:10} {change.path} ({change.size_bytes} bytes)\")\n\n# Count changes\nchange_count = len(list(branch.uncommitted()))\nprint(f\"Total changes: {change_count}\")\n\n# Filter changes by prefix\ndata_changes = [c for c in branch.uncommitted() if c.path.startswith(\"data/\")]\nprint(f\"Changes in data/ folder: {len(data_changes)}\")\n</code></pre> Committing Changes <p>Create a commit with your changes:</p> <pre><code>import lakefs\n\nbranch = lakefs.repository(\"my-data-repo\").branch(\"feature-branch\")\n\n# Upload some data first\nbranch.object(\"data/dataset.csv\").upload(data=b\"id,value\\n1,100\\n2,200\")\n\n# Commit with message and metadata\nref = branch.commit(\n    message=\"Add customer dataset\",\n    metadata={\n        \"author\": \"data-team\",\n        \"version\": \"1.0\",\n        \"dataset-type\": \"raw\"\n    }\n)\n\nprint(f\"Committed: {ref.id}\")\nprint(f\"Message: {ref.get_commit().message}\")\nprint(f\"Metadata: {ref.get_commit().metadata}\")\n</code></pre>"},{"location":"integrations/python-versioning-branches/#comparing-branches","title":"Comparing Branches","text":"Diff Between References <p>See what changed between two branches or commits:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nmain_branch = repo.branch(\"main\")\nfeature_branch = repo.branch(\"feature-add-models\")\n\n# Compare branches\nprint(\"Changes in feature-add-models vs main:\")\nfor change in main_branch.diff(other_ref=feature_branch):\n    print(f\"  {change.type:10} {change.path}\")\n\n# Count different types of changes\nchanges = list(main_branch.diff(other_ref=feature_branch))\nadded = len([c for c in changes if c.type == \"added\"])\nremoved = len([c for c in changes if c.type == \"removed\"])\nmodified = len([c for c in changes if c.type == \"modified\"])\n\nprint(f\"Added: {added}, Removed: {removed}, Modified: {modified}\")\n</code></pre>"},{"location":"integrations/python-versioning-branches/#merging-branches","title":"Merging Branches","text":"Merging Into Another Branch <p>Merge changes from one branch into another:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nfeature_branch = repo.branch(\"feature-branch\")\nmain_branch = repo.branch(\"main\")\n\ntry:\n    # Merge feature branch into main\n    merge_result = feature_branch.merge_into(main_branch)\n    print(f\"Merge successful: {merge_result}\")\n\n    # Verify merge by checking that differences are gone\n    remaining_diffs = list(main_branch.diff(other_ref=feature_branch))\n    print(f\"Remaining differences: {len(remaining_diffs)}\")\n\nexcept Exception as e:\n    print(f\"Merge failed: {e}\")\n    # Resolve conflicts or adjust data and try again\n</code></pre> Merge with Conflict Detection <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\n\ntry:\n    branch1 = repo.branch(\"feature-1\")\n    branch2 = repo.branch(\"feature-2\")\n    main = repo.branch(\"main\")\n\n    # Check for conflicts before merging\n    conflicts = list(main.diff(other_ref=branch1))\n\n    if conflicts:\n        print(f\"Potential conflicts: {len(conflicts)} changes\")\n        for change in conflicts[:5]:  # Show first 5\n            print(f\"  {change.type}: {change.path}\")\n\n    # Proceed with merge if acceptable\n    merge_commit = branch1.merge_into(main)\n    print(f\"Merged into main: {merge_commit}\")\n\nexcept Exception as e:\n    print(f\"Merge error: {e}\")\n</code></pre>"},{"location":"integrations/python-versioning-branches/#cherry-picking-commits","title":"Cherry-Picking Commits","text":"<p>Apply a specific commit from one branch to another:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nmain_branch = repo.branch(\"main\")\nrelease_branch = repo.branch(\"release-v1.0\")\n\ntry:\n    # Cherry-pick a specific commit onto release branch\n    # First, get a commit ID from main\n    main_commits = list(main_branch.log(max_amount=5))\n    if main_commits:\n        commit_to_cherry_pick = main_commits[0]\n\n        # Cherry-pick it onto release branch\n        new_commit = release_branch.cherry_pick(commit_to_cherry_pick.id)\n        print(f\"Cherry-picked commit: {new_commit.id}\")\n        print(f\"Message: {new_commit.message}\")\n\nexcept Exception as e:\n    print(f\"Cherry-pick failed: {e}\")\n</code></pre>"},{"location":"integrations/python-versioning-branches/#reverting-changes","title":"Reverting Changes","text":"Revert a Commit <p>Undo the changes from a specific commit:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\nbranch = repo.branch(\"develop\")\n\ntry:\n    # Get recent commits\n    recent_commits = list(branch.log(max_amount=10))\n\n    if len(recent_commits) &gt; 1:\n        # Revert the most recent commit\n        commit_to_revert = recent_commits[0]\n\n        revert_result = branch.revert(reference=commit_to_revert.id)\n        print(f\"Reverted commit: {commit_to_revert.id}\")\n        print(f\"New commit created: {revert_result.id}\")\n\nexcept Exception as e:\n    print(f\"Revert failed: {e}\")\n</code></pre> Reverting Merge Commits <p>When reverting a merge commit, you can specify which parent to revert against:</p> <pre><code>import lakefs\n\nbranch = lakefs.repository(\"my-data-repo\").branch(\"main\")\n\ntry:\n    # Get commit history\n    commits = list(branch.log(max_amount=5))\n\n    for commit in commits:\n        # If this is a merge commit (has multiple parents)\n        if len(commit.parents) &gt; 1:\n            # Revert against parent 1 (the original main)\n            result = branch.revert(reference=commit.id, parent_number=1)\n            print(f\"Reverted merge commit: {commit.id}\")\n            break\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"integrations/python-versioning-branches/#resetting-branches","title":"Resetting Branches","text":"Reset Uncommitted Changes <p>Discard uncommitted changes on a branch:</p> <pre><code>import lakefs\n\nbranch = lakefs.repository(\"my-data-repo\").branch(\"feature-branch\")\n\n# Show uncommitted changes before reset\nchanges_before = list(branch.uncommitted())\nprint(f\"Uncommitted changes before reset: {len(changes_before)}\")\n\ntry:\n    # Reset all changes\n    branch.reset_changes(path_type=\"reset\")\n\n    # Verify changes are gone\n    changes_after = list(branch.uncommitted())\n    print(f\"Uncommitted changes after reset: {len(changes_after)}\")\n\nexcept Exception as e:\n    print(f\"Reset failed: {e}\")\n</code></pre> Reset Changes for Specific Paths <p>Reset changes only for certain paths or prefixes:</p> <pre><code>import lakefs\n\nbranch = lakefs.repository(\"my-data-repo\").branch(\"develop\")\n\ntry:\n    # Reset changes for a specific object\n    branch.reset_changes(path_type=\"object\", path=\"data/temp.csv\")\n    print(\"Reset: data/temp.csv\")\n\n    # Reset all changes in a folder (common prefix)\n    branch.reset_changes(path_type=\"common_prefix\", path=\"logs/\")\n    print(\"Reset: logs/*\")\n\n    # Verify remaining changes\n    remaining = list(branch.uncommitted())\n    print(f\"Remaining changes: {len(remaining)}\")\n\nexcept Exception as e:\n    print(f\"Reset error: {e}\")\n</code></pre>"},{"location":"integrations/python-versioning-branches/#deleting-branches","title":"Deleting Branches","text":"Delete a Branch <p>Remove a branch that's no longer needed:</p> <pre><code>import lakefs\n\nrepo = lakefs.repository(\"my-data-repo\")\n\ntry:\n    # Delete a branch\n    branch = repo.branch(\"old-experiment\")\n    branch.delete()\n    print(\"Branch deleted: old-experiment\")\n\nexcept Exception as e:\n    print(f\"Delete failed: {e}\")\n\n# Verify it's gone\nremaining_branches = [b.id for b in repo.branches()]\nprint(f\"Remaining branches: {remaining_branches}\")\n</code></pre>"},{"location":"integrations/python-versioning-branches/#real-world-workflows","title":"Real-World Workflows","text":""},{"location":"integrations/python-versioning-branches/#isolated-devtest-environment","title":"Isolated Dev/Test Environment","text":"Feature Branch Workflow <p>Implement a typical feature development workflow:</p> <pre><code>import lakefs\n\ndef feature_workflow(repo_name, feature_name, data_updates):\n    \"\"\"\n    Complete feature branch workflow:\n    1. Create feature branch\n    2. Make changes\n    3. Test/commit\n    4. Merge to main\n    \"\"\"\n    repo = lakefs.repository(repo_name)\n    main = repo.branch(\"main\")\n    feature = repo.branch(feature_name)\n\n    try:\n        # 1. Create feature branch from main\n        feature.create(source_reference=\"main\")\n        print(f\"Created feature branch: {feature_name}\")\n\n        # 2. Make changes\n        for path, data in data_updates.items():\n            feature.object(path).upload(data=data)\n        print(f\"Uploaded {len(data_updates)} objects\")\n\n        # 3. Review changes\n        changes = list(feature.uncommitted())\n        print(f\"Changes to commit: {len(changes)}\")\n\n        # 4. Commit\n        commit_ref = feature.commit(\n            message=f\"Implement {feature_name}\",\n            metadata={\"feature\": feature_name}\n        )\n        print(f\"Committed: {commit_ref.id}\")\n\n        # 5. Verify diff before merge\n        diff = list(main.diff(other_ref=feature))\n        print(f\"Ready to merge {len(diff)} changes\")\n\n        # 6. Merge to main\n        merge_result = feature.merge_into(main)\n        print(f\"Merged to main: {merge_result}\")\n\n        # 7. Cleanup\n        feature.delete()\n        print(f\"Deleted feature branch: {feature_name}\")\n\n        return True\n\n    except Exception as e:\n        print(f\"Feature workflow failed: {e}\")\n        # Cleanup on error\n        try:\n            feature.delete()\n        except:\n            pass\n        return False\n\n\n# Usage:\nfeature_workflow(\n    repo_name=\"analytics-repo\",\n    feature_name=\"add-customer-metrics\",\n    data_updates={\n        \"data/metrics/customer_v2.csv\": b\"id,value\\n1,100\\n2,200\",\n        \"data/metrics/metadata.json\": b'{\"version\": \"2\", \"date\": \"2024-01-15\"}'\n    }\n)\n</code></pre> Experimentation Branch Workflow <p>Create isolated branches for experiments:</p> <pre><code>import lakefs\nimport time\n\ndef create_experiment(repo_name, experiment_name, experiment_logic):\n    \"\"\"Run an experiment in isolation, keep results if successful\"\"\"\n    repo = lakefs.repository(repo_name)\n    exp_branch = repo.branch(experiment_name)\n\n    try:\n        # Create isolated experiment branch\n        exp_branch.create(source_reference=\"main\")\n        print(f\"Started experiment: {experiment_name}\")\n\n        # Run experiment logic\n        experiment_logic(exp_branch)\n\n        # Commit results\n        changes = list(exp_branch.uncommitted())\n        if changes:\n            exp_branch.commit(\n                message=f\"Results from {experiment_name}\",\n                metadata={\"experiment\": experiment_name, \"timestamp\": str(time.time())}\n            )\n            print(f\"Experiment successful, committed {len(changes)} changes\")\n\n        return exp_branch.get_commit().id\n\n    except Exception as e:\n        print(f\"Experiment failed: {e}\")\n        # Clean up failed experiment\n        try:\n            exp_branch.delete()\n        except:\n            pass\n        return None\n\n\n# Example experiment\ndef model_training_experiment(branch):\n    # Simulate model training\n    branch.object(\"models/trained_v2.pkl\").upload(data=b\"model_binary_data\")\n    branch.object(\"logs/training.log\").upload(data=b\"Training complete\")\n\n\n# Usage:\nresult_commit = create_experiment(\n    repo_name=\"ml-repo\",\n    experiment_name=\"neural-net-v3\",\n    experiment_logic=model_training_experiment\n)\n\nif result_commit:\n    print(f\"Keep results from commit: {result_commit}\")\n</code></pre> Release Branch Management <p>Manage release versions with branches and tags:</p> <pre><code>import lakefs\n\ndef create_release(repo_name, version_tag):\n    \"\"\"Create a release: branch from main, prepare, merge, tag\"\"\"\n    repo = lakefs.repository(repo_name)\n    main = repo.branch(\"main\")\n    release_branch = repo.branch(f\"release-{version_tag}\")\n\n    try:\n        # 1. Create release branch\n        release_branch.create(source_reference=\"main\")\n        print(f\"Created release branch: release-{version_tag}\")\n\n        # 2. Any release-specific changes (version bumps, etc)\n        release_branch.object(\"VERSION\").upload(data=version_tag.encode())\n        release_branch.object(\"RELEASE_NOTES.md\").upload(\n            data=f\"# Release {version_tag}\\nDate: 2024-01-15\".encode()\n        )\n\n        # 3. Commit release changes\n        release_ref = release_branch.commit(\n            message=f\"Release {version_tag}\",\n            metadata={\"version\": version_tag, \"type\": \"release\"}\n        )\n\n        # 4. Merge back to main\n        release_branch.merge_into(main)\n        print(f\"Merged release to main\")\n\n        # 5. Create tag for this release\n        tag = repo.tag(f\"v{version_tag}\").create(source_ref=release_ref)\n        print(f\"Tagged release: v{version_tag}\")\n\n        # 6. Clean up release branch\n        release_branch.delete()\n        print(f\"Cleaned up release branch\")\n\n        return f\"v{version_tag}\"\n\n    except Exception as e:\n        print(f\"Release creation failed: {e}\")\n        return None\n\n\n# Usage:\nrelease_tag = create_release(\"prod-repo\", \"1.5.0\")\nif release_tag:\n    print(f\"Release ready: {release_tag}\")\n</code></pre>"},{"location":"integrations/python-versioning-branches/#error-handling","title":"Error Handling","text":"Handling Common Branch Errors <pre><code>import lakefs\nfrom lakefs.exceptions import NotFoundException, ConflictException, ForbiddenException\n\nrepo = lakefs.repository(\"my-data-repo\")\n\n# Branch already exists\ntry:\n    branch = repo.branch(\"main\").create(source_reference=\"main\", exist_ok=False)\nexcept ConflictException:\n    print(\"Branch already exists\")\n    branch = repo.branch(\"main\")\n\n# Branch doesn't exist\ntry:\n    branch = repo.branch(\"non-existent\")\n    commit = branch.get_commit()\nexcept NotFoundException:\n    print(\"Branch not found\")\n\n# Protected branch (cannot delete)\ntry:\n    repo.branch(\"main\").delete()\nexcept ForbiddenException:\n    print(\"Cannot delete protected branch\")\n\n# Source reference not found\ntry:\n    branch = repo.branch(\"new-branch\").create(\n        source_reference=\"non-existent-ref\"\n    )\nexcept NotFoundException:\n    print(\"Source reference does not exist\")\n</code></pre>"},{"location":"integrations/python/","title":"Use Python to interact with lakeFS","text":"<p>Warning</p> <p>If your project is currently using the legacy Python <code>lakefs-client</code>, please be aware that this version has been deprecated. As of release v1.44.0, it's no longer supported for new updates or features.</p>"},{"location":"integrations/python/#getting-started","title":"Getting Started","text":"<p>New to lakeFS Python SDK? Start with the High-Level SDK Guide to install and configure the recommended Python package.</p> <p>The High-Level SDK provides an intuitive interface for:</p> <ul> <li>Branches &amp; Merging - Feature branch workflows</li> <li>References, Commits &amp; Tags - References, Commits &amp; Tags</li> <li>Transactions - Atomic operations</li> <li>Data Operations - Batch operations and cleanup</li> </ul>"},{"location":"integrations/python/#integration-options","title":"Integration Options","text":"<p>lakeFS provides multiple Python packages to suit different use cases and preferences:</p> Package Abstraction Best For Installation Learning Curve High-Level SDK High Versioning operations, data operations, transactions <code>pip install lakefs</code> Low Generated SDK Low Direct API access, full API surface, programmatic control <code>pip install lakefs-sdk</code> Medium lakefs-spec High File system operations, pandas/data science integration, S3-like interface <code>pip install lakefs-spec</code> Low Boto / S3 Gateway Medium S3-compatible operations, existing S3 workflows, direct gateway access <code>pip install boto3</code> Low"},{"location":"integrations/python/#references-resources","title":"References &amp; Resources","text":"<ul> <li>High Level Python SDK Documentation: https://pydocs-lakefs.lakefs.io</li> <li>Generated Python SDK Documentation: https://pydocs-sdk.lakefs.io</li> <li>lakefs-spec Project: https://lakefs-spec.org</li> <li>Boto S3 Router: https://github.com/treeverse/boto-s3-router</li> </ul>"},{"location":"integrations/r/","title":"Using R with lakeFS","text":"<p>R is a powerful language used widely in data science. lakeFS interfaces with R in two ways: </p> <ul> <li>To read and write data in lakeFS use standard S3 tools such as the <code>aws.s3</code> library. lakeFS has a S3 gateway which presents a lakeFS repository as an S3 bucket. </li> <li>For working with lakeFS operations such as branches and commits use the API for which can be accessed from R using the <code>httr</code> library. </li> </ul> <p>Examples</p> <p>To see examples of R in action with lakeFS please visit the lakeFS-samples repository and the sample notebooks</p>"},{"location":"integrations/r/#reading-and-writing-from-lakefs-with-r","title":"Reading and Writing from lakeFS with R","text":"<p>Working with data stored in lakeFS from R is the same as you would with an S3 bucket, via the S3 Gateway that lakeFS provides.</p> <p>You can use any library that interfaces with S3. In this example we'll use the aws.s3 library.</p> <pre><code>install.packages(c(\"aws.s3\"))\nlibrary(aws.s3)\n</code></pre>"},{"location":"integrations/r/#configuration","title":"Configuration","text":"<p>The R S3 client documentation includes full details of the configuration options available. A good approach for using it with lakeFS set the endpoint and authentication details as environment variables: </p> <pre><code>Sys.setenv(\"AWS_ACCESS_KEY_ID\" = \"AKIAIOSFODNN7EXAMPLE\",\n           \"AWS_SECRET_ACCESS_KEY\" = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n           \"AWS_S3_ENDPOINT\" = \"lakefs.mycorp.com:8000\")\n</code></pre> <p>Note: it is generally best practice to set these environment variables outside of the R script; it is done so here for convenience of the example.</p> <p>In conjunction with this you must also specify <code>region</code> and <code>use_https</code> in each call of an <code>aws.s3</code> function as these cannot be set globally. For example: </p> <pre><code>bucketlist(\n    region = \"\",\n    use_https = FALSE\n    )\n</code></pre> <ul> <li><code>region</code> should always be empty</li> <li><code>use_https</code> should be set to <code>TRUE</code> or <code>FALSE</code> depending on whether your lakeFS endpoint uses HTTPS.</li> </ul>"},{"location":"integrations/r/#listing-repositories","title":"Listing repositories","text":"<p>The S3 gateway exposes a repository as a bucket, and so using the <code>aws.s3</code> function <code>bucketlist</code> will return a list of available repositories on lakeFS: </p> <pre><code>bucketlist(\n    region = \"\",\n    use_https = FALSE\n    )\n</code></pre>"},{"location":"integrations/r/#writing-to-lakefs-from-r","title":"Writing to lakeFS from R","text":"<p>Assuming you're using the <code>aws.s3</code> library there various functions available including <code>s3save</code>, <code>s3saveRDS</code>, and <code>put_object</code>. Here's an example of writing an R object to lakeFS: </p> <pre><code>repo_name &lt;- \"example\"\nbranch &lt;- \"development\"\n\ns3saveRDS(x=my_df, \n          bucket = repo_name, \n          object = paste0(branch,\"/my_df.R\"), \n          region = \"\",\n          use_https = FALSE)\n</code></pre> <p>You can also upload local files to lakeFS using R and the <code>put_object</code> function: </p> <pre><code>repo_name &lt;- \"example\"\nbranch &lt;- \"development\"\nlocal_file &lt;- \"/tmp/never.gonna\"\n\nput_object(file = local_file, \n           bucket = repo_name, \n           object = paste0(branch,\"/give/you/up\"),\n           region = \"\",\n           use_https = FALSE)\n</code></pre>"},{"location":"integrations/r/#reading-from-lakefs-with-r","title":"Reading from lakeFS with R","text":"<p>As with writing data from R to lakeFS, there is a similar set of functions for reading data. These include <code>s3load</code>, <code>s3readRDS</code>, and <code>get_object</code>. Here's an example of reading an R object from lakeFS: </p> <pre><code>repo_name &lt;- \"example\"\nbranch &lt;- \"development\"\n\nmy_df &lt;- s3readRDS(bucket = repo_name, \n                   object = paste0(branch,\"/my_data.R\"),\n                   region = \"\",\n                   use_https = FALSE)\n</code></pre>"},{"location":"integrations/r/#listing-objects","title":"Listing Objects","text":"<p>In general you should always specify a branch prefix when listing objects. Here's an example to list the <code>main</code> branch in the <code>quickstart</code> repository: </p> <pre><code>get_bucket_df(bucket = \"quickstart\",\n              prefix = \"main/\",\n              region = \"\",\n              use_https = FALSE)\n</code></pre> <p>When listing objects in lakeFS there is a special case which is the repository/bucket level. When you list at this level you will get the branches returned as folders. These are not listed recursively, unless you list something under the branch. To understand more about this please refer to #5441</p>"},{"location":"integrations/r/#working-with-arrow","title":"Working with Arrow","text":"<p>Arrow's R library includes powerful support for data analysis, including reading and writing multiple file formats including Parquet, Arrow, CSV, and JSON. It has functionality for connecting to S3, and thus integrates perfectly with lakeFS. </p> <p>To start with install and load the library</p> <pre><code>install.packages(\"arrow\")\nlibrary(arrow)\n</code></pre> <p>Then create an S3FileSystem object to connect to your lakeFS instance</p> <pre><code>lakefs &lt;- S3FileSystem$create(\n    endpoint_override = \"lakefs.mycorp.com:8000\",\n    scheme = \"http\"\n    access_key = \"AKIAIOSFODNN7EXAMPLE\", \n    secret_key = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \n    region = \"\",\n)\n</code></pre> <p>From here you can list the contents of a particular lakeFS repository and branch</p> <pre><code>lakefs$ls(path = \"quickstart/main\")\n</code></pre> <p>To read a Parquet from lakeFS with R use the <code>read_parquet</code> function</p> <pre><code>lakes &lt;- read_parquet(lakefs$path(\"quickstart/main/lakes.parquet\"))\n</code></pre> <p>Writing a file follows a similar pattern. Here is rewriting the same file as above but in Arrow format</p> <pre><code>write_feather(x = lakes,\n              sink = lakefs$path(\"quickstart/main/lakes.arrow\"))\n</code></pre>"},{"location":"integrations/r/#performing-lakefs-operations-using-the-lakefs-api-from-r","title":"Performing lakeFS Operations using the lakeFS API from R","text":"<p>As well as reading and writing data, you will also want to carry out lakeFS operations from R including creating branches, committing data, and more. </p> <p>To do this call the lakeFS API from the <code>httr</code> library. You should refer to the API documentation for full details of the endpoints and their behaviour. Below are a few examples to illustrate the usage. </p>"},{"location":"integrations/r/#check-the-lakefs-server-version","title":"Check the lakeFS Server Version","text":"<p>This is a useful API call to establish connectivity and test authentication. </p> <pre><code>library(httr)\nlakefs_api_url &lt;- \"lakefs.mycorp.com:8000/api/v1\"\nlakefsAccessKey &lt;- \"AKIAIOSFODNN7EXAMPLE\"\nlakefsSecretKey &lt;- \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n\nr=GET(url=paste0(lakefs_api_url, \"/config/version\"), \n      authenticate(lakefsAccessKey, lakefsSecretKey))\n</code></pre> <p>The returned object <code>r</code> can be inspected to determine the outcome of the operation by comparing it to the status codes specified in the API. Here is some example R code to demonstrate the idea: </p> <pre><code>if (r$status_code == 200) {\n    print(paste0(\"\u2705 lakeFS credentials and connectivity verified. \u2139\ufe0f lakeFS version \",content(r)$version))   \n} else {\n    print(\"\ud83d\uded1 failed to get lakeFS version\")\n    print(content(r)$message)\n}\n</code></pre>"},{"location":"integrations/r/#create-a-repository","title":"Create a Repository","text":"<pre><code>library(httr)\nlakefs_api_url &lt;- \"lakefs.mycorp.com:8000/api/v1\"\nlakefsAccessKey &lt;- \"AKIAIOSFODNN7EXAMPLE\"\nlakefsSecretKey &lt;- \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\nrepo_name &lt;- \"my_new_repo\"\n\n# Define the payload\nbody=list(name=repo_name, \n          storage_namespace=\"s3://example-bucket/foo\")\n\n# Call the API\nr=POST(url=paste0(lakefs_api_url, \"/repositories\"), \n        authenticate(lakefsAccessKey, lakefsSecretKey),\n        body=body, encode=\"json\")\n</code></pre>"},{"location":"integrations/r/#commit-data","title":"Commit Data","text":"<pre><code>library(httr)\nlakefs_api_url &lt;- \"lakefs.mycorp.com:8000/api/v1\"\nlakefsAccessKey &lt;- \"AKIAIOSFODNN7EXAMPLE\"\nlakefsSecretKey &lt;- \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\nrepo_name &lt;- \"my_new_repo\"\nbranch &lt;- \"example\"\n\n# Define the payload\nbody=list(message=\"add some data and charts\", \n          metadata=list(\n              client=\"httr\", \n              author=\"rmoff\"))\n\n# Call the API\nr=POST(url=paste0(lakefs_api_url, \"/repositories/\", repo_name, \"/branches/\", branch, \"/commits\"), \n       authenticate(lakefsAccessKey, lakefsSecretKey),\n       body=body, encode=\"json\")\n</code></pre>"},{"location":"integrations/red_hat_openshift_ai/","title":"Using lakeFS with Red Hat OpenShift AI","text":"<p>Red Hat\u00ae OpenShift\u00ae is an enterprise-ready Kubernetes container platform with full-stack automated operations to manage hybrid cloud, multi-cloud, and edge deployments. OpenShift includes an enterprise-grade Linux operating system, container runtime, networking, monitoring, registry, and authentication and authorization solutions.</p> <p>Red Hat\u00ae OpenShift\u00ae AI is a flexible, scalable artificial intelligence (AI) and machine learning (ML) platform that enables enterprises to create and deliver AI-enabled applications at scale across hybrid cloud environments. Built using open source technologies, OpenShift AI provides trusted, operationally consistent capabilities for teams to experiment, serve models, and deliver innovative apps.</p> <p>OpenShift AI and lakeFS can be deployed in OpenShift cluster in 3 different architectures: 1. OpenShift AI, lakeFS and object storage are delpoyed in OpenShift cluster 2. OpenShift AI and lakeFS are deployed in OpenShift cluster while object storage is external 3. OpenShift AI is deployed in OpenShift cluster while lakeFS and object storage are external</p> <p></p> <p>Refer to an example in lakeFS-samples to deploy lakeFS, MinIO and OpenShift AI tutorial (Fraud Detection demo) in OpenShift cluster. Fraud detection demo is a step-by-step guide for using OpenShift AI to train an example model in JupyterLab, deploy the model, and refine the model by using automated pipelines.</p> <p>In this example, OpenShift AI is configured to connect over S3 interace to lakeFS, which will version the data in a backend MinIO instance. This is the architecture to run Fraud Detection demo with or without lakeFS:</p> <p></p> <p>lakeFS-samples also includes multiple Helm chart examples to deploy lakeFS and MinIO in different scenarios:</p> <ol> <li>lakefs-local.yaml: Bring up lakeFS using local object storage. This would be useful for a quick demo where MinIO is not included.</li> <li>lakefs-minio.yaml: Bring up lakeFS configured to use MinIO as backend object storage. This will be used in the lakeFS demo.</li> <li>minio-direct.yaml: This file would only be used if lakeFS is not in the picture and OpenShift AI will communicate directly with MinIO. It will bring up MinIO as it is in the default Fraud Detection demo, complete with configuring MinIO storage buckets and the OpenShift AI data connections. It may serve useful in debugging an issue.</li> <li>minio-via-lakefs.yaml: Bring up MinIO for the modified Fraud Detection demo that includes lakeFS, complete with configuring MinIO storage buckets, but do NOT configure the OpenShift AI data connections. This will be used in the lakeFS demo.</li> </ol>"},{"location":"integrations/sagemaker/","title":"Using lakeFS with Amazon SageMaker","text":"<p>Amazon SageMaker helps to prepare, build, train and deploy ML models quickly by bringing together a broad set of capabilities purpose-built for ML.</p>"},{"location":"integrations/sagemaker/#initializing-session-and-client","title":"Initializing session and client","text":"<p>Initialize a Sagemaker session and an S3 client with lakeFS as the endpoint:</p> <pre><code>import sagemaker\nimport boto3\n\nendpoint_url = '&lt;LAKEFS_ENDPOINT&gt;'\naws_access_key_id = '&lt;LAKEFS_ACCESS_KEY_ID&gt;'\naws_secret_access_key = '&lt;LAKEFS_SECRET_ACCESS_KEY&gt;'\nrepo = 'example-repo'\n\nsm = boto3.client('sagemaker',\n    endpoint_url=endpoint_url,\n    aws_access_key_id=aws_access_key_id,\n    aws_secret_access_key=aws_secret_access_key)\n\ns3_resource = boto3.resource('s3',\n    endpoint_url=endpoint_url,\n    aws_access_key_id=aws_access_key_id,\n    aws_secret_access_key=aws_secret_access_key)\n\nsession = sagemaker.Session(boto3.Session(), sagemaker_client=sm, default_bucket=repo)\nsession.s3_resource = s3_resource\n</code></pre>"},{"location":"integrations/sagemaker/#usage-examples","title":"Usage Examples","text":""},{"location":"integrations/sagemaker/#upload-train-and-test-data","title":"Upload train and test data","text":"<p>Let's use the created session for uploading data to the 'main' branch:</p> <pre><code>prefix = \"/prefix-within-branch\"\nbranch = 'main'\n\ntrain_file = 'train_data.csv';\ntrain_data.to_csv(train_file, index=False, header=True)\ntrain_data_s3_path = session.upload_data(path=train_file, key_prefix=branch + prefix + \"/train\")\n\ntest_file = 'test_data.csv';\ntest_data_no_target.to_csv(test_file, index=False, header=False)\ntest_data_s3_path = session.upload_data(path=test_file, key_prefix=branch + prefix + \"/test\")\n</code></pre>"},{"location":"integrations/sagemaker/#download-objects","title":"Download objects","text":"<p>You can use the integration with lakeFS to download a portion of the data you see fit:</p> <pre><code>repo = 'example-repo'\nprefix = \"/prefix-to-download\"\nbranch = 'main'\nlocalpath = './' + branch\n\nsession.download_data(path=localpath, bucket=repo, key_prefix = branch + prefix)\n</code></pre> <p>Note</p> <p>Advanced AWS SageMaker features, like Autopilot jobs, are encapsulated and don't have the option to override the S3 endpoint. However, it is possible to export the required inputs from lakeFS to S3.</p> <p>If you're using SageMaker features that aren't supported by lakeFS, we'd love to hear from you.</p>"},{"location":"integrations/spark/","title":"Using lakeFS with Apache Spark","text":"<p>There are several ways to use lakeFS with Spark:</p> <ul> <li>Recommended: using the lakeFS Iceberg REST Catalog: Read and write Iceberg tables using a standards-compliant catalog, built into lakeFS Enterprise.</li> <li>The lakeFS FileSystem: Direct data flow from client to storage, highly scalable. AWS S3<ul> <li>lakeFS FileSystem in Presigned mode: Best of both worlds. AWS S3Azure Blob</li> </ul> </li> <li>The S3-compatible API:  All Storage Vendors</li> </ul> Method Metadata operations Data operations Supported Data Formats Compatibility Iceberg REST Catalog \u2705 Table-level operations only \u2705 Direct I/O to underlying storage Apache Iceberg Tables \u2705 Any Spark environment capable of connecting to an Apache Iceberg REST Catalog (most) lakeFS FileSystem \u26a0\ufe0f Object-level operations require lakeFS API calls \u2705 Direct I/O to underlying storage All \u26a0\ufe0f Any Spark environment capable of loading user provided jar files (some) S3-compatible API N/A \ud83d\udea9 All data operations are proxied through lakeFS All \u2705 Any Spark environment capable of connecting to an S3-compatible API (most)"},{"location":"integrations/spark/#iceberg-rest-catalog","title":"Iceberg REST Catalog","text":"<p>lakeFS Iceberg REST Catalog allow you to use lakeFS as a spec-compliant Apache Iceberg REST catalog,  allowing Apache Spark to manage and access tables using a standard REST API.</p> <p></p> <p>Example</p> <pre><code>// Configure Spark to use the lakeFS REST catalog\nspark.sql(\"USE my_repo.main.inventory\")\n\n// List available tables\nspark.sql(\"SHOW TABLES\").show()\n\n// Query data with branch isolation\nspark.sql(\"SELECT * FROM books\").show()\n\n// Switch to a feature branch\nspark.sql(\"USE my_repo.new_branch.inventory\")\nspark.sql(\"SELECT * FROM books\").show()\n</code></pre> <p>In this mode, lakeFS stays completely outside the data path: data itself is read and written by Spark executors, directly to the underlying object store. Metadata is managed by Iceberg at the table level, while lakeFS keeps track of new snapshots to provide versioning and isolation.</p> <p>Read more about using the Iceberg REST Catalog.</p>"},{"location":"integrations/spark/#lakefs-hadoop-filesystem","title":"lakeFS Hadoop FileSystem","text":"<p>In this mode, Spark will read and write objects directly from the underlying object store, reducing the load on the lakeFS server. It will only access the lakeFS server for metadata operations, which works for most other data formats</p> <p>After configuring the lakeFS Hadoop FileSystem below, use URIs of the form <code>lakefs://example-repo/ref/path/to/data</code> to interact with your data on lakeFS.</p>"},{"location":"integrations/spark/#installation","title":"Installation","text":"Spark StandaloneDatabricksCloudera Spark <p>Add the package to your <code>spark-submit</code> command:</p> <pre><code>--packages io.lakefs:hadoop-lakefs-assembly:0.2.5\n</code></pre> <p>In  your cluster settings, under the Libraries tab, add the following Maven package:</p> <pre><code>io.lakefs:hadoop-lakefs-assembly:0.2.5\n</code></pre> <p>Once installed, it should look something like this:</p> <p></p> <p>Add the package to your <code>pyspark</code> or <code>spark-submit</code> command:</p> <pre><code>--packages io.lakefs:hadoop-lakefs-assembly:0.2.5\n</code></pre> <p>Add the configuration to access the S3 bucket used by lakeFS to your <code>pyspark</code> or <code>spark-submit</code> command or add this configuration at the Cloudera cluster level (see below):</p> <pre><code>--conf spark.yarn.access.hadoopFileSystems=s3a://bucket-name\n</code></pre> <p>Add the configuration to access the S3 bucket used by lakeFS at the Cloudera cluster level:</p> <ol> <li>Log in to the CDP (Cloudera Data Platform) web interface.</li> <li>From the CDP home screen, click the <code>Management Console</code> icon.</li> <li>In the Management Console, select <code>Data Hub Clusters</code> from the navigation pane.</li> <li>Select the cluster you want to configure. Click on <code>CM-UI</code> link under Services:     </li> <li> <p>In Cloudera Manager web interface, click on <code>Clusters</code> from the navigation pane and click on <code>spark_on_yarn</code> option:</p> <p></p> </li> <li> <p>Click on <code>Configuration</code> tab and search for <code>spark.yarn.access.hadoopFileSystems</code> in the search box:</p> <p></p> </li> <li> <p>Add S3 bucket used by lakeFS <code>s3a://bucket-name</code> in the <code>spark.yarn.access.hadoopFileSystems</code> list:</p> <p></p> </li> </ol>"},{"location":"integrations/spark/#configuration","title":"Configuration","text":"<p>Set the <code>fs.lakefs.*</code> Hadoop configurations to point to your lakeFS installation:</p> <ul> <li><code>fs.lakefs.impl</code>: <code>io.lakefs.LakeFSFileSystem</code></li> <li><code>fs.lakefs.access.key</code>: lakeFS access key</li> <li><code>fs.lakefs.secret.key</code>: lakeFS secret key</li> <li><code>fs.lakefs.endpoint</code>: lakeFS API URL (e.g. <code>https://example-org.us-east-1.lakefscloud.io/api/v1</code>)</li> </ul> <p>Configure the lakeFS client to use a temporary token instead of static credentials:</p> <ul> <li><code>fs.lakefs.auth.provider</code>: The default is <code>basic_auth</code> with <code>fs.lakefs.access.key</code> and <code>fs.lakefs.secret.key</code> for basic authentication.</li> </ul> <p>Can be set to <code>io.lakefs.auth.TemporaryAWSCredentialsLakeFSTokenProvider</code> for using temporary AWS credentials, you can read more about it here.</p> <p>When using <code>io.lakefs.auth.TemporaryAWSCredentialsLakeFSTokenProvider</code> as the auth provider the following configuration are relevant:</p> <ul> <li><code>fs.lakefs.token.aws.access.key</code>: AWS assumed role access key</li> <li><code>fs.lakefs.token.aws.secret.key</code>: AWS assumed role secret key</li> <li><code>fs.lakefs.token.aws.session.token</code>: AWS assumed role temporary session token</li> <li><code>fs.lakefs.token.aws.sts.endpoint</code>: AWS STS regional endpoint for generated the presigned-url (i.e <code>https://sts.us-west-2.amazonaws.com</code>)</li> <li><code>fs.lakefs.token.aws.sts.duration_seconds</code>: Optional, the duration in seconds for the initial identity token (default is 60)</li> <li><code>fs.lakefs.token.duration_seconds</code>: Optional, the duration in seconds for the lakeFS token (default is set in the lakeFS configuration auth.login_duration)</li> <li><code>fs.lakefs.token.sts.additional_headers</code>: Optional, comma separated list of <code>header:value</code> to attach when generating presigned sts request. Default is <code>X-Lakefs-Server-ID:fs.lakefs.endpoint</code>.</li> </ul> <p>Configure the S3A FileSystem to access your S3 storage, for example using the <code>fs.s3a.*</code> configurations (these are not your lakeFS credentials):</p> <ul> <li><code>fs.s3a.access.key</code>: AWS S3 access key</li> <li><code>fs.s3a.secret.key</code>: AWS S3 secret key</li> </ul> <p>Here are some configuration examples:</p> CLIScalaPySparkXML ConfigurationDatabricks <pre><code>spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAIOSFODNN7EXAMPLE' \\\n            --conf spark.hadoop.fs.s3a.secret.key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' \\\n            --conf spark.hadoop.fs.s3a.endpoint='https://s3.eu-central-1.amazonaws.com' \\\n            --conf spark.hadoop.fs.lakefs.impl=io.lakefs.LakeFSFileSystem \\\n            --conf spark.hadoop.fs.lakefs.access.key=AKIAlakefs12345EXAMPLE \\\n            --conf spark.hadoop.fs.lakefs.secret.key=abc/lakefs/1234567bPxRfiCYEXAMPLEKEY \\\n            --conf spark.hadoop.fs.lakefs.endpoint=https://example-org.us-east-1.lakefscloud.io/api/v1 \\\n            --packages io.lakefs:hadoop-lakefs-assembly:0.2.5 \\\n            io.example.ExampleClass\n</code></pre> <pre><code>spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://s3.eu-central-1.amazonaws.com\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\")\n</code></pre> <pre><code>sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"https://s3.eu-central-1.amazonaws.com\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\")\n</code></pre> <p>Make sure that you load the lakeFS FileSystem into Spark by running it with <code>--packages</code> or <code>--jars</code>, and then add these into a configuration file, e.g., <code>$SPARK_HOME/conf/hdfs-site.xml</code>:</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n            &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;\n            &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;\n        &lt;value&gt;https://s3.eu-central-1.amazonaws.com&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.impl&lt;/name&gt;\n        &lt;value&gt;io.lakefs.LakeFSFileSystem&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.secret.key&lt;/name&gt;\n        &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.endpoint&lt;/name&gt;\n        &lt;value&gt;https://example-org.us-east-1.lakefscloud.io/api/v1&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Add the following the cluster's configuration under <code>Configuration \u27a1\ufe0f Advanced options</code>:</p> <pre><code>spark.hadoop.fs.lakefs.impl io.lakefs.LakeFSFileSystem\nspark.hadoop.fs.lakefs.access.key AKIAlakefs12345EXAMPLE\nspark.hadoop.fs.lakefs.secret.key abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\nspark.hadoop.fs.s3a.access.key AKIAIOSFODNN7EXAMPLE\nspark.hadoop.fs.s3a.secret.key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nspark.hadoop.fs.s3a.impl shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.hadoop.fs.lakefs.endpoint https://example-org.us-east-1.lakefscloud.io/api/v1\n</code></pre> <p>Alternatively, follow this step by step Databricks integration tutorial, including lakeFS Hadoop File System, Python client and lakeFS SPARK client.</p> <p>Note</p> <p>If your bucket is on a region other than us-east-1, you may also need to configure <code>fs.s3a.endpoint</code> with the correct region. Amazon provides S3 endpoints you can use.</p>"},{"location":"integrations/spark/#usage-with-temporaryawscredentialslakefstokenprovider","title":"Usage with TemporaryAWSCredentialsLakeFSTokenProvider","text":"<p>An initial setup is required - you must have AWS Auth configured with lakeFS. The <code>TemporaryAWSCredentialsLakeFSTokenProvider</code> depends on the caller to provide AWS credentials (e.g Assumed Role Key,Secret and Token) as input to the lakeFS client.</p> <p>Warning</p> <p>Configure <code>sts.endpoint</code> with a valid sts regional service endpoint and it must be be equal to the region that is used for authentication first place. The only exception is <code>us-east-1</code> which is the default region for STS.</p> <p>Warning</p> <p>Using the current provider the lakeFS token will not renew upon expiry and the user will need to re-authenticate.</p> <p>PySpark example using <code>TemporaryAWSCredentialsLakeFSTokenProvider</code> with boto3 and AWS session credentials:</p> <pre><code>import boto3 \n\nsession = boto3.session.Session()\n\n# AWS credentials used s3a to access lakeFS bucket\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"https://s3.us-west-2.amazonaws.com\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.endpoint\", \"https://example-org.us-west-2.lakefscloud.io/api/v1\")\nsc._jsc.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.auth.provider\", \"io.lakefs.auth.TemporaryAWSCredentialsLakeFSTokenProvider\")\n# AWS tempporary session credentials to use with lakeFS\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.token.aws.access.key\", session.get_credentials().access_key)\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.token.aws.secret.key\", session.get_credentials().secret_key)\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.token.aws.session.token\", session.get_credentials().token)\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.token.aws.sts.endpoint\", \"https://sts.us-west-2.amazonaws.com\")\n</code></pre>"},{"location":"integrations/spark/#usage","title":"Usage","text":"<p>Hadoop FileSystem paths use the <code>lakefs://</code> protocol, with paths taking the form <code>lakefs://&lt;repository&gt;/&lt;ref&gt;/path/to/object</code>. <code>&lt;ref&gt;</code> can be a branch, tag, or commit ID in lakeFS. Here's an example for reading a Parquet file from lakeFS to a Spark DataFrame:</p> <pre><code>val repo = \"example-repo\"\nval branch = \"main\"\nval df = spark.read.parquet(s\"lakefs://${repo}/${branch}/example-path/example-file.parquet\")\n</code></pre> <p>Here's how to write some results back to a lakeFS path:</p> <pre><code>df.write.partitionBy(\"example-column\").parquet(s\"lakefs://${repo}/${branch}/output-path/\")\n</code></pre> <p>The data is now created in lakeFS as new changes in your branch. You can now commit these changes or revert them.</p>"},{"location":"integrations/spark/#hadoop-filesystem-in-presigned-mode","title":"Hadoop FileSystem in Presigned mode","text":"<p>Info</p> <p>Available starting version 0.1.13 of the FileSystem</p> <p>In this mode, the lakeFS server is responsible for authenticating with your storage. The client will still perform data operations directly on the storage. To do so, it will use pre-signed storage URLs provided by the lakeFS server.</p> <p>When using this mode, you don't need to configure the client with access to your storage:</p> CLIScalaPySparkXML ConfigurationDatabricks <pre><code>spark-shell --conf spark.hadoop.fs.lakefs.access.mode=presigned \\\n            --conf spark.hadoop.fs.lakefs.impl=io.lakefs.LakeFSFileSystem \\\n            --conf spark.hadoop.fs.lakefs.access.key=AKIAlakefs12345EXAMPLE \\\n            --conf spark.hadoop.fs.lakefs.secret.key=abc/lakefs/1234567bPxRfiCYEXAMPLEKEY \\\n            --conf spark.hadoop.fs.lakefs.endpoint=https://example-org.us-east-1.lakefscloud.io/api/v1 \\\n            --packages io.lakefs:hadoop-lakefs-assembly:0.2.5\n</code></pre> <pre><code>spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.mode\", \"presigned\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\")\n</code></pre> <pre><code>sc._jsc.hadoopConfiguration().set(\"fs.lakefs.access.mode\", \"presigned\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\")\n</code></pre> <p>Make sure that you load the lakeFS FileSystem into Spark by running it with <code>--packages</code> or <code>--jars</code>, and then add these into a configuration file, e.g., <code>$SPARK_HOME/conf/hdfs-site.xml</code>:</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.access.mode&lt;/name&gt;\n        &lt;value&gt;presigned&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.impl&lt;/name&gt;\n        &lt;value&gt;io.lakefs.LakeFSFileSystem&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.secret.key&lt;/name&gt;\n        &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.endpoint&lt;/name&gt;\n        &lt;value&gt;https://example-org.us-east-1.lakefscloud.io/api/v1&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Add the following the cluster's configuration under <code>Configuration \u27a1\ufe0f Advanced options</code>:</p> <pre><code>spark.hadoop.fs.lakefs.access.mode presigned\nspark.hadoop.fs.lakefs.impl io.lakefs.LakeFSFileSystem\nspark.hadoop.fs.lakefs.access.key AKIAlakefs12345EXAMPLE\nspark.hadoop.fs.lakefs.secret.key abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\nspark.hadoop.fs.lakefs.endpoint https://example-org.us-east-1.lakefscloud.io/api/v1\n</code></pre>"},{"location":"integrations/spark/#s3-compatible-api","title":"S3-compatible API","text":"<p>lakeFS has an S3-compatible endpoint which you can point Spark at to get started quickly.</p> <p>You will access your data using S3-style URIs, e.g. <code>s3a://example-repo/example-branch/example-table</code>.</p> <p>You can use the S3-compatible API regardless of where your data is hosted.</p>"},{"location":"integrations/spark/#configuration_1","title":"Configuration","text":"<p>To configure Spark to work with lakeFS, we set S3A Hadoop configuration to the lakeFS endpoint and credentials:</p> <ul> <li><code>fs.s3a.access.key</code>: lakeFS access key</li> <li><code>fs.s3a.secret.key</code>: lakeFS secret key</li> <li><code>fs.s3a.endpoint</code>: lakeFS S3-compatible API endpoint (e.g. https://example-org.us-east-1.lakefscloud.io)</li> <li><code>fs.s3a.path.style.access</code>: <code>true</code></li> </ul> <p>Here is how to do it:</p> CLIScalaXML ConfigurationEMR <pre><code>spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAlakefs12345EXAMPLE' \\\n            --conf spark.hadoop.fs.s3a.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\\n            --conf spark.hadoop.fs.s3a.path.style.access=true \\\n            --conf spark.hadoop.fs.s3a.endpoint='https://example-org.us-east-1.lakefscloud.io' ...\n</code></pre> <pre><code>spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAlakefs12345EXAMPLE\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://example-org.us-east-1.lakefscloud.io\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\")\n</code></pre> <p>Add these into a configuration file, e.g. <code>$SPARK_HOME/conf/hdfs-site.xml</code>:</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n            &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;\n            &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;\n        &lt;value&gt;https://example-org.us-east-1.lakefscloud.io&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Use the below configuration when creating the cluster. You may delete any app configuration that is not suitable for your use case:</p> <pre><code>[\n{\n    \"Classification\": \"spark-defaults\",\n    \"Properties\": {\n    \"spark.sql.catalogImplementation\": \"hive\"\n    }\n},\n{\n    \"Classification\": \"core-site\",\n    \"Properties\": {\n        \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.path.style.access\": \"true\",\n        \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"emrfs-site\",\n    \"Properties\": {\n        \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.path.style.access\": \"true\",\n        \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"presto-connector-hive\",\n    \"Properties\": {\n        \"hive.s3.aws-access-key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"hive.s3.aws-secret-key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"hive.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"hive.s3.path-style-access\": \"true\",\n        \"hive.s3-file-system-type\": \"PRESTO\"\n    }\n},\n{\n    \"Classification\": \"hive-site\",\n    \"Properties\": {\n        \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.path.style.access\": \"true\",\n        \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"hdfs-site\",\n    \"Properties\": {\n        \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.path.style.access\": \"true\",\n        \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"mapred-site\",\n    \"Properties\": {\n        \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.path.style.access\": \"true\",\n        \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.path.style.access\": \"true\"\n    }\n}\n]\n</code></pre> <p>Alternatively, you can pass these configuration values when adding a step.</p> <p>For example:</p> <pre><code>aws emr add-steps --cluster-id j-197B3AEGQ9XE4 \\\n--steps=\"Type=Spark,Name=SparkApplication,ActionOnFailure=CONTINUE, \\\nArgs=[--conf,spark.hadoop.fs.s3a.access.key=AKIAIOSFODNN7EXAMPLE, \\\n--conf,spark.hadoop.fs.s3a.secret.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY, \\\n--conf,spark.hadoop.fs.s3a.endpoint=https://example-org.us-east-1.lakefscloud.io, \\\n--conf,spark.hadoop.fs.s3a.path.style.access=true, \\\ns3a://&lt;lakefs-repo&gt;/&lt;lakefs-branch&gt;/path/to/jar]\"\n</code></pre>"},{"location":"integrations/spark/#per-bucket-configuration","title":"Per-bucket configuration","text":"<p>The above configuration will use lakeFS as the sole S3 endpoint. To use lakeFS in parallel with S3, you can configure Spark to use lakeFS only for specific bucket names. For example, to configure only <code>example-repo</code> to use lakeFS, set the following configurations:</p> CLIScalaXML ConfigurationEMR <pre><code>spark-shell --conf spark.hadoop.fs.s3a.bucket.example-repo.access.key='AKIAlakefs12345EXAMPLE' \\\n            --conf spark.hadoop.fs.s3a.bucket.example-repo.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\\n            --conf spark.hadoop.fs.s3a.bucket.example-repo.endpoint='https://example-org.us-east-1.lakefscloud.io' \\\n            --conf spark.hadoop.fs.s3a.path.style.access=true\n</code></pre> <pre><code>spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.access.key\", \"AKIAlakefs12345EXAMPLE\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.endpoint\", \"https://example-org.us-east-1.lakefscloud.io\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\")\n</code></pre> <p>Add these into a configuration file, e.g. <code>$SPARK_HOME/conf/hdfs-site.xml</code>:</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.bucket.example-repo.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.bucket.example-repo.secret.key&lt;/name&gt;\n        &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.bucket.example-repo.endpoint&lt;/name&gt;\n        &lt;value&gt;https://example-org.us-east-1.lakefscloud.io&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Use the below configuration when creating the cluster. You may delete any app configuration that is not suitable for your use case:</p> <pre><code>[\n{\n    \"Classification\": \"spark-defaults\",\n    \"Properties\": {\n    \"spark.sql.catalogImplementation\": \"hive\"\n    }\n},\n{\n    \"Classification\": \"core-site\",\n    \"Properties\": {\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.bucket.example-repo.path.style.access\": \"true\",\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.bucket.example-repo.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"emrfs-site\",\n    \"Properties\": {\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.bucket.example-repo.path.style.access\": \"true\",\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.bucket.example-repo.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"presto-connector-hive\",\n    \"Properties\": {\n        \"hive.s3.aws-access-key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"hive.s3.aws-secret-key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"hive.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"hive.s3.path-style-access\": \"true\",\n        \"hive.s3-file-system-type\": \"PRESTO\"\n    }\n},\n{\n    \"Classification\": \"hive-site\",\n    \"Properties\": {\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.bucket.example-repo.path.style.access\": \"true\",\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.bucket.example-repo.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"hdfs-site\",\n    \"Properties\": {\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.bucket.example-repo.path.style.access\": \"true\",\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.bucket.example-repo.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"mapred-site\",\n    \"Properties\": {\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.bucket.example-repo.path.style.access\": \"true\",\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.bucket.example-repo.path.style.access\": \"true\"\n    }\n}\n]\n</code></pre> <p>Alternatively, you can pass these configuration values when adding a step.</p> <p>For example:</p> <pre><code>aws emr add-steps --cluster-id j-197B3AEGQ9XE4 \\\n--steps=\"Type=Spark,Name=SparkApplication,ActionOnFailure=CONTINUE, \\\nArgs=[--conf,spark.hadoop.fs.s3a.bucket.example-repo.access.key=AKIAIOSFODNN7EXAMPLE, \\\n--conf,spark.hadoop.fs.s3a.bucket.example-repo.secret.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY, \\\n--conf,spark.hadoop.fs.s3a.bucket.example-repo.endpoint=https://example-org.us-east-1.lakefscloud.io, \\\n--conf,spark.hadoop.fs.s3a.path.style.access=true, \\\ns3a://&lt;lakefs-repo&gt;/&lt;lakefs-branch&gt;/path/to/jar]\"\n</code></pre> <p>With this configuration set, you read S3A paths with <code>example-repo</code> as the bucket will use lakeFS, while all other buckets will use AWS S3.</p>"},{"location":"integrations/spark/#usage_1","title":"Usage","text":"<p>Here's an example for reading a Parquet file from lakeFS to a Spark DataFrame:</p> <pre><code>val repo = \"example-repo\"\nval branch = \"main\"\nval df = spark.read.parquet(s\"s3a://${repo}/${branch}/example-path/example-file.parquet\")\n</code></pre> <p>Here's how to write some results back to a lakeFS path: <pre><code>df.write.partitionBy(\"example-column\").parquet(s\"s3a://${repo}/${branch}/output-path/\")\n</code></pre></p> <p>The data is now created in lakeFS as new changes in your branch. You can now commit these changes or revert them.</p>"},{"location":"integrations/spark/#configuring-azure-databricks-with-the-s3-compatible-api","title":"Configuring Azure Databricks with the S3-compatible API","text":"<p>If you use Azure Databricks, you can take advantage of the lakeFS S3-compatible API with your Azure account and the S3A FileSystem.  This will require installing the <code>hadoop-aws</code> package (with the same version as your <code>hadoop-azure</code> package) to your Databricks cluster.</p> <p>Define your FileSystem configurations in the following way:</p> <pre><code>spark.hadoop.fs.lakefs.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.hadoop.fs.lakefs.access.key=\u2018AKIAlakefs12345EXAMPLE\u2019                   // The access key to your lakeFS server\nspark.hadoop.fs.lakefs.secret.key=\u2018abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\u2019     // The secret key to your lakeFS server\nspark.hadoop.fs.lakefs.path.style.access=true\nspark.hadoop.fs.lakefs.endpoint=\u2018https://example-org.us-east-1.lakefscloud.io\u2019                 // The endpoint of your lakeFS server\n</code></pre> <p>For more details about Mounting cloud object storage on Databricks.</p>"},{"location":"integrations/spark/#configuring-databricks-sql-warehouse-with-the-s3-compatible-api","title":"Configuring Databricks SQL Warehouse with the S3-compatible API","text":"<p>A SQL warehouse is a compute resource that lets you run SQL commands on data  objects within Databricks SQL.</p> <p>If you use Databricks SQL warehouse, you can take advantage of the lakeFS  S3-compatible API with the S3A FileSystem. </p> <p>Define your SQL Warehouse configurations in the following way:</p> <ol> <li> <p>In the top right, select <code>Admin Settings</code> and then <code>Compute</code>, and <code>SQL warehouses</code>.</p> </li> <li> <p>Under <code>Data Access Properties</code> add the following key-value pairs for     each lakeFS repository you want to access:</p> </li> </ol> <pre><code>spark.hadoop.fs.s3a.impl shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.hadoop.fs.s3a.bucket.example-repo.access.key AKIAIOSFODNN7EXAMPLE // The access key to your lakeFS server\nspark.hadoop.fs.s3a.bucket.example-repo.secret.key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY // The secret key to your lakeFS server\nspark.hadoop.fs.s3a.bucket.example-repo.endpoint https://example-org.us-east-1.lakefscloud.io // The endpoint of your lakeFS server\nspark.hadoop.fs.s3a.bucket.example-repo.path.style.access true               \n</code></pre> <ol> <li>Changes are applied automatically after the SQL Warehouse restarts.</li> <li>You can now use the lakeFS S3-compatible API with your SQL Warehouse, e.g.:</li> </ol> <pre><code>SELECT * FROM delta.`s3a://example-repo/main/datasets/delta-table/` LIMIT 100\n</code></pre>"},{"location":"integrations/spark/#experimental-pre-signed-mode-for-s3a","title":"\u26a0\ufe0f Experimental: Pre-signed mode for S3A","text":"<p>In Hadoop 3.1.4 version and above (as tested using our lakeFS Hadoop FS), it is possible to use pre-signed URLs as return values from the lakeFS S3 Gateway.</p> <p>This has the immediate benefit of reducing the amount of traffic that has to go through the lakeFS server thus improving IO performance.  To read more about pre-signed URLs, see this guide.</p> <p>Here's an example Spark configuration to enable this support:</p> <pre><code>spark.hadoop.fs.s3a.impl shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.hadoop.fs.s3a.bucket.example-repo.access.key AKIAIOSFODNN7EXAMPLE // The access key to your lakeFS server\nspark.hadoop.fs.s3a.bucket.example-repo.secret.key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY // The secret key to your lakeFS server\nspark.hadoop.fs.s3a.path.style.access true\nspark.hadoop.fs.s3a.bucket.example-repo.signing-algorithm QueryStringSignerType\nspark.hadoop.fs.s3a.bucket.example-repo.user.agent.prefix s3RedirectionSupport\n</code></pre> <p>Note</p> <p><code>user.agent.prefix</code> should contain the string <code>s3RedirectionSupport</code> but does not have to match the string exactly.</p> <p>Once configured, requests will include the string <code>s3RedirectionSupport</code> in the <code>User-Agent</code> HTTP header sent with GetObject requests, resulting in lakeFS responding with a pre-signed URL. Setting the <code>signing-algorithm</code> to <code>QueryStringSignerType</code> is required to stop S3A from signing a pre-signed URL, since the existence of more than one signature method will return an error from S3.</p> <p>Info</p> <p>This feature requires a lakeFS server of version &gt;1.18.0</p>"},{"location":"integrations/unity-catalog/","title":"Using lakeFS with the Unity Catalog","text":""},{"location":"integrations/unity-catalog/#overview","title":"Overview","text":"<p>Databricks Unity Catalog serves as a centralized data governance platform for your data lakes. Through the Unity Catalog, you can search for and locate data assets across workspaces via a unified catalog. Leveraging the external tables feature within Unity Catalog, you can register a Delta Lake table exported from lakeFS and access it through the unified catalog. </p> <p>The subsequent step-by-step guide will lead you through the process of configuring a Lua hook that exports Delta Lake tables from lakeFS, and subsequently registers them in Unity Catalog.</p> <p>Note</p> <p>Currently, Unity Catalog export feature exclusively supports AWS S3 and Azure ADLS Gen2 as the underlying storage solution. It's planned to support other cloud providers soon.</p> <p>Reference Guide: lakeFS + Unity Catalog Integration: Step-by-Step Tutorial </p>"},{"location":"integrations/unity-catalog/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following:</p> <ol> <li>Access to Unity Catalog</li> <li>An active lakeFS installation with S3 as the backing storage, and a repository in this installation.</li> <li>A Databricks SQL warehouse.</li> <li>AWS Credentials with S3 access.</li> <li>lakeFS credentials with access to your Delta Tables.</li> </ol> <p>Info</p> <p>Supported from lakeFS v1.4.0</p>"},{"location":"integrations/unity-catalog/#databricks-authentication","title":"Databricks authentication","text":"<p>Given that the hook will ultimately register a table in Unity Catalog, authentication with Databricks is imperative. Make sure that:</p> <ol> <li>You have a Databricks Service Principal.</li> <li>The Service principal has token usage permissions,    and an associated token    configured.</li> <li>The service principal has the <code>Service principal: Manager</code> privilege over itself (Workspace: Admin console -&gt; Service principals -&gt; <code>&lt;service principal&gt;</code> -&gt; Permissions -&gt; Grant access (<code>&lt;service principal&gt;</code>:    <code>Service principal: Manager</code>), with <code>Workspace access</code> and <code>Databricks SQL access</code> checked (Admin console -&gt; Service principals -&gt; <code>&lt;service principal&gt;</code> -&gt; Configurations).</li> <li>Your SQL warehouse allows the service principal to use it (SQL Warehouses -&gt; <code>&lt;SQL warehouse&gt;</code> -&gt; Permissions -&gt; <code>&lt;service principal&gt;</code>: <code>Can use</code>).</li> <li>The catalog grants the <code>USE CATALOG</code>, <code>USE SCHEMA</code>, <code>CREATE SCHEMA</code> permissions to the service principal(Catalog -&gt; <code>&lt;catalog name&gt;</code> -&gt; Permissions -&gt; Grant -&gt; <code>&lt;service principal&gt;</code>: <code>USE CATALOG</code>, <code>USE SCHEMA</code>, <code>CREATE SCHEMA</code>).</li> <li>You have an External Location configured, and the service principal has the <code>CREATE EXTERNAL TABLE</code> permission over it (Catalog -&gt; External Data -&gt; External Locations -&gt; Create location).</li> </ol>"},{"location":"integrations/unity-catalog/#guide","title":"Guide","text":""},{"location":"integrations/unity-catalog/#table-descriptor-definition","title":"Table descriptor definition","text":"<p>To guide the Unity Catalog exporter in configuring the table in the catalog, define its properties in the Delta Lake table descriptor.  The table descriptor should include (at minimum) the following fields:</p> <ol> <li><code>name</code>: The table name.</li> <li><code>type</code>: Should be <code>delta</code>.</li> <li><code>catalog</code>: The name of the catalog in which the table will be created.</li> <li><code>path</code>: The path in lakeFS (starting from the root of the branch) in which the Delta Lake table's data is found.</li> </ol> <p>Let's define the table descriptor and upload it to lakeFS:</p> <p>Save the following as <code>famous-people-td.yaml</code>:</p> <pre><code>---\nname: famous_people\ntype: delta\ncatalog: my-catalog-name\npath: tables/famous-people\n</code></pre> <p>Tip</p> <p>It's recommended to create a Unity catalog with the same name as your repository</p> <p>Upload the table descriptor to <code>_lakefs_tables/famous-people-td.yaml</code> and commit:</p> <pre><code>lakectl fs upload lakefs://repo/main/_lakefs_tables/famous-people-td.yaml -s ./famous-people-td.yaml &amp;&amp; \\\nlakectl commit lakefs://repo/main -m \"add famous people table descriptor\"\n</code></pre>"},{"location":"integrations/unity-catalog/#write-some-data","title":"Write some data","text":"<p>Insert data into the table path, using your preferred method (e.g. Spark), and commit upon completion.</p> <p>We shall use Spark and lakeFS's S3 gateway to write some data as a Delta table:</p> <pre><code>pyspark --packages \"io.delta:delta-spark_2.12:3.0.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\" \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\\n  --conf spark.hadoop.fs.s3a.aws.credentials.provider='org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider' \\\n  --conf spark.hadoop.fs.s3a.endpoint='&lt;LAKEFS_SERVER_URL&gt;' \\\n  --conf spark.hadoop.fs.s3a.access.key='&lt;LAKEFS_ACCESS_KEY&gt;' \\\n  --conf spark.hadoop.fs.s3a.secret.key='&lt;LAKEFS_SECRET_ACCESS_KEY&gt;' \\\n  --conf spark.hadoop.fs.s3a.path.style.access=true\n</code></pre> <pre><code>data = [\n   ('James','Bond','England','intelligence'),\n   ('Robbie','Williams','England','music'),\n   ('Hulk','Hogan','USA','entertainment'),\n   ('Mister','T','USA','entertainment'),\n   ('Rafael','Nadal','Spain','professional athlete'),\n   ('Paul','Haver','Belgium','music'),\n]\ncolumns = [\"firstname\",\"lastname\",\"country\",\"category\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"category\", \"country\").save(\"s3a://repo/main/tables/famous-people\")\n</code></pre>"},{"location":"integrations/unity-catalog/#the-unity-catalog-exporter-script","title":"The Unity Catalog exporter script","text":"<p>Example</p> <p>For code references check delta_exporter and  unity_exporter docs.</p> <p>Create <code>unity_exporter.lua</code>:</p> <pre><code>local aws = require(\"aws\")\nlocal formats = require(\"formats\")\nlocal databricks = require(\"databricks\")\nlocal delta_export = require(\"lakefs/catalogexport/delta_exporter\")\nlocal unity_export = require(\"lakefs/catalogexport/unity_exporter\")\n\nlocal sc = aws.s3_client(args.aws.access_key_id, args.aws.secret_access_key, args.aws.region)\n\n-- Export Delta Lake tables export:\nlocal delta_client = formats.delta_client(args.lakefs.access_key_id, args.lakefs.secret_access_key, args.aws.region)\nlocal delta_table_details = delta_export.export_delta_log(action, args.table_defs, sc.put_object, delta_client, \"_lakefs_tables\")\n\n-- Register the exported table in Unity Catalog:\nlocal databricks_client = databricks.client(args.databricks_host, args.databricks_token)\nlocal registration_statuses = unity_export.register_tables(action, \"_lakefs_tables\", delta_table_details, databricks_client, args.warehouse_id)\n\nfor t, status in pairs(registration_statuses) do\n    print(\"Unity catalog registration for table \\\"\" .. t .. \"\\\" completed with commit schema status : \" .. status .. \"\\n\")\nend\n</code></pre> <p>Upload the lua script to the <code>main</code> branch under <code>scripts/unity_exporter.lua</code> and commit:</p> <pre><code>lakectl fs upload lakefs://repo/main/scripts/unity_exporter.lua -s ./unity_exporter.lua &amp;&amp; \\\nlakectl commit lakefs://repo/main -m \"upload unity exporter script\"\n</code></pre>"},{"location":"integrations/unity-catalog/#action-configuration","title":"Action configuration","text":"<p>Define an action configuration that will run the above script after a commit is completed (<code>post-commit</code>) over the <code>main</code> branch.</p> <p>Create <code>unity_exports_action.yaml</code>:</p> <pre><code>---\nname: unity_exports\non:\n  post-commit:\n     branches: [\"main\"]\nhooks:\n  - id: unity_export\n    type: lua\n    properties:\n      script_path: scripts/unity_exporter.lua\n      args:\n        aws:\n          access_key_id: &lt;AWS_ACCESS_KEY_ID&gt;\n          secret_access_key: &lt;AWS_SECRET_ACCESS_KEY&gt;\n          region: &lt;AWS_REGION&gt;\n        lakefs: # provide credentials of a user that has access to the script and Delta Table\n          access_key_id: &lt;LAKEFS_ACCESS_KEY_ID&gt; \n          secret_access_key: &lt;LAKEFS_SECRET_ACCESS_KEY&gt;\n        table_defs: # an array of table descriptors used to be defined in Unity Catalog\n          - famous-people-td\n        databricks_host: &lt;DATABRICKS_HOST_URL&gt;\n        databricks_token: &lt;DATABRICKS_SERVICE_PRINCIPAL_TOKEN&gt;\n        warehouse_id: &lt;WAREHOUSE_ID&gt;\n</code></pre> <p>Upload the action configurations to <code>_lakefs_actions/unity_exports_action.yaml</code> and commit:</p> <p>Note</p> <p>Once the commit will finish its run, the action will start running since we've configured it to run on <code>post-commit</code>  events on the <code>main</code> branch.</p> <pre><code>lakectl fs upload lakefs://repo/main/_lakefs_actions/unity_exports_action.yaml -s ./unity_exports_action.yaml &amp;&amp; \\\nlakectl commit lakefs://repo/main -m \"upload action and run it\"\n</code></pre> <p>The action has run and exported the <code>famous_people</code> Delta Lake table to the repo's storage namespace, and has register  the table as an external table in Unity Catalog under the catalog <code>my-catalog-name</code>, schema <code>main</code> (as the branch's name) and  table name <code>famous_people</code>: <code>my-catalog-name.main.famous_people</code>.</p> <p></p>"},{"location":"integrations/unity-catalog/#databricks-integration","title":"Databricks Integration","text":"<p>After registering the table in Unity, you can leverage your preferred method to query the data  from the exported table under <code>my-catalog-name.main.famous_people</code>, and view it in the Databricks's Catalog Explorer, or retrieve it using the Databricks CLI with the following command: </p> <pre><code>databricks tables get my-catalog-name.main.famous_people\n</code></pre> <p></p>"},{"location":"integrations/vertex_ai/","title":"Using Vertex AI with lakeFS","text":"<p>Vertex AI lets Google Cloud users Build, deploy, and scale machine learning (ML) models faster, with fully managed ML tools for any use case.</p> <p>lakeFS Works with Vertex AI by allowing users to create repositories on GCS Buckets, then use either the Dataset API to create managed Datasets on top of lakeFS version, or by automatically exporting lakeFS object versions in a way readable by Cloud Storage Mounts. </p>"},{"location":"integrations/vertex_ai/#using-lakefs-with-vertex-managed-datasets","title":"Using lakeFS with Vertex Managed Datasets","text":"<p>Vertex's ImageDataset and VideoDataset allow creating a dataset by importing a CSV file from gcs (see <code>gcs_source</code>).</p> <p>This CSV file contains GCS addresses of image files and their corresponding labels.</p> <p>Since the lakeFS API supports exporting the underlying GCS address of versioned objects, we can generate such a CSV file when creating the dataset:  </p> <pre><code>#!/usr/bin/env python\n\n# Requirements:\n# google-cloud-aiplatform&gt;=1.31.0\n# lakefs&gt;=1.0.0\n\nimport csv\nfrom pathlib import PosixPath\nfrom io import StringIO\n\nimport lakefs\nfrom google.cloud import storage\nfrom google.cloud import aiplatform\n\n# Dataset configuration\nlakefs_repo = 'my-repository'\nlakefs_ref = 'main'\nimg_dataset = 'datasets/my-images/'\n\n# Vertex configuration\nimport_bucket = 'underlying-gcs-bucket'\n\n# produce import file for Vertex's SDK\nbuf = StringIO()\ncsv_writer = csv.writer(buf)\nfor obj in lakefs.repository(lakefs_repo).ref(lakefs_ref).objects(prefix=img_dataset):\n    p = PosixPath(obj.path)\n    csv_writer.writerow((obj.physical_address, p.parent.name))\n\n# spit out CSV\nprint('Generated path and labels CSV')\nbuf.seek(0)\n\n# Write it to storage\nstorage_client = storage.Client()\nbucket = storage_client.bucket(import_bucket)\nblob = bucket.blob(f'vertex/imports/{lakefs_repo}/{lakefs_ref}/labels.csv')\nwith blob.open('w') as out:\n    out.write(buf.read())\n\nprint(f'Wrote CSV to gs://{import_bucket}/vertex/imports/{lakefs_repo}/{lakefs_ref}/labels.csv')\n\n# import in Vertex, as dataset\nprint('Importing dataset...')\nds = aiplatform.ImageDataset.create(\n    display_name=f'{lakefs_repo}_{lakefs_ref}_imgs',\n    gcs_source=f'gs://{import_bucket}/vertex/imports/{lakefs_repo}/{lakefs_ref}/labels.csv',\n    import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n    sync=True\n)\nds.wait()\nprint(f'Done! {ds.display_name} ({ds.resource_name})')\n</code></pre>"},{"location":"integrations/vertex_ai/#using-lakefs-with-cloud-storage-fuse","title":"Using lakeFS with Cloud Storage Fuse","text":"<p>Vertex allows using Google Cloud Storage mounted as a Fuse Filesystem as custom input for training jobs.</p> <p>Instead of having to copy lakeFS files for each version we want to consume, we can create symlinks by using gcsfuse's native symlink inodes.</p> <p>This process can be fully automated by using the example gcsfuse_symlink_exporter.lua Lua hook.</p> <p>Here's what we need to do:</p> <ol> <li>Upload the example <code>.lua</code> file into our lakeFS repository. For this example, we'll put it under <code>scripts/gcsfuse_symlink_exporter.lua</code>.</li> <li>Create a new hook definition file and upload to <code>_lakefs_actions/export_images.yaml</code>:</li> </ol> <pre><code>---\n# Example hook declaration: (_lakefs_actions/export_images.yaml):\nname: export_images\n\non:\n  post-commit:\n    branches: [\"main\"]\n  post-merge:\n    branches: [\"main\"]\n  post-create-tag:\n\nhooks:\n- id: gcsfuse_export_images\n  type: lua\n  properties:\n    script_path: scripts/export_gcs_fuse.lua  # Path to the script we uploaded in the previous step\n    args:\n      prefix: \"datasets/images/\"  # Path we want to export every commit\n      destination: \"gs://my-bucket/exports/my-repo/\"  # Where should we create the symlinks?\n      mount:\n        from: \"gs://my-bucket/repos/my-repo/\"  # Symlinks are to a unix-mounted file\n        to: \"/gcs/my-bucket/repos/my-repo/\"    #  This will ensure they point to a location that exists.\n\n      # Should be the contents of a valid credentials.json file\n      # See: https://developers.google.com/workspace/guides/create-credentials\n      # Will be used to write the symlink files\n      gcs_credentials_json_string: |\n        {\n          \"client_id\": \"...\",\n          \"client_secret\": \"...\",\n          \"refresh_token\": \"...\",\n          \"type\": \"...\"\n        }\n</code></pre> <p>Done! On the next tag creation or update to the <code>main</code> branch, we'll automatically export the lakeFS version of <code>datasets/images/</code> to a mountable location.</p> <p>To consume the symlink-ed files, we can read them normally from the mount:</p> <pre><code>with open('/gcs/my-bucket/exports/my-repo/branches/main/datasets/images/001.jpg') as f:\n    image_data = f.read()\n</code></pre> <p>Previously exported commits are also readable, if we exported them in the past:</p> <pre><code>commit_id = 'abcdef123deadbeef567'\nwith open(f'/gcs/my-bucket/exports/my-repo/commits/{commit_id}/datasets/images/001.jpg') as f:\n    image_data = f.read()\n</code></pre>"},{"location":"integrations/vertex_ai/#considerations-when-using-lakefs-with-cloud-storage-fuse","title":"Considerations when using lakeFS with Cloud Storage Fuse","text":"<p>For lakeFS paths to be readable by gcsfuse, the mount option <code>--implicit-dirs</code> must be specified.</p>"},{"location":"project/","title":"The lakeFS Project","text":"<p>lakeFS provides version control over the data lake and lakehouse, and uses Git-like semantics to create and access those versions. If you know git, you\u2019ll be right at home with lakeFS.</p> <p>lakeFS is an open-source project under the Apache 2.0 license.</p> <p>The project was created and is supported by Treeverse, a commercial company founded by engineers passionate about providing solutions to the evolving world of data engineering.</p>"},{"location":"project/#our-commitment-to-open-source","title":"Our commitment to open source","text":"<p>lakeFS is an open-source project under the Apache 2.0 license. The project was created and is supported by Treeverse, a commercial company founded by engineers passionate about providing solutions to the evolving world of data engineering.</p> <p>Why did we choose to open the source of our core capabilities?</p> <p>We believe in the bottom-up adoption of technologies. We believe collaborative communities have the power to bring the best solutions to the community. We believe that every engineer should be able to use, contribute to, and influence cutting edge technologies, so they can innovate in their domain.</p> <p>What is our commitment to open source?</p> <p>We created lakeFS, our open-source project, to provide a Git-like interface on top of object stores - so that you can fully take advantage of with any data application at any scale.</p> <p>For that reason, we commit that the following capabilities are and will remain open-source as part of lakeFS:</p> <ul> <li>All versioning capabilities,</li> <li>Git-Like interface for the versioning operations,</li> <li>Support for public object store APIs,</li> <li>Integrations with publicly available applications accessing an object store,</li> <li>CLI, API, and GUI interfaces.</li> </ul> <p>We also commit to keeping lakeFS scalable in throughput and performance.</p> <p>We are deeply committed to our community of engineers who use and contribute to the project. We are and will continue to be highly responsive and shape lakeFS together to provide the data versioning capabilities we are all looking for.</p> <p>What is lakeFS Cloud?</p> <p>Treeverse offers lakeFS Cloud, which provides all the same benefits of the Git-like interface on top of object stores as a fully-managed service.</p> <p>The vision behind lakeFS Cloud is to provide a managed data versioning and management solution for data practitioners. lakeFS Cloud will leverage the lakeFS open-source technology, integrate capabilities and unique features, and lead its users to implement best practices.</p> <p>As part of our commitment to the open source values of transparency and interoperability, we believe everyone should be able to enjoy these benefits, regardless of whether or not they choose to use the managed offering. </p> <p>Because of that, we will not intentionally make it harder to build these features independently on top of the open source solution.</p>"},{"location":"project/code-migrate-1.0-sdk/","title":"lakeFS 1.0 - Code Migration Guide","text":"<p>Version 1.0.0 promises API and SDK stability. By \"API\" we mean any access to a lakeFS REST endpoint. By \"SDK\" we mean auto-generated lakeFS clients: <code>lakefs-sdk</code> for Python and <code>io.lakefs:sdk</code> for Java. This guide details the steps to allow you to upgrade your code to enjoy this stability.</p> <p>Avoid using APIs and SDKs labeled as <code>experimental</code>, <code>internal</code>, or <code>deprecated</code>. If you must use them, be prepared to adjust your application to align with any lakeFS server updates.</p> <p>Your software developed without such APIs should be compatible with all minor version updates of the lakeFS server from the version you originally developed with.</p> <p>If you rely on a publicly released API and SDK, it will adhere to semantic versioning. Transitioning your application to a minor SDK version update should be smooth.</p> <p>The operation names and tags from the <code>api/swagger.yml</code> specification might differ based on the SDK or coding language in use.</p>"},{"location":"project/code-migrate-1.0-sdk/#deleted-api-operations","title":"Deleted API Operations","text":"<p>The following API operations have been removed:</p> <ul> <li><code>updatePassword</code></li> <li><code>forgotPassword</code></li> <li><code>logBranchCommits</code></li> <li><code>expandTemplate</code></li> <li><code>createMetaRange</code></li> <li><code>ingestRange</code></li> <li><code>updateBranchToken</code></li> </ul>"},{"location":"project/code-migrate-1.0-sdk/#internal-api-operations","title":"Internal API Operations","text":"<p>The following operations are for <code>internal</code> use only and should not be used in your application code. Some deprecated operations have alternatives provided.</p> <ul> <li><code>setupCommPrefs</code></li> <li><code>getSetupState</code></li> <li><code>setup</code></li> <li><code>getAuthCapabilities</code></li> <li><code>uploadObjectPreflight</code></li> <li><code>setGarbageCollectionRulesPreflight</code></li> <li><code>createBranchProtectionRulePreflight</code></li> <li><code>postStatsEvents</code></li> <li><code>dumpRefs</code> (will be replaced with a long-running API later)</li> <li><code>restoreRefs</code> (will be replaced with a long-running API later)</li> <li><code>createSymlinkFile</code> (Deprecated)</li> <li><code>getStorageConfig</code> (Deprecated. Alternative: <code>getConfig</code>)</li> <li><code>getLakeFSVersion</code> (Deprecated. Alternative: <code>getConfig</code>)</li> <li><code>stageObject</code> (Deprecated. Alternatives: <code>get/link physical address</code> or <code>import</code>)</li> <li><code>internalDeleteBranchProtectionRule</code> (Deprecated. Temporary backward support. Alternative: <code>setBranchProtectionRules</code>)</li> <li><code>internalCreateBranchProtectionRule</code> (Deprecated. Temporary backward support. Alternative: <code>setBranchProtectionRules</code>)</li> <li><code>internalGetBranchProtectionRule</code> (Deprecated. Temporary backward support. Alternative: <code>getBranchProtectionRules</code>)</li> <li><code>internalDeleteGarbageCollectionRules</code> (Deprecated. Temporary backward support. Alternative: <code>deleteGCRules</code>)</li> <li><code>internalSetGarbageCollectionRules</code> (Deprecated. Temporary backward support. Alternative: <code>setGCRules</code>)</li> <li><code>internalGetGarbageCollectionRules</code> (Deprecated. Temporary backward support. Alternative: <code>getGCRules</code>)</li> <li><code>prepareGarbageCollectionCommits</code></li> <li><code>getGarbageCollectionConfig</code></li> </ul>"},{"location":"project/code-migrate-1.0-sdk/#newupdated-api-operations","title":"New/Updated API Operations","text":"<p>Here are the newly added or updated operations:</p> <ul> <li><code>getConfig</code> (Retrieve lakeFS version and storage info)</li> <li><code>setBranchProtectionRules</code> (Route updated)</li> <li><code>getBranchProtectionRules</code> (Route updated)</li> <li><code>getGCRules</code> (New route introduced)</li> <li><code>setGCRules</code> (New route introduced)</li> <li><code>deleteGCRules</code> (New route introduced)</li> <li><code>importStatus</code> (Response structure updated: 'ImportStatusResp' to 'ImportStatus')</li> <li><code>uploadObject</code> (Parameters 'if-none-match' and 'storageClass' are now deprecated)</li> <li><code>prepareGarbageCollectionCommits</code> (Request body removed)</li> <li><code>getOtfDiffs</code> &amp; <code>otfDiff</code> (Removed from 'otf diff' tag; retained in 'experimental' tag)</li> </ul>"},{"location":"project/code-migrate-1.0-sdk/#migrating-sdk-code-for-java-and-jvm-based-languages","title":"Migrating SDK Code for Java and JVM-based Languages","text":""},{"location":"project/code-migrate-1.0-sdk/#introduction","title":"Introduction","text":"<p>If you are using the lakeFS client for Java or for any other JVM-based language, be aware that the current package is not stable with respect to minor version upgrades. Transitioning from <code>io.lakefs:lakefs-client</code> to <code>io.lakefs:sdk</code> will necessitate rewriting your API calls to fit the new design paradigm.</p>"},{"location":"project/code-migrate-1.0-sdk/#problem-with-the-old-style","title":"Problem with the Old Style","text":"<p>Previously, API calls required developers to pass all parameters, including optional ones, in a single function call. As demonstrated in this older style:</p> <pre><code>ObjectStats objectStat = objectsApi.statObject(\n    objectLoc.getRepository(), objectLoc.getRef(), objectLoc.getPath(),\n    false, false);\n</code></pre> <p>This method posed a couple of challenges:</p> <ol> <li>Inflexibility with Upgrades: If an optional parameter were introduced in newer versions, existing code would fail to compile.</li> <li>Maintenance Difficulty: Long argument lists can be challenging to manage and understand, leading to potential mistakes and readability issues.</li> </ol> <p>Adopting the Fluent Style</p> <p>In the revised SDK, API calls adopt a fluent style, making the code more modular and adaptive to changes.</p> <p>Here's an example of the new style:</p> <pre><code>ObjectStats objectStat = objectsApi\n    .statObject(\n        objectLoc.getRepository(), objectLoc.getRef(), objectLoc.getPath()\n    )\n    .userMetadata(true)\n    .execute();\n</code></pre>"},{"location":"project/code-migrate-1.0-sdk/#heres-a-breakdown-of-the-changes","title":"Here's a breakdown of the changes:","text":"<ol> <li>Initial Function Call: Begin by invoking the desired function with all required parameters.</li> <li>Modifying Optional Parameters: Chain any modifications to optional parameters after the initial function. For instance, <code>userMetadata</code> is changed in the example above.</li> <li>Unused Optional Parameters: You can safely ignore these. For instance, this code ignores the <code>presign</code> optional parameter because it never uses it.</li> <li>Execution: Complete the call with the <code>.execute()</code> method.</li> </ol> <p>This new design offers several advantages:</p> <ul> <li>Compatibility with Upgrades: When a new optional parameter is introduced, existing code will use its default value, preserving compatibility with minor server version upgrades.</li> <li>Improved Readability: The fluent style makes it evident which parameters are required and which ones are optional, enhancing code clarity.</li> </ul> <p>When migrating your code, ensure you refactor all your API calls to adopt the new fluent style. This ensures that your application remains maintainable and is safeguarded against potential issues arising from minor SDK version upgrades.</p> <p>For an illustrative example of the transition between styles, you can view the changes made in this pull request: lakeFS pull request #6529.</p>"},{"location":"project/code-migrate-1.0-sdk/#migrating-sdk-code-for-python","title":"Migrating SDK Code for Python","text":""},{"location":"project/code-migrate-1.0-sdk/#introduction_1","title":"Introduction","text":"<p>If you continue using the Python <code>lakefs-client</code> package for lakeFS, it's important to note that the package has reached its end of support with minor version updates. You need to switch from <code>lakefs-client</code> to <code>lakefs-sdk</code>, which will require rewriting your API calls.</p>"},{"location":"project/code-migrate-1.0-sdk/#heres-a-breakdown-of-the-changes_1","title":"Here's a breakdown of the changes:","text":"<ol> <li>Modules change</li> <li>The previous <code>model</code> module was renamed to <code>models</code>, meaning that <code>lakefs_client.model</code> imports should be replaced with <code>lakefs_sdk.models</code> imports.</li> <li>The <code>apis</code> module in <code>lakefs_client</code> is deprecated and no longer supported. To migrate to the new <code>api</code> module in <code>lakefs_sdk</code>, you should replace all imports of <code>lakefs_client.apis</code> with imports of <code>lakefs_sdk.api</code>. We still recommend using the <code>lakefs_sdk.LakeFSClient</code> class instead of using the <code>api</code> module directly. The <code>LakeFSClient</code> class provides a higher-level interface to the LakeFS API and makes it easier to use LakeFS in your applications.</li> <li><code>upload_object</code> API call: The <code>content</code> parameter value passed to the <code>objects_api.upload_object</code> method call should be either a <code>string</code> containing the path to the uploaded file, or <code>bytes</code> of data to be uploaded.</li> <li><code>get_object</code> API call: The return value of <code>client.get_object(...)</code> is a <code>bytearray</code> containing the content of the object.</li> <li><code>**client.{operation}_api**</code>: The <code>lakefs-client</code> package\u2019s <code>LakeFSClient</code> class\u2019s deprecation-marked operations (<code>client.{operation}</code>) will no longer be available in the <code>lakefs-sdk</code> package\u2019s <code>LakeFSClient</code> class. In their place, the <code>client.{operation}_api</code> should be used.</li> <li>Minimum Python Version: 3.7</li> <li>Fetching results from response objects: Instead of fetching the required results properties from a dictionary using <code>response_result.get_property(prop_name)</code>, the response objects will include domain specific entities, thus referring to the properties in the <code>results</code> of the response - <code>response_result.prop_name</code>.</li> </ol> <p>For example, instead of:</p> <pre><code>response = lakefs_client.branches.diff_branch(repository='repo', branch='main')\ndiff = response.results[0] # 'results' is a 'DiffList' object\npath = diff.get_property('path') # 'diff' is a dictionary\n</code></pre> <p>You should use:</p> <pre><code>response = lakefs_client.branches_api.diff_branch(repository='repo', branch='main')\ndiff = response.results[0] # 'results' is a 'DiffList' object\npath = diff.path # 'diff' is a 'Diff' object\n</code></pre>"},{"location":"project/contributing/","title":"Contributing to lakeFS","text":"<p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure that we have all the necessary information to effectively respond to your bug report or contribution.</p> <p>Don't know where to start?</p> <p>Reach out on the #dev channel on our Slack and we will help you get started. We also recommend this free series about contributing to OSS projects.</p>"},{"location":"project/contributing/#getting-started","title":"Getting Started","text":"<p>Before you get started, we kindly ask that you:</p> <ul> <li>Check out the code of conduct.</li> <li>Sign the lakeFS CLA when making your first pull request (individual / corporate)</li> <li>Submit any security issues directly to security@treeverse.io.</li> <li>Contributions should have an associated GitHub issue. </li> <li>Before making major contributions, please reach out to us on the #dev channel on Slack.   We will make sure no one else is working on the same feature. </li> </ul>"},{"location":"project/contributing/#setting-up-an-environment","title":"Setting up an Environment","text":"<p>This section was tested on macOS and Linux (Fedora 32, Ubuntu 20.04) - Your mileage may vary</p> <p>Our Go release workflow holds the Go and Node.js versions we currently use under go-version and node-version compatibly. The Java workflows use Maven 3.8.x (but any recent version of Maven should work).</p> <ol> <li>Install the required dependencies for your OS:<ol> <li>Git</li> <li>GNU make (probably best to install from your OS package manager such as apt or brew)</li> <li>Docker</li> <li>Go</li> <li>Node.js &amp; npm</li> <li>Java 8<ul> <li>Apple Silicon Mac users can install this from Azul Zulu Builds for Java JDK (up to version 17). Builds for Intel-based Macs are available from java.com.</li> </ul> </li> <li>Maven <ul> <li>Required for building and testing Spark client code, as well as the hadoopfs client.</li> </ul> </li> <li>Optional - PostgreSQL 11 (useful for running and debugging locally)</li> <li>Optional - Rust &amp; Cargo (useful for building the Rust SDK)</li> <li>Optional - Buf CLI (only needed if you like to update Protocol Buffer files)</li> </ol> </li> <li> <p>Clone the repository from GitHub.</p> <p>This gives you read-only access to the repository. To contribute, see the next section.</p> </li> <li> <p>Build the project:     <pre><code>make build\n</code></pre></p> <p>Warning</p> <p><code>make build</code> won't work for Windows users.</p> </li> <li> <p>Make sure tests are passing. The following should not return any errors:     <pre><code>make test\n</code></pre></p> </li> </ol>"},{"location":"project/contributing/#before-creating-a-pull-request","title":"Before creating a pull request","text":"<ol> <li>Review this document in full.</li> <li>Make sure there's an open issue on GitHub that this pull request addresses, and that it isn't labeled <code>x/wontfix</code>.</li> <li>Fork the lakeFS repository.</li> <li>If you're adding new functionality, create a new branch named <code>feature/&lt;DESCRIPTIVE NAME&gt;</code>.</li> <li>If you're fixing a bug, create a new branch named <code>fix/&lt;DESCRIPTIVE NAME&gt;-&lt;ISSUE NUMBER&gt;</code>.</li> </ol>"},{"location":"project/contributing/#testing-your-change","title":"Testing your change","text":"<p>Once you've made the necessary changes to the code, make sure the tests pass:</p> <p>Run unit tests:</p> <pre><code>make test\n</code></pre> <p>Check that linting rules are passing. </p> <pre><code>make checks-validator\n</code></pre> <p>Note</p> <p>You will need GNU diff to run this. On the macOS it can be installed with <code>brew install diffutils</code></p> <p>lakeFS uses go fmt as a style guide for Go code.</p> <p>Run system-tests:</p> <pre><code>make system-tests\n</code></pre> <p>Info</p> <p>Want to dive deeper into our system tests infrastructure? Need to debug the tests? Follow this documentation.</p>"},{"location":"project/contributing/#submitting-a-pull-request","title":"Submitting a pull request","text":"<p>Open a GitHub pull request with your change. The PR description should include a brief explanation of your change. You should also mention the related GitHub issue. If the issue should be automatically closed after the merge, please link it to the PR.</p> <p>After submitting your pull request, GitHub Actions will automatically run tests on your changes and make sure that your updated code builds and runs on Go 1.19.x.</p> <p>Check back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors.</p> <p>A developer from our team will review your pull request, and may request some changes to it. After the request is approved, it will be merged to our main branch.</p>"},{"location":"project/contributing/#documentation","title":"Documentation","text":"<p>Any contribution to the docs, whether it is in conjunction with a code contribution or as a standalone, is appreciated.</p> <p>Documentation of features and changes in behaviour should be included in the pull request. You can create separate pull requests for documentation changes only.</p> <p>To learn how to contribute to the lakeFS documentation see this page, which also includes details on how to build the documentation locally. </p>"},{"location":"project/contributing/#changelogmd","title":"CHANGELOG.md","text":"<p>Any user-facing change should be labeled with <code>include-changelog</code>.</p> <p>The PR title should contain a concise summary of the feature or fix and the description should have the GitHub issue number. When we publish a new version of lakeFS, we will add this to the relevant version section of the changelog. If the change should not be included in the changelog, label it with <code>exclude-changelog</code>.</p>"},{"location":"project/contributing/#user-facing-changes-examples","title":"User-Facing Changes Examples","text":"<ol> <li>UI/UX modifications: Changes to the layout, color scheme, or navigation structure.</li> <li>New features: Adding functionality that users can directly interact with, unless defined as internal.</li> <li>Configuration changes: Updates to settings that users can adjust.</li> <li>Performance improvements: Enhancements that noticeably speed up the application.</li> <li>Bug fixes.</li> <li>Security updates: Changes that address vulnerabilities or privacy concerns.</li> </ol>"},{"location":"project/contributing/#non-user-facing-changes","title":"Non-User-Facing Changes:","text":"<ol> <li>Code refactoring: Restructuring the codebase without changing its external behavior.</li> <li>Backend optimizations: Improvements to server-side processes that don't noticeably affect performance.</li> <li>Database schema changes: Modifications to the data structure that don't alter the user interface and do not require data migration.</li> <li>Development tooling updates: Changes to build processes or development environments.</li> <li>Internal API: Adding/Altering APIs tagged as internal.</li> <li>Documentation updates.</li> <li>Imported libraries: Updates to third-party libraries that don't introduce security updates.</li> </ol>"},{"location":"project/docs/","title":"lakeFS Documentation","text":"<p>Any contribution to the docs, whether it is in conjunction with a code contribution or as a standalone, is appreciated.</p> <p>Please see the contributing guide for details on contributing to lakeFS in general.</p> <p>Notice</p> <p>lakeFS documentation is written using Markdown.  Make sure to familiarize yourself with the Markdown Guide.</p>"},{"location":"project/docs/#lakefs-documentation-philosophy","title":"lakeFS Documentation Philosophy","text":"<p>We are heavily inspired by the Di\u00e1taxis approach to documentation.</p> <p>At a very high-level, it defines documentation as falling into one of four categories:</p> <ul> <li>How To</li> <li>Tutorial</li> <li>Reference</li> <li>Explanation</li> </ul> <p>There is a lot more to it than this, and you are encouraged to read the Di\u00e1taxis website for more details. Its application to lakeFS was discussed in #6197</p>"},{"location":"project/docs/#lakefs-style-guide","title":"lakeFS Style Guide","text":"<ul> <li>Don't use unnecessary tech jargon or vague/wordy constructions - keep it friendly, not condescending.</li> <li>Be inclusive and welcoming - use gender-neutral words and pronouns when talking about abstract people like users and developers.</li> <li>Replace complex expressions with simpler ones.</li> <li>Keep it short - 25-30 words max per sentence.  Otherwise, your readers might get lost on the way.</li> <li>Use active voice instead of passive. For example: This feature can be used to do task X. vs. You can use this feature to do task X. The second one reads much better, right?</li> <li>You can explain things better by including examples. Show, not tell. Use illustrations, images, gifs, code snippets, etc.</li> <li>Establish a visual hierarchy to help people quickly find the information they need. Use text formatting to create levels of title and subtitle (such as <code>#</code> to <code>######</code> markdown headings).  The title of every page should use the topmost heading <code>#</code>; all other headings on the page should use lower headers <code>##</code> to <code>######</code>.</li> </ul>"},{"location":"project/docs/#headings","title":"Headings","text":"<p>The title of the page should be H1 (<code>#</code> in markdown). Use headings in descending order and do not skip any.</p>"},{"location":"project/docs/#prerequisites","title":"Prerequisites","text":"<p>Before you can build and serve the docs, make sure you have:</p> <ul> <li>Python 3.13+ installed</li> <li>pip (Python package installer)</li> </ul> <p>Then, from the root of the repository, install all documentation-related dependencies:</p> <pre><code>pip install -r docs/requirements-docs.txt\n</code></pre>"},{"location":"project/docs/#test-your-changes-locally","title":"Test your changes locally","text":"<p>If you have the necessary dependencies installed, you can run mkdocs to build and serve the documentation from your machine using the provided Makefile target:</p> <pre><code>make docs-serve\n</code></pre>"},{"location":"project/docs/#link-checking-locally","title":"Link Checking locally","text":"<p>When making a pull request to lakeFS that involves a <code>docs/*</code> file, a GitHub action will automagically check the links. You can also run this link checker manually on your local machine:</p> <ol> <li> <p>Build the site:</p> <pre><code>mkdocs build\n</code></pre> </li> <li> <p>Check the links:</p> <pre><code>docker run --rm \\\n        --name lakefs_docs_lychee \\\n        --volume \"$PWD:/data\"\\\n        --volume \"/tmp:/output\"\\\n        --tty \\\n        lycheeverse/lychee:master \\\n        --exclude-file /data/.lycheeignore \\\n        --output /output/lychee_report.md \\\n        --format markdown \\\n        /data/docs/site\n</code></pre> </li> <li> <p>Review the <code>lychee_report.md</code> in your local <code>/tmp</code> folder</p> </li> </ol>"},{"location":"quickstart/","title":"lakeFS Quickstart","text":"<p>Welcome to lakeFS!</p> <p>Tip</p> <p>You can use the 30-day free trial of lakeFS Cloud if you want to try out lakeFS without installing anything. </p> <p>lakeFS provides a \"Git for data\" platform enabling you to implement best practices from software engineering on your data lake, including branching and merging, CI/CD, and production-like dev/test environments. </p> <p>This quickstart will introduce you to some of the core ideas in lakeFS and show what you can do by illustrating the concept of branching, merging, and rolling back changes to data. It's laid out in five short sections: </p> <p>Start Here \ud83d\udc49</p>"},{"location":"quickstart/actions-and-hooks/","title":"Actions and Hooks in lakeFS","text":"<p>When we interact with lakeFS it can be useful to have certain checks performed at stages along the way. Let's see how actions in lakeFS can be of benefit here. </p> <p>We're going to enforce a rule that when a commit is made to any branch that begins with <code>etl</code>: </p> <ul> <li>the commit message must not be blank</li> <li>there must be <code>job_name</code> and <code>version</code> metadata</li> <li>the <code>version</code> must be numeric</li> </ul> <p>To do this we'll create an action. In lakeFS, an action specifies one or more events that will trigger it, and references one or more hooks to run when triggered. Actions are YAML files written to lakeFS under the <code>_lakefs_actions/</code> folder of the lakeFS repository.</p> <p>Hooks can be either a Lua script that lakeFS will execute itself, an external web hook, or an Airflow DAG. In this example, we're using a Lua hook.</p>"},{"location":"quickstart/actions-and-hooks/#configuring-the-action","title":"Configuring the Action","text":"<ol> <li>In lakeFS create a new branch called <code>add_action</code>. You can do this through the UI or with <code>lakectl</code>:      <pre><code>lakectl branch create lakefs://quickstart/add_action --source lakefs://quickstart/main\n</code></pre></li> <li>Open up your favorite text editor (or emacs), and paste the following YAML:      <pre><code>name: Check Commit Message and Metadata\non:\n  pre-commit:\n    branches:\n      - etl**\nhooks:\n  - id: check_metadata\n    type: lua\n    properties:\n    script: |\n        commit_message=action.commit.message\n        if commit_message and #commit_message&gt;0 then\n            print(\"\u2705 The commit message exists and is not empty: \" .. commit_message)\n        else\n            error(\"\\n\\n\u274c A commit message must be provided\")\n        end\n\n        job_name=action.commit.metadata[\"job_name\"]\n        if job_name == nil then\n            error(\"\\n\u274c Commit metadata must include job_name\")\n        else\n            print(\"\u2705 Commit metadata includes job_name: \" .. job_name)\n        end\n\n        version=action.commit.metadata[\"version\"]\n        if version == nil then\n            error(\"\\n\u274c Commit metadata must include version\")\n        else\n            print(\"\u2705 Commit metadata includes version: \" .. version)\n            if tonumber(version) then\n                print(\"\u2705 Commit metadata version is numeric\")\n            else\n                error(\"\\n\u274c Version metadata must be numeric: \" .. version)\n            end\n        end\n</code></pre></li> <li> <p>Save this file as <code>/tmp/check_commit_metadata.yml</code></p> <ul> <li>You can save it elsewhere, but make sure you change the path below when uploading</li> </ul> </li> <li> <p>Upload the <code>check_commit_metadata.yml</code> file to the <code>add_action</code> branch under <code>_lakefs_actions/</code>. As above, you can use the UI (make sure you select the correct branch when you do), or with <code>lakectl</code>:</p> <pre><code>lakectl fs upload lakefs://quickstart/add_action/_lakefs_actions/check_commit_metadata.yml --source /tmp/check_commit_metadata.yml\n</code></pre> </li> <li> <p>Go to the Uncommitted Changes tab in the UI, and make sure that you see the new file in the path shown: </p> <p> <p>Click Commit Changes and enter a suitable message to commit this new file to the branch. </p> <li> <p>Now we'll merge this new branch into <code>main</code>. From the Compare tab in the UI compare the <code>main</code> branch with <code>add_action</code> and click Merge</p> <p>"},{"location":"quickstart/actions-and-hooks/#testing-the-action","title":"Testing the Action","text":"<p>Let's remind ourselves what the rules are that the action is going to enforce.</p> <p>When a commit is made to any branch that begins with <code>etl</code>: </p> <ul> <li>the commit message must not be blank</li> <li>there must be <code>job_name</code> and <code>version</code> metadata</li> <li>the <code>version</code> must be numeric</li> </ul> <p>We'll start by creating a branch that's going to match the <code>etl</code> pattern, and then go ahead and commit a change and see how the action works. </p> <ol> <li> <p>Create a new branch (see above instructions on how to do this if necessary) called <code>etl_20230504</code>. Make sure you use <code>main</code> as the source branch. </p> <p>In your new branch you should see the action that you created and merged above: </p> <p></p> </li> <li> <p>To simulate an ETL job we'll use the built-in DuckDB editor to run some SQL and write the result back to the lakeFS branch. </p> <p>Open the <code>lakes.parquet</code> file on the <code>etl_20230504</code> branch from the Objects tab. Replace the SQL statement with the following: </p> <pre><code>COPY (\n    WITH src AS (\n        SELECT lake_name, country, depth_m,\n            RANK() OVER ( ORDER BY depth_m DESC) AS lake_rank\n        FROM READ_PARQUET('lakefs://quickstart/etl_20230504/lakes.parquet'))\n    SELECT * FROM SRC WHERE lake_rank &lt;= 10\n) TO 'lakefs://quickstart/etl_20230504/top10_lakes.parquet'    \n</code></pre> </li> <li> <p>Head to the Uncommitted Changes tab in the UI and notice that there is now a file called <code>top10_lakes.parquet</code> waiting to be committed. </p> <p></p> <p>Now we're ready to start trying out the commit rules, and seeing what happens if we violate them.</p> </li> <li> <p>Click on Commit Changes, leave the Commit message blank, and click Commit Changes to confirm. </p> <p>Note that the commit fails because the hook did not succeed</p> <p><code>pre-commit hook aborted</code></p> <p>with the output from the hook's code displayed</p> <p><code>\u274c A commit message must be provided</code></p> <p></p> </li> <li> <p>Do the same as the previous step, but provide a message this time: </p> <p></p> <p>The commit still fails as we need to include metadata too, which is what the error tells us</p> <p><code>\u274c Commit metadata must include job_name</code></p> </li> <li> <p>Repeat the Commit Changes dialog and use the Add Metadata field to add the required metadata: </p> <p></p> <p>We're almost there, but this still fails (as it should), since the version is not entirely numeric but includes <code>v</code> and <code>\u00df</code>: </p> <p><code>\u274c Version metadata must be numeric: v1.00\u00df</code></p> <p>Repeat the commit attempt specify the version as <code>1.00</code> this time, and rejoice as the commit succeeds</p> <p></p> </li> </ol> <p>You can view the history of all action runs from the Action tab: </p> <p></p> <p>\u2190 Rollback the changes Work with lakeFS data on your local environment \u2192</p>"},{"location":"quickstart/branch/","title":"Create a Branch","text":"<p>lakeFS uses branches in a similar way to Git. It's a great way to isolate changes until, or if, we are ready to re-integrate them. lakeFS uses a zero-copy branching technique which means that it's very efficient to create branches of your data. </p> <p>Having seen the lakes data in the previous step we're now going to create a new dataset to hold data only for lakes in Denmark. Why? Well, because :)</p> <p>The first thing we'll do is create a branch for us to do this development against. We'll use the <code>lakectl</code> tool to create the branch, which we first need to configure with our credentials.  In a new terminal window run the following:</p> <pre><code>lakectl config\n</code></pre> <p>Tip</p> <p>If for some reason you get a command not found error, you can call lakectl by using <code>python -m lakectl</code> instead.</p> <p>Follow the prompts to enter the credentials that you got in the first step. Leave the Server endpoint URL as <code>http://127.0.0.1:8000</code>. </p> <p>Now that lakectl is configured, we can use it to create the branch. Run the following:</p> <pre><code>lakectl branch create lakefs://quickstart/denmark-lakes --source lakefs://quickstart/main\n</code></pre> <p>You should get a confirmation message like this:</p> <pre><code>Source ref: lakefs://quickstart/main\ncreated branch 'denmark-lakes' 3384cd7cdc4a2cd5eb6249b52f0a709b49081668bb1574ce8f1ef2d956646816\n</code></pre>"},{"location":"quickstart/branch/#transforming-the-data","title":"Transforming the Data","text":"<p>Now we'll make a change to the data. lakeFS has several native clients, as well as an S3-compatible endpoint. This means that anything that can use S3 will work with lakeFS. Pretty neat.</p> <p>We're going to use DuckDB which is embedded within the web interface of lakeFS. </p> <p>From the lakeFS Objects page select the <code>lakes.parquet</code> file to open the DuckDB editor: </p> <p></p> <p>To start with, we'll load the lakes data into a DuckDB table so that we can manipulate it. Replace the previous text in the DuckDB editor with this: </p> <pre><code>CREATE OR REPLACE TABLE lakes AS \n    SELECT * FROM READ_PARQUET('lakefs://quickstart/denmark-lakes/lakes.parquet');\n</code></pre> <p>You'll see a row count of 100,000 to confirm that the DuckDB table has been created. </p> <p>Just to check that it's the same data that we saw before we'll run the same query. Note that we are querying a DuckDB table (<code>lakes</code>), rather than using a function to query a parquet file directly. </p> <pre><code>SELECT   country, COUNT(*)\nFROM     lakes\nGROUP BY country\nORDER BY COUNT(*) \nDESC LIMIT 5;\n</code></pre> <p></p>"},{"location":"quickstart/branch/#making-a-change-to-the-data","title":"Making a Change to the Data","text":"<p>Now we can change our table, which was loaded from the original <code>lakes.parquet</code>, to remove all rows not for Denmark:</p> <pre><code>DELETE FROM lakes WHERE Country != 'Denmark';\n</code></pre> <p></p> <p>We can verify that it's worked by reissuing the same query as before:</p> <pre><code>SELECT   country, COUNT(*)\nFROM     lakes\nGROUP BY country\nORDER BY COUNT(*) \nDESC LIMIT 5;\n</code></pre> <p></p>"},{"location":"quickstart/branch/#write-the-data-back-to-lakefs","title":"Write the Data back to lakeFS","text":"<p>The changes so far have only been to DuckDB's copy of the data. Let's now push it back to lakeFS. Note the path is different this time as we're writing it to the <code>denmark-lakes</code> branch, not <code>main</code>: </p> <pre><code>COPY lakes TO 'lakefs://quickstart/denmark-lakes/lakes.parquet';\n</code></pre> <p></p>"},{"location":"quickstart/branch/#verify-that-the-datas-changed-on-the-branch","title":"Verify that the Data's Changed on the Branch","text":"<p>Let's just confirm for ourselves that the parquet file itself has the new data. We'll drop the <code>lakes</code> table just to be sure, and then query the parquet file directly:</p> <pre><code>DROP TABLE lakes;\n\nSELECT   country, COUNT(*)\nFROM     READ_PARQUET('lakefs://quickstart/denmark-lakes/lakes.parquet')\nGROUP BY country\nORDER BY COUNT(*) \nDESC LIMIT 5;\n</code></pre> <p></p>"},{"location":"quickstart/branch/#what-about-the-data-in-main","title":"What about the data in <code>main</code>?","text":"<p>So we've changed the data in our <code>denmark-lakes</code> branch, deleting swathes of the dataset. What's this done to our original data in the <code>main</code> branch? Absolutely nothing! See for yourself by running the same query as above, but against the <code>main</code> branch:</p> <p><pre><code>SELECT   country, COUNT(*)\nFROM     READ_PARQUET('lakefs://quickstart/main/lakes.parquet')\nGROUP BY country\nORDER BY COUNT(*) \nDESC LIMIT 5;\n</code></pre> </p> <p>In the next step we'll see how to commit our changes and merge our branch back into main. </p> <p>\u2190 Query the pre-populated data Merge the branch back into main \u2192</p>"},{"location":"quickstart/commit-and-merge/","title":"Committing Changes in lakeFS","text":"<p>Info</p> <p>In the previous step we branched our data from <code>main</code> into a new <code>denmark-lakes</code> branch, and overwrote the <code>lakes.parquet</code> to hold solely information about lakes in Denmark. Now we're going to commit that change (just like Git) and merge it back to main (just like git).</p> <p>Having make the change to the datafile in the <code>denmark-lakes</code> branch, we now want to commit it. There are various options for interacting with the lakeFS API, including the web interface, a Python client, and <code>lakectl</code> which is what we'll use here. Run the following from a terminal window:</p> <pre><code>lakectl commit lakefs://quickstart/denmark-lakes -m \"Create a dataset of just the lakes in Denmark\"\n</code></pre> <p>You will get confirmation of the commit including its hash. <pre><code>Branch: lakefs://quickstart/denmark-lakes\nCommit for branch \"denmark-lakes\" completed.\n\nID: ba6d71d0965fa5d97f309a17ce08ad006c0dde15f99c5ea0904d3ad3e765bd74\nMessage: Create a dataset of just the lakes in Denmark\nTimestamp: 2023-03-15 08:09:36 +0000 UTC\nParents: 3384cd7cdc4a2cd5eb6249b52f0a709b49081668bb1574ce8f1ef2d956646816\n</code></pre></p> <p>With our change committed, it's now time to merge it to back to the <code>main</code> branch. </p>"},{"location":"quickstart/commit-and-merge/#merging-branches-in-lakefs","title":"Merging Branches in lakeFS \ud83d\udd00","text":"<p>As above, we'll use <code>lakectl</code> to do this too. The syntax just requires us to specify the source and target of the merge. Run this from a terminal window.</p> <pre><code>lakectl merge lakefs://quickstart/denmark-lakes lakefs://quickstart/main\n</code></pre> <p>We can confirm that this has worked by returning to the same object view of <code>lakes.parquet</code> as before and clicking on Execute to rerun the same query. You'll see that the country row counts have changed, and only Denmark is left in the data: </p> <p></p> <p>But\u2026oh no! A slow chill creeps down your spine, and the bottom drops out of your stomach. What have you done! \ud83d\ude31 You were supposed to create a separate file of Denmark's lakes - not replace the original one</p> <p>Is all lost? Will our hero overcome the obstacles? No, and yes respectively!</p> <p>Have no fear; lakeFS can revert changes. Tune in for the final part of the quickstart to see how. </p> <p>\u2190 Create a branch of the data Rollback the changes \u2192</p>"},{"location":"quickstart/launch/","title":"Spin up the environment","text":"<p>Tip</p> <p>If you don't want to install lakeFS locally, you can use the 30-day free trial of lakeFS Cloud. Once you launch the free trial you will have access to the same content as this quickstart within the provided repository once you login.</p> <p>install and launch lakeFS:</p> <pre><code>pip install lakefs\npython -m lakefs.quickstart\n</code></pre> <p>After a few moments you should see the lakeFS container ready to use: </p> <pre><code>\u2502\n\u2502 lakeFS running in quickstart mode.\n\u2502     Login at http://127.0.0.1:8000/\n\u2502\n\u2502     Access Key ID    : AKIAIOSFOLQUICKSTART\n\u2502     Secret Access Key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\u2502\n</code></pre> <p>You're now ready to dive into lakeFS! </p> <ol> <li> <p>Open lakeFS's web interface at http://127.0.0.1:8000/</p> </li> <li> <p>Login with the quickstart credentials. </p> <ul> <li>Access Key ID: <code>AKIAIOSFOLQUICKSTART</code></li> <li>Secret Access Key: <code>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</code></li> </ul> </li> <li> <p>You'll notice that there aren't any repositories created yet. Click the Create Sample Repository button. </p> </li> </ol> <p></p> <p>You will see the sample repository created and the quickstart guide within it. You can follow along there, or here - it's the same :) </p> <p></p> <p>\u2190 Quickstart introduction Query the pre-populated data \u2192</p>"},{"location":"quickstart/learning-more-lakefs/","title":"Learn more about lakeFS","text":"<p>The lakeFS quickstart is just the beginning of your lakeFS journey \ud83d\udee3\ufe0f</p> <p>Here are some more resources to help you find out more about lakeFS.</p>"},{"location":"quickstart/learning-more-lakefs/#connecting-lakefs-to-your-own-object-storage","title":"Connecting lakeFS to your own object storage","text":"<p>Enjoyed the quickstart and want to try out lakeFS against your own data? Here's how to run lakeFS locally, connecting to an object store.</p> <p>Note</p> <p>Make sure the Quickstart server from the previous steps isn't also running as you'll get a port conflict.</p> AWS S3Azure Blob StorageGoogle Cloud StorageMinIO <pre><code>export LAKEFS_BLOCKSTORE_TYPE=\"s3\"\nexport AWS_ACCESS_KEY_ID=\"YourAccessKeyValue\"\nexport AWS_SECRET_ACCESS_KEY=\"YourSecretKeyValue\"\nlakefs run --local-settings\n</code></pre> <pre><code>export LAKEFS_BLOCKSTORE_TYPE=\"azure\"\nexport LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT=\"YourAzureStorageAccountName\"\nexport LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY=\"YourAzureStorageAccessKey\"\nlakefs run --local-settings\n</code></pre> <p><pre><code>export LAKEFS_BLOCKSTORE_TYPE=\"gs\"\nexport LAKEFS_BLOCKSTORE_GS_CREDENTIALS_JSON=\"YourGoogleServiceAccountKeyJSON\"\nlakefs run --local-settings\n</code></pre> where you will replace <code>YourGoogleServiceAccountKeyJSON</code> with JSON string that contains your Google service account key.</p> <p>If you want to use the JSON file that contains your Google service account key instead of JSON string (as in the previous command) then go to the directory where JSON file is stored and run the command with local parameters:</p> <pre><code>export LAKEFS_BLOCKSTORE_TYPE=\"gs\"\nexport LAKEFS_BLOCKSTORE_GS_CREDENTIALS_FILE=\"/myfiles/YourGoogleServiceAccountKey.json\"\nlakefs run --local-settings\n</code></pre> <p>This command will mount your present working directory (PWD) within the container and will read the JSON file from your PWD.</p> <p>To use lakeFS with MinIO (or other S3-compatible object storage), use the following example:</p> <pre><code>export LAKEFS_BLOCKSTORE_TYPE=\"s3\"\nexport LAKEFS_BLOCKSTORE_S3_FORCE_PATH_STYLE=\"true\"\nexport LAKEFS_BLOCKSTORE_S3_ENDPOINT=\"http://&lt;minio_endpoint&gt;\"\nexport LAKEFS_BLOCKSTORE_S3_DISCOVER_BUCKET_REGION=\"false\"\nexport LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID=\"&lt;minio_access_key&gt;\"\nexport LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY=\"&lt;minio_secret_key&gt;\"\nlakefs run --local-settings\n</code></pre>"},{"location":"quickstart/learning-more-lakefs/#deploying-lakefs","title":"Deploying lakeFS","text":"<p>Ready to do this thing for real? The deployment guides show you how to deploy lakeFS locally (including on Kubernetes) or on AWS, Azure, or GCP.</p> <p>Alternatively you might want to have a look at lakeFS Cloud which provides a fully-managed, SOC-2 compliant, lakeFS service.</p>"},{"location":"quickstart/learning-more-lakefs/#lakefs-samples","title":"lakeFS Samples","text":"<p>The lakeFS Samples GitHub repository includes some excellent examples including:</p> <ul> <li>How to implement multi-table transaction on multiple Delta Tables</li> <li>Notebooks to show integration of lakeFS with Spark, Python, Delta Lake, Airflow and Hooks.</li> <li>Examples of using lakeFS webhooks to run automated data quality checks on different branches.</li> <li>Using lakeFS branching features to create dev/test data environments for ETL testing and experimentation.</li> <li>Reproducing ML experiments with certainty using lakeFS tags.</li> </ul>"},{"location":"quickstart/learning-more-lakefs/#lakefs-community","title":"lakeFS Community","text":"<p>The lakeFS community is important to us. Our guiding principles are:</p> <ul> <li>Fully open, in code and conversation</li> <li>We learn and grow together</li> <li>Compassion and respect in every interaction</li> </ul> <p>We'd love for you to join our Slack group and come and introduce yourself on <code>#announcements-and-more</code>. Or just lurk and soak up the vibes \ud83d\ude0e</p> <p>If you're interested in getting involved in the development of lakeFS, head over our the GitHub repo to look at the code and peruse the issues. The comprehensive contributing document should have you covered on next steps but if you've any questions the <code>#dev</code> channel on Slack will be delighted to help.</p> <p>We love speaking at meetups and chatting to community members at them - you can find a list of these here.</p> <p>Finally, make sure to drop by to say hi on Twitter or LinkedIn \ud83d\udc4b\ud83c\udffb</p>"},{"location":"quickstart/learning-more-lakefs/#lakefs-concepts-and-internals","title":"lakeFS Concepts and Internals","text":"<p>We describe lakeFS as \"Git for data\" but what does that actually mean? Have a look at the concepts and architecture guides, as well as the explanation of how merges are handled. To go deeper you might be interested in the internals of versioning and our internal database structure.</p> <p>\u2190 Work with lakeFS data on your local environment</p>"},{"location":"quickstart/query/","title":"Let's Query Something","text":"<p>The lakeFS server has been loaded with a sample parquet datafile. Fittingly enough for a piece of software to help users of data lakes, the <code>lakes.parquet</code> file holds data about lakes around the world. </p> <p>You'll notice that the branch is set to <code>main</code>. This is conceptually the same as your main branch in Git against which you develop software code. </p> <p></p> <p>Let's have a look at the data, ahead of making some changes to it on a branch in the following steps. </p> <p>Click on <code>lakes.parquet</code> and notice that the built-it DuckDB runs a query to show a preview of the file's contents. </p> <p></p> <p>Now we'll run our own query on it to look at the top five countries represented in the data. </p> <p>Copy and paste the following SQL statement into the DuckDB query panel and click on Execute.</p> <pre><code>SELECT   country, COUNT(*)\nFROM     READ_PARQUET('lakefs://quickstart/main/lakes.parquet')\nGROUP BY country\nORDER BY COUNT(*) \nDESC LIMIT 5;\n</code></pre> <p></p> <p>Next we're going to make some changes to the data\u2014but on a development branch so that the data in the main branch remains untouched. </p> <p>\u2190 Launch the quickstart environment Create a branch of the data \u2192</p>"},{"location":"quickstart/rollback/","title":"Rolling back Changes in lakeFS","text":"<p>Our intrepid user (you) merged a change back into the <code>main</code> branch and realised that they had made a mistake \ud83e\udd26\ud83c\udffb. </p> <p>The good news for them (you) is that lakeFS can revert changes made, similar to how you would in Git \ud83d\ude05. </p> <p>From your terminal window run <code>lakectl</code> with the <code>revert</code> command:</p> <p><pre><code>lakectl branch revert lakefs://quickstart/main main --parent-number 1 --yes\n</code></pre> You should see a confirmation of a successful rollback: <pre><code>Branch: lakefs://quickstart/main\ncommit main successfully reverted\n</code></pre></p> <p>Back in the object page and the DuckDB query we can see that the original file is now back to how it was:  </p> <p>\u2190 Merge the branch back into main Using Actions and Hooks in lakeFS \u2192</p>"},{"location":"quickstart/work-with-data-locally/","title":"Work with lakeFS Data Locally","text":"<p>When working with lakeFS, there are scenarios where we need to access and manipulate data locally. An example use case for working locally is machine learning model development. Machine learning model development is dynamic and iterative. To optimize this process, experiments need to be conducted with speed, tracking ease, and reproducibility. Localizing model data during development accelerates the process by enabling interactive and offline development and reducing data access latency.</p> <p>lakeFS provides 2 ways to expose versioned data locally</p>"},{"location":"quickstart/work-with-data-locally/#lakefs-mount","title":"lakeFS Mount","text":"<p>Info</p> <p>lakeFS Mount is available for lakeFS Enterprise and lakeFS Cloud customers. You can try it out by signing up</p>"},{"location":"quickstart/work-with-data-locally/#getting-started-with-lakefs-mount","title":"Getting started with lakeFS Mount","text":"<p>Prerequisites:</p> <ul> <li>A working lakeFS Server running either lakeFS Enterprise or lakeFS Cloud</li> <li>You\u2019ve installed the lakectl command line utility: this is the official lakeFS command line interface, on top of which lakeFS Mount is built.</li> <li>lakectl is configured properly to access your lakeFS server as detailed in the configuration instructions</li> </ul>"},{"location":"quickstart/work-with-data-locally/#mounting-a-path-to-a-local-directory","title":"Mounting a path to a local directory:","text":"<ol> <li> <p>In lakeFS create a new branch called <code>my-experiment</code>. You can do this through the UI or with <code>lakectl</code>:</p> <pre><code>lakectl branch create \\\n    lakefs://quickstart/my-experiment \\\n    --source lakefs://quickstart/main\n</code></pre> </li> <li> <p>Mount images from your quickstart repository into a local directory named <code>my_local_dir</code></p> <pre><code>everest mount lakefs://quickstart/my-experiment/images my_local_dir\n</code></pre> <p>Once complete, <code>my_local_dir</code> should be mounted with the specified path.</p> </li> <li> <p>Verify that <code>my_local_dir</code> is linked to the correct path in your lakeFS remote:     <pre><code>ls -l my_local_dir\n</code></pre></p> </li> <li> <p>To unmount the directory, simply run:</p> <pre><code>everest umount ./my_local_dir\n</code></pre> <p>Which will unmount the path and terminate the local mount-server.</p> </li> </ol>"},{"location":"quickstart/work-with-data-locally/#lakectl-local","title":"lakectl local","text":"<p>Alternatively, we can use lakectl local to bring a subset of our lakeFS data to a local directory within the lakeFS container and edit an image dataset used for ML model development. Unlike lakeFS Mount, using <code>lakectl local</code> requires copying data to/from lakeFS and your local machine.</p> <p>Reference Guide: lakeFS lakectl local for machine learning</p>"},{"location":"quickstart/work-with-data-locally/#cloning-a-subset-of-lakefs-data-into-a-local-directory","title":"Cloning a Subset of lakeFS Data into a Local Directory","text":"<ol> <li>In lakeFS create a new branch called <code>my-experiment</code>. You can do this through the UI or with <code>lakectl</code>:     <pre><code>lakectl branch create lakefs://quickstart/my-experiment --source lakefs://quickstart/main\n</code></pre></li> <li>Clone images from your quickstart repository into a local directory named <code>my_local_dir</code> within your container:     <pre><code>lakectl local clone lakefs://quickstart/my-experiment/images my_local_dir\n</code></pre></li> <li>Verify that <code>my_local_dir</code> is linked to the correct path in your lakeFS remote:     <pre><code>lakectl local list\n</code></pre>    You should see confirmation that my_local_dir is tracking the desired lakeFS path.:    <pre><code>    my_local_dir lakefs://quickstart/my-experiment/images/8614575b5488b47a094163bd17a12ed0b82e0bcbfd22ed1856151c671f1faa53\n</code></pre></li> <li> <p>Verify that your local environment is up-to-date with its remote path:</p> <pre><code>lakectl local status my_local_dir\n</code></pre> <p>You should get a confirmation message like this showing that there is no difference between your local environment and the lakeFS remote:</p> <pre><code>diff 'local:///home/lakefs/my_local_dir' &lt;--&gt; 'lakefs://quickstart/8614575b5488b47a094163bd17a12ed0b82e0bcbfd22ed1856151c671f1faa53/images/'...\ndiff 'lakefs://quickstart/8614575b5488b47a094163bd17a12ed0b82e0bcbfd22ed1856151c671f1faa53/images/' &lt;--&gt; 'lakefs://quickstart/my-experiment/images/'...\n\nNo diff found.\n</code></pre> </li> </ol>"},{"location":"quickstart/work-with-data-locally/#making-changes-to-data-locally","title":"Making Changes to Data Locally","text":"<ol> <li>Clean the dataset by removing images larger than 225 KB:     <pre><code>find my_local_dir -type f -size +225k -delete\n</code></pre></li> <li> <p>Check the status of your local changes compared to the lakeFS remote path:     <pre><code>lakectl local status my_local_dir\n</code></pre></p> <p>You should get a confirmation message like this, showing the modifications you made locally: <pre><code>diff 'local:///home/lakefs/my_local_dir' &lt;--&gt; 'lakefs://quickstart/8614575b5488b47a094163bd17a12ed0b82e0bcbfd22ed1856151c671f1faa53/images/'...\ndiff 'lakefs://quickstart/8614575b5488b47a094163bd17a12ed0b82e0bcbfd22ed1856151c671f1faa53/images/' &lt;--&gt; 'lakefs://quickstart/my-experiment/images/'...\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 SOURCE \u2551 CHANGE   \u2551 PATH                \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 local  \u2551 modified \u2551 axolotl.png         \u2551\n\u2551 local  \u2551 removed  \u2551 duckdb-main-02.png  \u2551\n\u2551 local  \u2551 removed  \u2551 empty-repo-list.png \u2551\n\u2551 local  \u2551 removed  \u2551 repo-contents.png   \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre></p> </li> </ol>"},{"location":"quickstart/work-with-data-locally/#pushing-local-changes-to-lakefs","title":"Pushing Local Changes to lakeFS","text":"<p>Once we are done with editing the image dataset in our local environment, we will push our changes to the lakeFS remote so that the improved dataset is shared and versioned.</p> <ol> <li> <p>Commit your local changes to lakeFS:</p> <pre><code>lakectl local commit -m 'Deleted images larger than 225KB in size and changed the Axolotl image' my_local_dir\n</code></pre> <p>In your branch, you should see the commit including your local changes:</p> <p></p> </li> <li> <p>Compare <code>my-experiment</code> branch to the <code>main</code> branch to visualize your changes:</p> <p></p> </li> </ol> <p>Bonus Challenge</p> <p>And so with that, this quickstart for lakeFS draws to a close. If you're simply having too much fun to stop then here's an exercise for you.</p> <p>Implement the requirement from the beginning of this quickstart correctly, such that you write <code>denmark-lakes.parquet</code> in the respective branch and successfully merge it back into main. Look up how to list the contents of the <code>main</code> branch and verify that it looks like this:</p> <pre><code>object          2023-03-21 17:33:51 +0000 UTC    20.9 kB         denmark-lakes.parquet\nobject          2023-03-21 14:45:38 +0000 UTC    916.4 kB        lakes.parquet\n</code></pre> <p>\u2190 Using Actions and Hooks in lakeFS Learn more about lakeFS \u2192</p>"},{"location":"reference/api/","title":"lakeFS API","text":""},{"location":"reference/auditing/","title":"Auditing","text":"<p>Info</p> <p>This feature is available on lakeFS Cloud and lakeFS Enterprise</p> <p>The lakeFS audit log allows you to view all relevant user action information in a clear and organized table, including when the action was performed, by whom, and what it was they did. </p> <p>This can be useful for several purposes, including: </p> <ol> <li> <p>Compliance - Audit logs can be used to show what data users accessed, as well as any changes they made to user management.</p> </li> <li> <p>Troubleshooting - If something changes on your underlying object store that you weren't expecting, such as a big file suddenly breaking into thousands of smaller files, you can use the audit log to find out what action led to this change. </p> </li> </ol>"},{"location":"reference/auditing/#setting-up-access-to-audit-logs-on-aws-s3","title":"Setting up access to Audit Logs on AWS S3","text":"<p>The access to the Audit Logs is done via AWS S3 Access Point.</p> <p>There are different ways to interact with an access point (see Using access points in AWS).</p> <p>The initial setup:</p> <ol> <li>Take note of the IAM Role ARN that will be used to access the data. This should be the user or role used by e.g. Athena.</li> <li>Reach out to customer success and provide this ARN. Once receiving the ARN role, an access point will be created and you should get in response the following details:<ol> <li>S3 Bucket (e.g. <code>arn:aws:s3:::lakefs-audit-logs-us-east-1-production</code>)</li> <li>S3 URI to an access point (e.g. <code>s3://arn:aws:s3:us-east-1:&lt;treeverse-id&gt;:accesspoint/lakefs-logs-&lt;organization&gt;</code>)</li> <li>Access Point alias. You can use this alias instead of the bucket name or Access Point ARN to access data through the Access Point. (e.g. <code>lakefs-logs-&lt;generated&gt;-s3alias</code>)</li> <li>Update your IAM Role policy and trust policy if required</li> </ol> </li> </ol> <p>A minimal example for IAM policy with 2 lakeFS installations in 2 regions (<code>us-east-1</code>, <code>us-west-2</code>):</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakefs-audit-logs-us-east-1-production\",\n                \"arn:aws:s3:::lakefs-audit-logs-us-east-1-production/*\",\n                \"arn:aws:s3:::lakefs-logs-&lt;generated&gt;-s3alias/*\",\n                \"arn:aws:s3:us-east-1:&lt;treeverse-id&gt;:accesspoint/lakefs-logs-&lt;organization&gt;\",\n                \"arn:aws:s3:us-east-1:&lt;treeverse-id&gt;:accesspoint/lakefs-logs-&lt;organization&gt;/*\"\n            ],\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"etl/v1/data/region=&lt;region_a&gt;/organization=org-&lt;organization&gt;/*\",\n                        \"etl/v1/data/region=&lt;region_b&gt;/organization=org-&lt;organization&gt;/*\"\n                    ]\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakefs-audit-logs-us-east-1-production\",\n                \"arn:aws:s3:::lakefs-audit-logs-us-east-1-production/etl/v1/data/region=&lt;region_a&gt;/organization=org-&lt;organization&gt;/*\",\n                \"arn:aws:s3:::lakefs-audit-logs-us-east-1-production/etl/v1/data/region=&lt;region_b&gt;/organization=org-&lt;organization&gt;/*\",\n                \"arn:aws:s3:::lakefs-logs-&lt;generated&gt;-s3alias/*\",\n                \"arn:aws:s3:us-east-1:&lt;treeverse-id&gt;:accesspoint/lakefs-logs-&lt;organization&gt;/object/etl/v1/data/region=&lt;region_a&gt;/organization=org-&lt;organization&gt;/*\",\n                \"arn:aws:s3:us-east-1:&lt;treeverse-id&gt;:accesspoint/lakefs-logs-&lt;organization&gt;/object/etl/v1/data/region=&lt;region_b&gt;/organization=org-&lt;organization&gt;/*\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"kms:Decrypt\"\n            ],\n            \"Resource\": [\n                \"arn:aws:kms:us-east-1:&lt;treeverse-id&gt;:key/&lt;encryption-key-id&gt;\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n</code></pre> <p>Trust Policy example that allows anyone in your account to assume the role above:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;YOUR_ACCOUNT_ID&gt;:root\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {}\n        }\n    ]\n}\n</code></pre> <p>Authentication is done by assuming an IAM Role:</p> <pre><code># Assume role use AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN:\naws sts assume-role --role-arn arn:aws:iam::&lt;your-aws-account&gt;:role/&lt;reader-role&gt; --role-session-name &lt;name&gt; \n\n# verify role assumed\naws sts get-caller-identity \n\n# list objects (can be used with --recursive) with access point ARN\naws s3 ls arn:aws:s3:us-east-1:&lt;treeverse-id&gt;:accesspoint/lakefs-logs-&lt;organization&gt;/etl/v1/data/region=&lt;region&gt;/organization=org-&lt;organization&gt;/\n\n# get object locally via s3 access point alias \naws s3api get-object --bucket lakefs-logs-&lt;generated&gt;-s3alias --key etl/v1/data/region=&lt;region&gt;/organization=org-&lt;organization&gt;/year=&lt;YY&gt;/month=&lt;MM&gt;/day=&lt;DD&gt;/hour=&lt;HH&gt;/&lt;file&gt;-snappy.parquet sample.parquet \n</code></pre>"},{"location":"reference/auditing/#data-layout","title":"Data layout","text":"<p>Tip</p> <p>The bucket name is important when creating the IAM policy but, the Access Point ARN and Alias will be the ones that are used to access the data (i.e AWS CLI, Spark etc).</p> <p>Bucket Name: <code>lakefs-audit-logs-us-east-1-production</code></p> <p>Root prefix: <code>etl/v1/data/region=&lt;region&gt;/organization=org-&lt;organization-name&gt;/</code></p> <p>Files Path pattern: All the audit logs files are in parquet format and their pattern is: <code>etl/v1/data/region=&lt;region&gt;/organization=org-&lt;organization-name&gt;/year=&lt;YY&gt;/month=&lt;MM&gt;/day=&lt;DD&gt;/hour=&lt;HH&gt;/*-snappy.parquet</code></p>"},{"location":"reference/auditing/#path-values","title":"Path Values","text":"<p>region: lakeFS installation region (e.g the region in lakeFS URL: https://..lakefscloud.io/) <p>organization: Found in the lakeFS URL <code>https://&lt;organization-name&gt;.&lt;region&gt;.lakefscloud.io/</code>. The value in the S3 path must be prefixed with <code>org-&lt;organization-name&gt;</code></p>"},{"location":"reference/auditing/#partitions","title":"Partitions","text":"<ul> <li><code>year</code></li> <li><code>month</code></li> <li><code>day</code></li> <li><code>hour</code></li> </ul>"},{"location":"reference/auditing/#example","title":"Example","text":"<p>As an example paths for \"Acme\" organization with 2 lakeFS installations:</p> <pre><code># ACME in us-east-1 \netl/v1/data/region=us-east-1/organization=org-acme/year=2024/month=02/day=12/hour=13/log_abc-snappy.parquet\n\n# ACME in us-west-2 \netl/v1/data/region=us-west-2/organization=org-acme/year=2024/month=02/day=12/hour=13/log_xyz-snappy.parquet\n</code></pre>"},{"location":"reference/auditing/#schema","title":"Schema","text":"<p>The files are in parquet format and can be accessed directly from Spark or any client that can read parquet files. Using Spark's <code>printSchema()</code> we can inspect the values, that\u2019s the latest schema with comments on important columns:</p> column type description <code>data_user</code> string the internal user ID for the user making the request. if using an external IdP (i.e SSO, Microsoft Entra, etc) it will be the UID represented by the IdP. (see below an  example how to extract the info of external IDs in python) <code>data_repository</code> string the repository ID relevant for this request. Currently only returned for s3_gateway requests <code>data_ref</code> string the reference ID (tag, branch, ...) relevant for this request. Currently only returned for s3_gateway requests <code>data_status_code</code> int HTTP status code returned for this request <code>data_service_name</code> string Service name for the request. Could be either \"rest_api\" or \"s3_gateway\" <code>data_request_id</code> string Unique ID representing this request <code>data_path</code> string HTTP path used for this request <code>data_operation_id</code> string Logical operation ID for this request. E.g. <code>list_objects</code>, <code>delete_repository</code>, ... <code>data_method</code> string HTTP method for the request <code>data_time</code> string datetime representing the start time of this request, in ISO 8601 format"},{"location":"reference/auditing/#idp-users-map-user-ids-from-audit-logs-to-an-email-in-lakefs","title":"IdP users: map user IDs from audit logs to an email in lakeFS","text":"<p>The <code>data_user</code> column in each log represents the user id that performed it.</p> <ul> <li>It might be empty in cases where authentication is not required (e.g login attempt).</li> <li>If the user is an API user created internally in lakeFS that id is also the name it was given.</li> <li><code>data_user</code> might contain an ID to an external IdP (i.e. SSO system), usually it is not human friendly, we can correlate the ID to a lakeFS email used, see an example using the Python lakefs-sdk.</li> </ul> <pre><code>import lakefs_sdk\n\n# Configure HTTP basic authorization: basic_auth\nconfiguration = lakefs_sdk.Configuration(\n    host = \"https://&lt;org&gt;.&lt;region&gt;.lakefscloud.io/api/v1\",\n    username = 'AKIA...',\n    password = '...'\n)\n\n# Print all user email and uid in lakeFS \n# the uid is equal to the user id in the audit logs.\nwith lakefs_sdk.ApiClient(configuration) as api_client:\n    auth_api = lakefs_sdk.AuthApi(api_client)\n    has_more = True\n    next_offset = ''\n    page_size = 100 \n    while has_more: \n        resp = auth_api.list_users(prefix='', after=next_offset, amount=page_size)\n        for u in resp.results:\n            email = u.email\n            uid = u.id\n            print(f'Email: {email}, UID: {uid}')\n\n        has_more = resp.pagination.has_more \n        next_offset = resp.pagination.next_offset\n</code></pre> <p>Example: Glue Notebook with Spark</p> <pre><code>from awsglue.transforms import *\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\n# connect to s3 access point \nalias = 's3://&lt;bucket-alias-name&gt;'\ns3_dyf = glueContext.create_dynamic_frame.from_options(\n    format_options={},\n    connection_type=\"s3\",\n    format=\"parquet\",\n    connection_options={\n        \"paths\": [alias + \"/etl/v1/data/region=&lt;region&gt;/organization=org-&lt;org&gt;/year=&lt;YY&gt;/month=&lt;MM&gt;/day=&lt;DD&gt;/hour=&lt;HH&gt;/\"],\n        \"recurse\": True,\n    },\n    transformation_ctx=\"sample-ctx\",\n)\n\ns3_dyf.show()\ns3_dyf.printSchema()\n</code></pre>"},{"location":"reference/cli/","title":"lakectl (lakeFS command-line tool)","text":"<p>Note</p> <p>This file (cli.md) is automatically generated from the Go code files under <code>cmd/lakectl</code>.  Any changes made directly to the Markdown file will be overwritten, and should instead be made to the relevant Go files. </p>"},{"location":"reference/cli/#installing-lakectl-locally","title":"Installing lakectl locally","text":"<p><code>lakectl</code> is available for Linux, macOS, and Windows. You can also run it using Docker.</p> <p> Download lakectl</p> <p>Or using Homebrew for Linux/macOS:</p> <pre><code>brew tap treeverse/lakefs\nbrew install lakefs\n</code></pre>"},{"location":"reference/cli/#configuring-credentials-and-api-endpoint","title":"Configuring credentials and API endpoint","text":"<p>Once you've installed the lakectl command, run:</p> <pre><code>lakectl config\n# output:\n# Config file /home/janedoe/.lakectl.yaml will be used\n# Access key ID: AKIAIOSFODNN7EXAMPLE\n# Secret access key: ****************************************\n# Server endpoint URL: http://localhost:8000\n</code></pre> <p>This will setup a <code>$HOME/.lakectl.yaml</code> file with the credentials and API endpoint you've supplied. When setting up a new installation and creating initial credentials (see Quickstart), the UI will provide a link to download a preconfigured configuration file for you.</p>"},{"location":"reference/cli/#lakectl-configuration","title":"lakectl Configuration","text":"<p><code>lakectl</code> reads its configuration from a YAML file (default path <code>~/.lakectl.yaml</code>, overridable with <code>--config</code> or <code>LAKECTL_CONFIG_FILE</code>) and/or from environment variables.</p> <ul> <li>Every configuration key can be supplied through an environment variable using the pattern <code>LAKECTL_&lt;UPPERCASE_KEY_WITH_DOTS_REPLACED_BY_UNDERSCORES&gt;</code>.</li> <li>Any value given on the command-line flags overrides the value in the configuration file, which in turn overrides the value supplied through the environment.</li> </ul>"},{"location":"reference/cli/#reference","title":"Reference","text":"<ul> <li><code>credentials.access_key_id</code> <code>(string : required)</code> - Access-key ID used to authenticate against lakeFS.</li> <li><code>credentials.secret_access_key</code> <code>(string : required)</code>  - Secret access key paired with the access key ID.</li> <li><code>credentials.provider.type</code> <code>(string : \"\")</code> - Enterprise only. Set to <code>aws_iam</code> to obtain temporary credentials from AWS IAM; empty for static credentials (default).</li> <li><code>credentials.provider.aws_iam.token_ttl_seconds</code> <code>(duration : 6h)</code> - Lifetime of the generated lakeFS token.</li> <li><code>credentials.provider.aws_iam.url_presign_ttl_seconds</code> <code>(duration : 1m)</code> - TTL of pre-signed URLs created by lakectl.</li> <li><code>credentials.provider.aws_iam.refresh_interval</code> <code>(duration : 5m)</code> - How often lakectl refreshes the IAM credentials.</li> <li><code>credentials.provider.aws_iam.token_request_headers</code> <code>(map[string]string : {})</code> - Extra HTTP headers to include when requesting the token.</li> <li><code>network.http2.enabled</code> <code>(bool : true)</code> - Enable HTTP/2 for the API client.</li> <li><code>server.endpoint_url</code> <code>(string :</code> http://127.0.0.1:8000 `) - Base URL of the lakeFS server.</li> <li><code>server.retries.enabled</code> <code>(bool : true)</code> - Whether lakectl tries more than once.</li> <li><code>server.retries.max_attempts</code> <code>(uint : 4)</code> - Maximum number of attempts per request.</li> <li><code>server.retries.min_wait_interval</code> <code>(duration : 200ms)</code> - Minimum back-off between retries.</li> <li><code>server.retries.max_wait_interval</code> <code>(duration : 30s)</code> - Maximum back-off between retries.</li> <li><code>options.parallelism</code> <code>(int : 25)</code> - Default concurrency level for I/O operations (upload, download, etc.).</li> <li><code>local.skip_non_regular_files</code> <code>(bool : false)</code> - When true, symbolic links and other non-regular files are skipped during <code>lakectl local</code> operations instead of causing an error.</li> <li><code>experimental.local.posix_permissions.enabled</code> <code>(bool : false)</code> - Preserve POSIX permissions when syncing files.</li> <li><code>experimental.local.posix_permissions.include_uid</code> <code>(bool : false)</code> - Include UID in the stored metadata.</li> <li><code>experimental.local.posix_permissions.include_gid</code> <code>(bool : false)</code> - Include GID in the stored metadata.</li> </ul>"},{"location":"reference/cli/#running-lakectl-from-docker","title":"Running lakectl from Docker","text":"<p>If you'd rather run <code>lakectl</code> from a Docker container you can do so by passing configuration elements as environment variables.  Here is an example: </p> <pre><code>docker run --rm --pull always \\\n          -e LAKECTL_CREDENTIALS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE \\\n          -e LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY=xxxxx\n          -e LAKECTL_SERVER_ENDPOINT_URL=https://host.us-east-2.lakefscloud.io/ \\\n          --entrypoint lakectl treeverse/lakefs \\\n          repo list\n</code></pre> <p>Bear in mind that if you are running lakeFS itself locally you will need to account for this in your networking configuration of  the Docker container. That is to say, <code>localhost</code> to a Docker container is itself, not the host machine on which it is running.</p>"},{"location":"reference/cli/#command-reference","title":"Command Reference","text":""},{"location":"reference/cli/#lakectl","title":"lakectl","text":"<p>A cli tool to explore manage and work with lakeFS</p> Synopsis <p>lakectl is a CLI tool allowing exploration and manipulation of a lakeFS environment.</p> <p>It can be extended with plugins; see 'lakectl plugin --help' for more information.</p> <pre><code>lakectl [flags]\n</code></pre> Options <pre><code>      --base-uri string      base URI used for lakeFS address parse\n  -c, --config string        config file (default is $HOME/.lakectl.yaml)\n  -h, --help                 help for lakectl\n      --log-format string    set logging output format\n      --log-level string     set logging level (default \"none\")\n      --log-output strings   set logging output(s)\n      --no-color             don't use fancy output colors (default value can be set by NO_COLOR environment variable)\n      --verbose              run in verbose mode\n  -v, --version              version for lakectl\n</code></pre> <p>Note</p> <p>The <code>base-uri</code> option can be controlled with the <code>LAKECTL_BASE_URI</code> environment variable.</p> Example usage <pre><code>$ export LAKECTL_BASE_URI=\"lakefs://my-repo/my-branch\"\n# Once set, use relative lakefs uri's:\n$ lakectl fs ls /path\n</code></pre>"},{"location":"reference/cli/#lakectl-actions","title":"lakectl actions","text":"<p>Manage Actions commands</p> Options <pre><code>  -h, --help   help for actions\n</code></pre>"},{"location":"reference/cli/#lakectl-actions-help","title":"lakectl actions help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type actions help [path to command] for full details.</p> <pre><code>lakectl actions help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-actions-runs","title":"lakectl actions runs","text":"<p>Explore runs information</p> Options <pre><code>  -h, --help   help for runs\n</code></pre>"},{"location":"reference/cli/#lakectl-actions-runs-describe","title":"lakectl actions runs describe","text":"<p>Describe run results</p> Synopsis <p>Show information about the run and all the hooks that were executed as part of the run</p> <pre><code>lakectl actions runs describe &lt;repository URI&gt; &lt;run_id&gt; [flags]\n</code></pre> Examples <pre><code>lakectl actions runs describe lakefs://my-repo 20230719152411arS0z6I\n</code></pre> Options <pre><code>      --after string   show results after this value (used for pagination)\n      --amount int     number of results to return. By default, all results are returned.\n  -h, --help           help for describe\n</code></pre>"},{"location":"reference/cli/#lakectl-actions-runs-help","title":"lakectl actions runs help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type runs help [path to command] for full details.</p> <pre><code>lakectl actions runs help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-actions-runs-list","title":"lakectl actions runs list","text":"<p>List runs</p> Synopsis <p>List all runs on a repository optional filter by branch or commit</p> <pre><code>lakectl actions runs list &lt;repository URI&gt; [--branch &lt;branch&gt;] [--commit &lt;commit_id&gt;] [flags]\n</code></pre> Examples <pre><code>lakectl actions runs list lakefs://my-repo --branch my-branch --commit 600dc0ffee\n</code></pre> Options <pre><code>      --branch string   show results for specific branch\n      --commit string   show results for specific commit ID\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-actions-validate","title":"lakectl actions validate","text":"<p>Validate action file</p> Synopsis <p>Tries to parse the input action file as lakeFS action file</p> <pre><code>lakectl actions validate [flags]\n</code></pre> Examples <pre><code>lakectl actions validate path/to/my/file\n</code></pre> Options <pre><code>  -h, --help   help for validate\n</code></pre>"},{"location":"reference/cli/#lakectl-annotate","title":"lakectl annotate","text":"<p>List entries under a given path, annotating each with the latest modifying commit</p> <pre><code>lakectl annotate &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>      --first-parent   follow only the first parent commit upon seeing a merge commit\n  -h, --help           help for annotate\n  -r, --recursive      recursively annotate all entries under a given path or prefix\n</code></pre>"},{"location":"reference/cli/#lakectl-auth","title":"lakectl auth","text":"<p>Manage authentication and authorization</p> Synopsis <p>Manage authentication and authorization including users, groups and ACLs This functionality is supported with an external auth service only.</p> Options <pre><code>  -h, --help   help for auth\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups","title":"lakectl auth groups","text":"<p>Manage groups</p> Options <pre><code>  -h, --help   help for groups\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-acl","title":"lakectl auth groups acl","text":"<p>Manage ACLs</p> Synopsis <p>manage ACLs of groups</p> Options <pre><code>  -h, --help   help for acl\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-acl-get","title":"lakectl auth groups acl get","text":"<p>Get ACL of group</p> <pre><code>lakectl auth groups acl get [flags]\n</code></pre> Options <pre><code>  -h, --help        help for get\n      --id string   Group identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-acl-help","title":"lakectl auth groups acl help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type acl help [path to command] for full details.</p> <pre><code>lakectl auth groups acl help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-acl-set","title":"lakectl auth groups acl set","text":"<p>Set ACL of group</p> Synopsis <p>Set ACL of group. permission will be attached to all repositories.</p> <pre><code>lakectl auth groups acl set [flags]\n</code></pre> Options <pre><code>  -h, --help                help for set\n      --id string           Group identifier\n      --permission string   Permission, typically one of \"Read\", \"Write\", \"Super\" or \"Admin\"\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-create","title":"lakectl auth groups create","text":"<p>Create a group</p> <pre><code>lakectl auth groups create [flags]\n</code></pre> Options <pre><code>  -h, --help        help for create\n      --id string   Group identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-delete","title":"lakectl auth groups delete","text":"<p>Delete a group</p> <pre><code>lakectl auth groups delete [flags]\n</code></pre> Options <pre><code>  -h, --help        help for delete\n      --id string   Group identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-help","title":"lakectl auth groups help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type groups help [path to command] for full details.</p> <pre><code>lakectl auth groups help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-list","title":"lakectl auth groups list","text":"<p>List groups</p> <pre><code>lakectl auth groups list [flags]\n</code></pre> Options <pre><code>      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-members","title":"lakectl auth groups members","text":"<p>Manage group user memberships</p> Options <pre><code>  -h, --help   help for members\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-members-add","title":"lakectl auth groups members add","text":"<p>Add a user to a group</p> <pre><code>lakectl auth groups members add [flags]\n</code></pre> Options <pre><code>  -h, --help          help for add\n      --id string     Group identifier\n      --user string   Username (email for password-based users, default: current user)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-members-help","title":"lakectl auth groups members help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type members help [path to command] for full details.</p> <pre><code>lakectl auth groups members help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-members-list","title":"lakectl auth groups members list","text":"<p>List users in a group</p> <pre><code>lakectl auth groups members list [flags]\n</code></pre> Options <pre><code>      --id string       Group identifier\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-members-remove","title":"lakectl auth groups members remove","text":"<p>Remove a user from a group</p> <pre><code>lakectl auth groups members remove [flags]\n</code></pre> Options <pre><code>  -h, --help          help for remove\n      --id string     Group identifier\n      --user string   Username (email for password-based users, default: current user)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-policies","title":"lakectl auth groups policies","text":"<p>Manage group policies</p> Synopsis <p>Manage group policies.  Requires an external authorization server with matching support.</p> Options <pre><code>  -h, --help   help for policies\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-policies-attach","title":"lakectl auth groups policies attach","text":"<p>Attach a policy to a group</p> <pre><code>lakectl auth groups policies attach [flags]\n</code></pre> Options <pre><code>  -h, --help            help for attach\n      --id string       User identifier\n      --policy string   Policy identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-policies-detach","title":"lakectl auth groups policies detach","text":"<p>Detach a policy from a group</p> <pre><code>lakectl auth groups policies detach [flags]\n</code></pre> Options <pre><code>  -h, --help            help for detach\n      --id string       User identifier\n      --policy string   Policy identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-policies-help","title":"lakectl auth groups policies help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type policies help [path to command] for full details.</p> <pre><code>lakectl auth groups policies help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-policies-list","title":"lakectl auth groups policies list","text":"<p>List policies for the given group</p> <pre><code>lakectl auth groups policies list [flags]\n</code></pre> Options <pre><code>      --id string       Group identifier\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-help","title":"lakectl auth help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type auth help [path to command] for full details.</p> <pre><code>lakectl auth help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-policies","title":"lakectl auth policies","text":"<p>Manage policies</p> Options <pre><code>  -h, --help   help for policies\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-policies-create","title":"lakectl auth policies create","text":"<p>Create a policy</p> <pre><code>lakectl auth policies create [flags]\n</code></pre> Options <pre><code>  -h, --help                        help for create\n      --id string                   Policy identifier\n      --statement-document string   JSON statement document path (or \"-\" for stdin)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-policies-delete","title":"lakectl auth policies delete","text":"<p>Delete a policy</p> <pre><code>lakectl auth policies delete [flags]\n</code></pre> Options <pre><code>  -h, --help        help for delete\n      --id string   Policy identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-policies-help","title":"lakectl auth policies help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type policies help [path to command] for full details.</p> <pre><code>lakectl auth policies help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-policies-list","title":"lakectl auth policies list","text":"<p>List policies</p> <pre><code>lakectl auth policies list [flags]\n</code></pre> Options <pre><code>      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-policies-show","title":"lakectl auth policies show","text":"<p>Show a policy</p> <pre><code>lakectl auth policies show [flags]\n</code></pre> Options <pre><code>  -h, --help        help for show\n      --id string   Policy identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users","title":"lakectl auth users","text":"<p>Manage users</p> Options <pre><code>  -h, --help   help for users\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-aws-iam","title":"lakectl auth users aws-iam","text":"<p>Manage AWS IAM Role for lakeFS Users (External Principals API)</p> Options <pre><code>  -h, --help   help for aws-iam\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-aws-iam-attach","title":"lakectl auth users aws-iam attach","text":"<p>Attach an IAM role to a lakeFS user</p> <pre><code>lakectl auth users aws-iam attach [flags]\n</code></pre> Options <pre><code>  -h, --help                  help for attach\n      --id string             lakeFS Username (default: current user)\n      --principal-id string   External principal ID (e.g., AWS IAM role ARN)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-aws-iam-detach","title":"lakectl auth users aws-iam detach","text":"<p>Detach an IAM Role from a lakeFS user</p> <pre><code>lakectl auth users aws-iam detach [flags]\n</code></pre> Options <pre><code>  -h, --help                  help for detach\n      --id string             lakeFS Username (default: current user)\n      --principal-id string   External principal ID (e.g., AWS IAM role ARN)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-aws-iam-help","title":"lakectl auth users aws-iam help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type aws-iam help [path to command] for full details.</p> <pre><code>lakectl auth users aws-iam help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-aws-iam-list","title":"lakectl auth users aws-iam list","text":"<p>List all IAM roles attached to a lakeFS user</p> <pre><code>lakectl auth users aws-iam list [flags]\n</code></pre> Options <pre><code>      --id string       lakeFS Username (default: current user)\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-aws-iam-lookup","title":"lakectl auth users aws-iam lookup","text":"<p>Lookup IAM Role attachments</p> <pre><code>lakectl auth users aws-iam lookup [flags]\n</code></pre> Options <pre><code>  -h, --help                  help for lookup\n      --principal-id string   External principal ID (e.g., AWS IAM role ARN)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-create","title":"lakectl auth users create","text":"<p>Create a user</p> <pre><code>lakectl auth users create [flags]\n</code></pre> Options <pre><code>  -h, --help        help for create\n      --id string   Username\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-credentials","title":"lakectl auth users credentials","text":"<p>Manage user credentials</p> Options <pre><code>  -h, --help   help for credentials\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-credentials-create","title":"lakectl auth users credentials create","text":"<p>Create user credentials</p> <pre><code>lakectl auth users credentials create [flags]\n</code></pre> Options <pre><code>  -h, --help        help for create\n      --id string   Username (email for password-based users, default: current user)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-credentials-delete","title":"lakectl auth users credentials delete","text":"<p>Delete user credentials</p> <pre><code>lakectl auth users credentials delete [flags]\n</code></pre> Options <pre><code>      --access-key-id string   Access key ID to delete\n  -h, --help                   help for delete\n      --id string              Username (email for password-based users, default: current user)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-credentials-help","title":"lakectl auth users credentials help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type credentials help [path to command] for full details.</p> <pre><code>lakectl auth users credentials help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-credentials-list","title":"lakectl auth users credentials list","text":"<p>List user credentials</p> <pre><code>lakectl auth users credentials list [flags]\n</code></pre> Options <pre><code>      --id string       Username (email for password-based users, default: current user)\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-delete","title":"lakectl auth users delete","text":"<p>Delete a user</p> <pre><code>lakectl auth users delete [flags]\n</code></pre> Options <pre><code>  -h, --help        help for delete\n      --id string   Username (email for password-based users)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-groups","title":"lakectl auth users groups","text":"<p>Manage user groups</p> Options <pre><code>  -h, --help   help for groups\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-groups-help","title":"lakectl auth users groups help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type groups help [path to command] for full details.</p> <pre><code>lakectl auth users groups help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-groups-list","title":"lakectl auth users groups list","text":"<p>List groups for the given user</p> <pre><code>lakectl auth users groups list [flags]\n</code></pre> Options <pre><code>      --id string       Username (email for password-based users)\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-help","title":"lakectl auth users help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type users help [path to command] for full details.</p> <pre><code>lakectl auth users help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-list","title":"lakectl auth users list","text":"<p>List users</p> <pre><code>lakectl auth users list [flags]\n</code></pre> Options <pre><code>      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-policies","title":"lakectl auth users policies","text":"<p>Manage user policies</p> Synopsis <p>Manage user policies.  Requires an external authorization server with matching support.</p> Options <pre><code>  -h, --help   help for policies\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-policies-attach","title":"lakectl auth users policies attach","text":"<p>Attach a policy to a user</p> <pre><code>lakectl auth users policies attach [flags]\n</code></pre> Options <pre><code>  -h, --help            help for attach\n      --id string       Username (email for password-based users)\n      --policy string   Policy identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-policies-detach","title":"lakectl auth users policies detach","text":"<p>Detach a policy from a user</p> <pre><code>lakectl auth users policies detach [flags]\n</code></pre> Options <pre><code>  -h, --help            help for detach\n      --id string       Username (email for password-based users)\n      --policy string   Policy identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-policies-help","title":"lakectl auth users policies help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type policies help [path to command] for full details.</p> <pre><code>lakectl auth users policies help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-policies-list","title":"lakectl auth users policies list","text":"<p>List policies for the given user</p> <pre><code>lakectl auth users policies list [flags]\n</code></pre> Options <pre><code>      --effective       List all distinct policies attached to the user, including by group memberships\n      --id string       Username (email for password-based users)\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-branch","title":"lakectl branch","text":"<p>Create and manage branches within a repository</p> Synopsis <p>Create delete and list branches within a lakeFS repository</p> Options <pre><code>  -h, --help   help for branch\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-create","title":"lakectl branch create","text":"<p>Create a new branch in a repository</p> <pre><code>lakectl branch create &lt;branch URI&gt; -s &lt;source ref URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch create lakefs://example-repo/new-branch -s lakefs://example-repo/main\n</code></pre> Options <pre><code>  -h, --help            help for create\n  -s, --source string   source branch uri\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-delete","title":"lakectl branch delete","text":"<p>Delete a branch in a repository, along with its uncommitted changes (CAREFUL)</p> <pre><code>lakectl branch delete &lt;branch URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch delete lakefs://my-repo/my-branch\n</code></pre> Options <pre><code>  -h, --help   help for delete\n  -y, --yes    Automatically say yes to all confirmations\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-help","title":"lakectl branch help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type branch help [path to command] for full details.</p> <pre><code>lakectl branch help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-list","title":"lakectl branch list","text":"<p>List branches in a repository</p> <pre><code>lakectl branch list &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch list lakefs://my-repo\n</code></pre> Options <pre><code>      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-reset","title":"lakectl branch reset","text":"<p>Reset uncommitted changes - all of them, or by path</p> Synopsis <p>reset changes.  There are four different ways to reset changes:   1. reset all uncommitted changes - reset lakefs://myrepo/main    2. reset uncommitted changes under specific path - reset lakefs://myrepo/main --prefix path   3. reset uncommitted changes for specific object - reset lakefs://myrepo/main --object path</p> <pre><code>lakectl branch reset &lt;branch URI&gt; [--prefix|--object] [flags]\n</code></pre> Examples <pre><code>lakectl branch reset lakefs://my-repo/my-branch\n</code></pre> Options <pre><code>  -h, --help            help for reset\n      --object string   path to object to be reset\n      --prefix string   prefix of the objects to be reset\n  -y, --yes             Automatically say yes to all confirmations\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-revert","title":"lakectl branch revert","text":"<p>Given a commit, record a new commit to reverse the effect of this commit</p> Synopsis <p>The commits will be reverted in left-to-right order</p> <pre><code>lakectl branch revert &lt;branch URI&gt; &lt;commit ref to revert&gt; [&lt;more commits&gt;...] [flags]\n</code></pre> Examples <pre><code>lakectl branch revert lakefs://example-repo/example-branch commitA\n              Revert the changes done by commitA in example-branch\n              branch revert lakefs://example-repo/example-branch HEAD~1 HEAD~2 HEAD~3\n              Revert the changes done by the second last commit to the fourth last commit in example-branch\n</code></pre> Options <pre><code>      --allow-empty-commit   allow empty commit (revert without changes)\n  -h, --help                 help for revert\n  -m, --parent-number int    the parent number (starting from 1) of the mainline. The revert will reverse the change relative to the specified parent.\n  -y, --yes                  Automatically say yes to all confirmations\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-show","title":"lakectl branch show","text":"<p>Show branch latest commit reference</p> <pre><code>lakectl branch show &lt;branch URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch show lakefs://my-repo/my-branch\n</code></pre> Options <pre><code>  -h, --help   help for show\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-protect","title":"lakectl branch-protect","text":"<p>Create and manage branch protection rules</p> Synopsis <p>Define branch protection rules to prevent direct changes. Changes to protected branches can only be done by merging from other branches.</p> Options <pre><code>  -h, --help   help for branch-protect\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-protect-add","title":"lakectl branch-protect add","text":"<p>Add a branch protection rule</p> Synopsis <p>Add a branch protection rule for a given branch name pattern</p> <pre><code>lakectl branch-protect add &lt;repository URI&gt; &lt;pattern&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch-protect add lakefs://my-repo 'stable_*'\n</code></pre> Options <pre><code>  -h, --help   help for add\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-protect-delete","title":"lakectl branch-protect delete","text":"<p>Delete a branch protection rule</p> Synopsis <p>Delete a branch protection rule for a given branch name pattern</p> <pre><code>lakectl branch-protect delete &lt;repository URI&gt; &lt;pattern&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch-protect delete lakefs://my-repo stable_*\n</code></pre> Options <pre><code>  -h, --help   help for delete\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-protect-help","title":"lakectl branch-protect help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type branch-protect help [path to command] for full details.</p> <pre><code>lakectl branch-protect help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-protect-list","title":"lakectl branch-protect list","text":"<p>List all branch protection rules</p> <pre><code>lakectl branch-protect list &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch-protect list lakefs://my-repo\n</code></pre> Options <pre><code>  -h, --help   help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-cherry-pick","title":"lakectl cherry-pick","text":"<p>Apply the changes introduced by an existing commit</p> Synopsis <p>Apply the changes from the given commit to the tip of the branch. The changes will be added as a new commit.</p> <pre><code>lakectl cherry-pick &lt;commit URI&gt; &lt;branch&gt; [flags]\n</code></pre> Examples <pre><code>lakectl cherry-pick lakefs://my-repo/600dc0ffee lakefs://my-repo/my-branch\n</code></pre> Options <pre><code>  -h, --help                help for cherry-pick\n  -m, --parent-number int   the parent number (starting from 1) of the cherry-picked commit. The cherry-pick will apply the change relative to the specified parent.\n</code></pre>"},{"location":"reference/cli/#lakectl-commit","title":"lakectl commit","text":"<p>Commit changes on a given branch</p> <pre><code>lakectl commit &lt;branch URI&gt; [flags]\n</code></pre> Options <pre><code>      --allow-empty-commit    allow a commit with no changes\n      --allow-empty-message   allow an empty commit message\n  -h, --help                  help for commit\n  -m, --message string        commit message\n      --meta strings          key value pair in the form of key=value\n</code></pre>"},{"location":"reference/cli/#lakectl-completion","title":"lakectl completion","text":"<p>Generate completion script</p> Synopsis <p>To load completions:</p> <p>Bash:</p> <pre><code>$ source &lt;(lakectl completion bash)\n</code></pre> <p>To load completions for each session, execute once: Linux:</p> <pre><code>$ lakectl completion bash &gt; /etc/bash_completion.d/lakectl\n</code></pre> <p>MacOS:</p> <pre><code>$ lakectl completion bash &gt; /usr/local/etc/bash_completion.d/lakectl\n</code></pre> <p>Zsh:</p> <p>If shell completion is not already enabled in your environment you will need to enable it.  You can execute the following once:</p> <pre><code>$ echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc\n</code></pre> <p>To load completions for each session, execute once: <pre><code>$ lakectl completion zsh &gt; \"${fpath[1]}/_lakectl\"\n</code></pre></p> <p>You will need to start a new shell for this setup to take effect.</p> <p>Fish:</p> <pre><code>$ lakectl completion fish | source\n</code></pre> <p>To load completions for each session, execute once:</p> <pre><code>$ lakectl completion fish &gt; ~/.config/fish/completions/lakectl.fish\n</code></pre> <pre><code>lakectl completion &lt;bash|zsh|fish&gt;\n</code></pre> Options <pre><code>  -h, --help   help for completion\n</code></pre>"},{"location":"reference/cli/#lakectl-config","title":"lakectl config","text":"<p>Create/update local lakeFS configuration</p> <pre><code>lakectl config [flags]\n</code></pre> Options <pre><code>  -h, --help   help for config\n</code></pre>"},{"location":"reference/cli/#lakectl-diff","title":"lakectl diff","text":"<p>Show changes between two commits, or the currently uncommitted changes</p> <pre><code>lakectl diff &lt;ref URI&gt; [ref URI] [flags]\n</code></pre> Examples <pre><code>    lakectl diff lakefs://example-repo/example-branch\n    Show uncommitted changes in example-branch.\n\n    lakectl diff lakefs://example-repo/main lakefs://example-repo/dev\n    This shows the differences between master and dev starting at the last common commit.\n    This is similar to the three-dot (...) syntax in git.\n    Uncommitted changes are not shown.\n\n    lakectl diff --two-way lakefs://example-repo/main lakefs://example-repo/dev\n    Show changes between the tips of the main and dev branches.\n    This is similar to the two-dot (..) syntax in git.\n    Uncommitted changes are not shown.\n\n    lakectl diff --two-way lakefs://example-repo/main lakefs://example-repo/dev$\n    Show changes between the tip of the main and the dev branch, including uncommitted changes on dev.\n\n    lakectl diff --prefix some/path lakefs://example-repo/main lakefs://example-repo/dev\n    Show changes of objects prefixed with 'some/path' between the tips of the main and dev branches.\n</code></pre> Options <pre><code>  -h, --help            help for diff\n      --prefix string   Show only changes in the given prefix.\n      --two-way         Use two-way diff: show difference between the given refs, regardless of a common ancestor.\n</code></pre>"},{"location":"reference/cli/#lakectl-doctor","title":"lakectl doctor","text":"<p>Run a basic diagnosis of the LakeFS configuration</p> <pre><code>lakectl doctor [flags]\n</code></pre> Options <pre><code>  -h, --help   help for doctor\n</code></pre>"},{"location":"reference/cli/#lakectl-fs","title":"lakectl fs","text":"<p>View and manipulate objects</p> Options <pre><code>  -h, --help   help for fs\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-cat","title":"lakectl fs cat","text":"<p>Dump content of object to stdout</p> <pre><code>lakectl fs cat &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help       help for cat\n      --pre-sign   Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-download","title":"lakectl fs download","text":"<p>Download object(s) from a given repository path</p> <pre><code>lakectl fs download &lt;path URI&gt; [&lt;destination path&gt;] [flags]\n</code></pre> Options <pre><code>  -h, --help              help for download\n      --no-progress       Disable progress bar animation for IO operations\n  -p, --parallelism int   Max concurrent operations to perform (default 25)\n      --part-size int     part size in bytes for multipart download (default 8388608)\n      --pre-sign          Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n  -r, --recursive         recursively download all objects under path\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-help","title":"lakectl fs help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type fs help [path to command] for full details.</p> <pre><code>lakectl fs help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-ls","title":"lakectl fs ls","text":"<p>List entries under a given tree</p> <pre><code>lakectl fs ls &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help        help for ls\n  -r, --recursive   list all objects under the specified path\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-presign","title":"lakectl fs presign","text":"<p>return a pre-signed URL for reading the specified object</p> <pre><code>lakectl fs presign &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help   help for presign\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-rm","title":"lakectl fs rm","text":"<p>Delete object</p> <pre><code>lakectl fs rm &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>  -C, --concurrency int   max concurrent single delete operations to send to the lakeFS server (default 50)\n  -h, --help              help for rm\n  -r, --recursive         recursively delete all objects under the specified path\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-stat","title":"lakectl fs stat","text":"<p>View object metadata</p> <pre><code>lakectl fs stat &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help       help for stat\n      --pre-sign   Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-upload","title":"lakectl fs upload","text":"<p>Upload a local file to the specified URI</p> <pre><code>lakectl fs upload &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>      --content-type string   MIME type of contents\n  -h, --help                  help for upload\n      --no-progress           Disable progress bar animation for IO operations\n  -p, --parallelism int       Max concurrent operations to perform (default 25)\n      --pre-sign              Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n  -r, --recursive             recursively copy all files under local source\n  -s, --source string         local file to upload, or \"-\" for stdin\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-stage","title":"lakectl fs stage","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Link an external object with a path in a repository</p> Synopsis <p>Link an external object with a path in a repository, creating an uncommitted change. The object location must be outside the repository's storage namespace</p> <pre><code>lakectl fs stage &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>      --checksum string       Object MD5 checksum as a hexadecimal string\n      --content-type string   MIME type of contents\n  -h, --help                  help for stage\n      --location string       fully qualified storage location (i.e. \"s3://bucket/path/to/object\")\n      --meta strings          key value pairs in the form of key=value\n      --mtime int             Object modified time (Unix Epoch in seconds). Defaults to current time\n      --size int              Object size in bytes\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-update-metadata","title":"lakectl fs update-metadata","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Update user metadata on the specified URI</p> <pre><code>lakectl fs update-metadata &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help               help for update-metadata\n      --metadata strings   Metadata to set, in the form key1=value1,key2=value2\n</code></pre>"},{"location":"reference/cli/#lakectl-gc","title":"lakectl gc","text":"<p>Manage the garbage collection policy</p> Options <pre><code>  -h, --help   help for gc\n</code></pre>"},{"location":"reference/cli/#lakectl-gc-delete-config","title":"lakectl gc delete-config","text":"<p>Deletes the garbage collection policy for the repository</p> <pre><code>lakectl gc delete-config &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl gc delete-config lakefs://my-repo\n</code></pre> Options <pre><code>  -h, --help   help for delete-config\n</code></pre>"},{"location":"reference/cli/#lakectl-gc-get-config","title":"lakectl gc get-config","text":"<p>Show the garbage collection policy for this repository</p> <pre><code>lakectl gc get-config &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl gc get-config lakefs://my-repo\n</code></pre> Options <pre><code>  -h, --help   help for get-config\n  -p, --json   get rules as JSON\n</code></pre>"},{"location":"reference/cli/#lakectl-gc-help","title":"lakectl gc help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type gc help [path to command] for full details.</p> <pre><code>lakectl gc help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-gc-set-config","title":"lakectl gc set-config","text":"<p>Set garbage collection policy JSON</p> Synopsis <p>Sets the garbage collection policy JSON. Example configuration file: {   \"default_retention_days\": 21,   \"branches\": [     {       \"branch_id\": \"main\",       \"retention_days\": 28     },     {       \"branch_id\": \"dev\",       \"retention_days\": 14     }   ] }</p> <pre><code>lakectl gc set-config &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl gc set-config lakefs://my-repo -f config.json\n</code></pre> Options <pre><code>  -f, --filename string   file containing the GC policy as JSON\n  -h, --help              help for set-config\n</code></pre>"},{"location":"reference/cli/#lakectl-gc-check-async","title":"lakectl gc check-async","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Check status of (async) PrepareGarbageCollectionCommits</p> <pre><code>lakectl gc check-async &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl gc check-async --id &lt;ID&gt; lakefs://my-repo\n</code></pre> Options <pre><code>  -h, --help        help for check-async\n      --id string   ID returned from \"gc prepare-async\"\n</code></pre>"},{"location":"reference/cli/#lakectl-gc-prepare-async","title":"lakectl gc prepare-async","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Runs (async) PrepareGarbageCollectionCommits on the repository</p> <pre><code>lakectl gc prepare-async &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl gc prepare-async lakefs://my-repo\n</code></pre> Options <pre><code>  -h, --help   help for prepare-async\n</code></pre>"},{"location":"reference/cli/#lakectl-help","title":"lakectl help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type lakectl help [path to command] for full details.</p> <pre><code>lakectl help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-identity","title":"lakectl identity","text":"<p>Show identity info</p> Synopsis <p>Show the info of the configured user in lakectl</p> <pre><code>lakectl identity [flags]\n</code></pre> Examples <pre><code>lakectl identity\n</code></pre> Options <pre><code>  -h, --help   help for identity\n</code></pre>"},{"location":"reference/cli/#lakectl-import","title":"lakectl import","text":"<p>Import data from external source to a destination branch</p> <pre><code>lakectl import --from &lt;object store URI&gt; --to &lt;lakeFS path URI&gt; [flags]\n</code></pre> Options <pre><code>      --allow-empty-message   allow an empty commit message (default true)\n      --from string           prefix to read from (e.g. \"s3://bucket/sub/path/\"). must not be in a storage namespace\n  -h, --help                  help for import\n  -m, --message string        commit message\n      --meta strings          key value pair in the form of key=value\n      --no-progress           switch off the progress output\n      --to string             lakeFS path to load objects into (e.g. \"lakefs://repo/branch/sub/path/\")\n</code></pre>"},{"location":"reference/cli/#lakectl-local","title":"lakectl local","text":"<p>Sync local directories with lakeFS paths</p> Options <pre><code>  -h, --help   help for local\n</code></pre>"},{"location":"reference/cli/#lakectl-local-checkout","title":"lakectl local checkout","text":"<p>Sync local directory with the remote state.</p> <pre><code>lakectl local checkout [directory] [flags]\n</code></pre> Options <pre><code>      --all               Checkout given source branch or reference for all linked directories\n  -h, --help              help for checkout\n      --no-progress       Disable progress bar animation for IO operations\n  -p, --parallelism int   Max concurrent operations to perform (default 25)\n      --pre-sign          Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n  -r, --ref string        Checkout the given reference\n  -y, --yes               Automatically say yes to all confirmations\n</code></pre>"},{"location":"reference/cli/#lakectl-local-clone","title":"lakectl local clone","text":"<p>Clone a path from a lakeFS repository into a new directory.</p> <pre><code>lakectl local clone &lt;path URI&gt; [directory] [flags]\n</code></pre> Options <pre><code>      --gitignore         Update .gitignore file when working in a git repository context (default true)\n  -h, --help              help for clone\n      --no-progress       Disable progress bar animation for IO operations\n  -p, --parallelism int   Max concurrent operations to perform (default 25)\n      --pre-sign          Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n</code></pre>"},{"location":"reference/cli/#lakectl-local-commit","title":"lakectl local commit","text":"<p>Commit changes from local directory to the lakeFS branch it tracks.</p> <pre><code>lakectl local commit [directory] [flags]\n</code></pre> Options <pre><code>      --allow-empty-message   allow an empty commit message\n      --force                 Commit changes even if remote branch includes uncommitted changes external to the synced path\n  -h, --help                  help for commit\n  -m, --message string        commit message\n      --meta strings          key value pair in the form of key=value\n      --no-progress           Disable progress bar animation for IO operations\n  -p, --parallelism int       Max concurrent operations to perform (default 25)\n      --pre-sign              Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n</code></pre>"},{"location":"reference/cli/#lakectl-local-help","title":"lakectl local help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type local help [path to command] for full details.</p> <pre><code>lakectl local help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-local-init","title":"lakectl local init","text":"<p>set a local directory to sync with a lakeFS path.</p> <pre><code>lakectl local init &lt;path URI&gt; [directory] [flags]\n</code></pre> Options <pre><code>      --force       Overwrites if directory already linked to a lakeFS path\n      --gitignore   Update .gitignore file when working in a git repository context (default true)\n  -h, --help        help for init\n</code></pre>"},{"location":"reference/cli/#lakectl-local-list","title":"lakectl local list","text":"<p>find and list directories that are synced with lakeFS.</p> <pre><code>lakectl local list [directory] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-local-pull","title":"lakectl local pull","text":"<p>Fetch latest changes from lakeFS.</p> <pre><code>lakectl local pull [directory] [flags]\n</code></pre> Options <pre><code>      --force             Reset any uncommitted local change\n  -h, --help              help for pull\n      --no-progress       Disable progress bar animation for IO operations\n  -p, --parallelism int   Max concurrent operations to perform (default 25)\n      --pre-sign          Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n</code></pre>"},{"location":"reference/cli/#lakectl-local-status","title":"lakectl local status","text":"<p>show modifications (both remote and local) to the directory and the remote location it tracks</p> <pre><code>lakectl local status [directory] [flags]\n</code></pre> Options <pre><code>  -h, --help    help for status\n  -l, --local   Don't compare against remote changes\n</code></pre>"},{"location":"reference/cli/#lakectl-log","title":"lakectl log","text":"<p>Show log of commits</p> Synopsis <p>Show log of commits for a given reference</p> <pre><code>lakectl log &lt;ref URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl log --dot lakefs://example-repository/main | dot -Tsvg &gt; graph.svg\n</code></pre> Options <pre><code>      --after string         show results after this value (used for pagination)\n      --amount int           number of results to return. By default, all results are returned\n      --dot                  return results in a dotgraph format\n      --first-parent         follow only the first parent commit upon seeing a merge commit\n  -h, --help                 help for log\n      --limit                limit result just to amount. By default, returns whether more items are available.\n      --no-merges            skip merge commits\n      --objects strings      show results that contains changes to at least one path in that list of objects. Use comma separator to pass all objects together\n      --prefixes strings     show results that contains changes to at least one path in that list of prefixes. Use comma separator to pass all prefixes together\n      --show-meta-range-id   also show meta range ID\n      --since string         show results since this date-time (RFC3339 format)\n      --stop-at string       a Ref to stop at (included in results)\n</code></pre>"},{"location":"reference/cli/#lakectl-login","title":"lakectl login","text":"<p>Use a web browser to log into lakeFS</p> Synopsis <p>Connect to lakeFS using a web browser.</p> <pre><code>lakectl login [flags]\n</code></pre> Examples <pre><code>lakectl login\n</code></pre> Options <pre><code>  -h, --help   help for login\n</code></pre>"},{"location":"reference/cli/#lakectl-merge","title":"lakectl merge","text":"<p>Merge &amp; commit changes from source branch into destination branch</p> Synopsis <p>Merge &amp; commit changes from source branch into destination branch</p> <pre><code>lakectl merge &lt;source ref&gt; &lt;destination ref&gt; [flags]\n</code></pre> Options <pre><code>      --allow-empty           Allow merge when the branches have the same content\n      --allow-empty-message   allow an empty commit message (default true)\n      --force                 Allow merge into a read-only branch or into a branch with the same content\n  -h, --help                  help for merge\n  -m, --message string        commit message\n      --meta strings          key value pair in the form of key=value\n      --squash                Squash all changes from source into a single commit on destination\n      --strategy string       In case of a merge conflict, this option will force the merge process to automatically favor changes from the dest branch (\"dest-wins\") or from the source branch(\"source-wins\"). In case no selection is made, the merge process will fail in case of a conflict\n</code></pre>"},{"location":"reference/cli/#lakectl-plugin","title":"lakectl plugin","text":"<p>Manage lakectl plugins</p> Synopsis <p>Provides utilities for managing lakectl plugins.</p> <p>Plugins are standalone executable files that extend lakectl's functionality. lakectl discovers plugins by looking for executables in your PATH that are named with the prefix \"lakectl-\".</p> <p>For example, an executable named \"lakectl-myfeature\" can be invoked as \"lakectl myfeature [args...]\".</p> <p>Plugin Naming:   - The executable must start with \"lakectl-\".   - The part after \"lakectl-\" becomes the command name users type.     (e.g., \"lakectl-foo\" -&gt; \"lakectl foo\")   - The plugin name is used exactly as-is in the command.     (e.g., \"lakectl-foo-bar\" -&gt; \"lakectl foo-bar\")</p> <p>Installation:   - Place your \"lakectl-...\" executable file (which may be any executable,     e.g. a Python application) in a directory listed in your PATH.   - Ensure the file has execute permissions.</p> <p>Execution:   - When you run \"lakectl some-plugin arg1 --flag\", lakectl searches for     \"lakectl-some-plugin\" in PATH.   - If found and executable, it runs the plugin, passing \"arg1 --flag\" as arguments.   - The plugin inherits environment variables from lakectl.   - Standard output, standard error, and the exit code of the plugin are propagated.   - Built-in lakectl commands always take precedence over plugins.</p> <p>Use \"lakectl plugin list\" to see all detected plugins and any warnings.</p> Options <pre><code>  -h, --help   help for plugin\n</code></pre>"},{"location":"reference/cli/#lakectl-plugin-help","title":"lakectl plugin help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type plugin help [path to command] for full details.</p> <pre><code>lakectl plugin help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-plugin-list","title":"lakectl plugin list","text":"<p>List available lakectl plugins</p> Synopsis <p>Scans the PATH for executables named \"lakectl-*\" and lists the detected plugins.</p> <pre><code>lakectl plugin list [flags]\n</code></pre> Options <pre><code>  -h, --help   help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-repo","title":"lakectl repo","text":"<p>Manage and explore repos</p> Options <pre><code>  -h, --help   help for repo\n</code></pre>"},{"location":"reference/cli/#lakectl-repo-create","title":"lakectl repo create","text":"<p>Create a new repository</p> <pre><code>lakectl repo create &lt;repository URI&gt; &lt;storage namespace&gt; [flags]\n</code></pre> Examples <pre><code>lakectl repo create lakefs://my-repo s3://my-bucket\n</code></pre> Options <pre><code>  -d, --default-branch string   the default branch of this repository (default \"main\")\n  -h, --help                    help for create\n      --sample-data             create sample data in the repository\n</code></pre>"},{"location":"reference/cli/#lakectl-repo-delete","title":"lakectl repo delete","text":"<p>Delete existing repository</p> <pre><code>lakectl repo delete &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl repo delete lakefs://my-repo\n</code></pre> Options <pre><code>  -h, --help   help for delete\n  -y, --yes    Automatically say yes to all confirmations\n</code></pre>"},{"location":"reference/cli/#lakectl-repo-help","title":"lakectl repo help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type repo help [path to command] for full details.</p> <pre><code>lakectl repo help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-repo-list","title":"lakectl repo list","text":"<p>List repositories</p> <pre><code>lakectl repo list [flags]\n</code></pre> Options <pre><code>      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-repo-create-bare","title":"lakectl repo create-bare","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Create a new repository with no initial branch or commit</p> <pre><code>lakectl repo create-bare &lt;repository URI&gt; &lt;storage namespace&gt; [flags]\n</code></pre> Examples <pre><code>lakectl repo create-bare lakefs://my-repo s3://my-bucket\n</code></pre> Options <pre><code>  -d, --default-branch string   the default branch name of this repository (will not be created) (default \"main\")\n  -h, --help                    help for create-bare\n</code></pre>"},{"location":"reference/cli/#lakectl-show","title":"lakectl show","text":"<p>See detailed information about an entity</p> Options <pre><code>  -h, --help   help for show\n</code></pre>"},{"location":"reference/cli/#lakectl-show-commit","title":"lakectl show commit","text":"<p>See detailed information about a commit</p> <pre><code>lakectl show commit &lt;commit URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help                 help for commit\n      --show-meta-range-id   show meta range ID\n</code></pre>"},{"location":"reference/cli/#lakectl-show-help","title":"lakectl show help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type show help [path to command] for full details.</p> <pre><code>lakectl show help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-tag","title":"lakectl tag","text":"<p>Create and manage tags within a repository</p> Synopsis <p>Create delete and list tags within a lakeFS repository</p> Options <pre><code>  -h, --help   help for tag\n</code></pre>"},{"location":"reference/cli/#lakectl-tag-create","title":"lakectl tag create","text":"<p>Create a new tag in a repository</p> <pre><code>lakectl tag create &lt;tag URI&gt; &lt;commit URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl tag create lakefs://example-repo/example-tag lakefs://example-repo/2397cc9a9d04c20a4e5739b42c1dd3d8ba655c0b3a3b974850895a13d8bf9917\n</code></pre> Options <pre><code>  -f, --force   override the tag if it exists\n  -h, --help    help for create\n</code></pre>"},{"location":"reference/cli/#lakectl-tag-delete","title":"lakectl tag delete","text":"<p>Delete a tag from a repository</p> <pre><code>lakectl tag delete &lt;tag URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help   help for delete\n</code></pre>"},{"location":"reference/cli/#lakectl-tag-help","title":"lakectl tag help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type tag help [path to command] for full details.</p> <pre><code>lakectl tag help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-tag-list","title":"lakectl tag list","text":"<p>List tags in a repository</p> <pre><code>lakectl tag list &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl tag list lakefs://my-repo\n</code></pre> Options <pre><code>      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-tag-show","title":"lakectl tag show","text":"<p>Show tag's commit reference</p> <pre><code>lakectl tag show &lt;tag URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help   help for show\n</code></pre>"},{"location":"reference/cli/#undocumented-commands","title":"Undocumented commands","text":"<p>Warning</p> <p>These commands are plumbing commands and for internal use only. Avoid using them unless you're really sure you know what you're doing, or have been in contact with lakeFS support!</p>"},{"location":"reference/cli/#lakectl-abuse","title":"lakectl abuse","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Abuse a running lakeFS instance. See sub commands for more info.</p> Options <pre><code>  -h, --help   help for abuse\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-commit","title":"lakectl abuse commit","text":"<p>Commits to the source branch repeatedly</p> <pre><code>lakectl abuse commit &lt;branch URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int     amount of commits to do (default 100)\n      --gap duration   duration to wait between commits (default 2s)\n  -h, --help           help for commit\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-create-branches","title":"lakectl abuse create-branches","text":"<p>Create a lot of branches very quickly.</p> <pre><code>lakectl abuse create-branches &lt;source ref URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int             amount of things to do (default 1000000)\n      --branch-prefix string   prefix to create branches under (default \"abuse-\")\n      --clean-only             only clean up past runs\n  -h, --help                   help for create-branches\n      --parallelism int        amount of things to do in parallel (default 100)\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-help","title":"lakectl abuse help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type abuse help [path to command] for full details.</p> <pre><code>lakectl abuse help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-link-same-object","title":"lakectl abuse link-same-object","text":"<p>Link the same object in parallel.</p> <pre><code>lakectl abuse link-same-object &lt;branch URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int        amount of link object to do (default 1000000)\n  -h, --help              help for link-same-object\n      --key string        key used for the test (default \"linked-object\")\n      --parallelism int   amount of link object to do in parallel (default 100)\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-list","title":"lakectl abuse list","text":"<p>List from the source ref</p> <pre><code>lakectl abuse list &lt;source ref URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int        amount of lists to do (default 1000000)\n  -h, --help              help for list\n      --parallelism int   amount of lists to do in parallel (default 100)\n      --prefix string     prefix to list under (default \"abuse/\")\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-merge","title":"lakectl abuse merge","text":"<p>Merge non-conflicting objects to the source branch in parallel</p> <pre><code>lakectl abuse merge &lt;branch URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int        amount of merges to perform (default 1000)\n  -h, --help              help for merge\n      --parallelism int   number of merges to perform in parallel (default 100)\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-random-delete","title":"lakectl abuse random-delete","text":"<p>Delete keys from a file and generate random delete from the source ref for those keys.</p> <pre><code>lakectl abuse random-delete &lt;source ref URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int         amount of reads to do (default 1000000)\n      --from-file string   read keys from this file (\"-\" for stdin)\n  -h, --help               help for random-delete\n      --parallelism int    amount of reads to do in parallel (default 100)\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-random-read","title":"lakectl abuse random-read","text":"<p>Read keys from a file and generate random reads from the source ref for those keys.</p> <pre><code>lakectl abuse random-read &lt;source ref URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int         amount of reads to do (default 1000000)\n      --from-file string   read keys from this file (\"-\" for stdin)\n  -h, --help               help for random-read\n      --parallelism int    amount of reads to do in parallel (default 100)\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-random-write","title":"lakectl abuse random-write","text":"<p>Generate random writes to the source branch</p> <pre><code>lakectl abuse random-write &lt;branch URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int        amount of writes to do (default 1000000)\n  -h, --help              help for random-write\n      --parallelism int   amount of writes to do in parallel (default 100)\n      --prefix string     prefix to create paths under (default \"abuse/\")\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect","title":"lakectl bisect","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Binary search to find the commit that introduced a bug</p> Options <pre><code>  -h, --help   help for bisect\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-bad","title":"lakectl bisect bad","text":"<p>Set 'bad' commit that is known to contain the bug</p> <pre><code>lakectl bisect bad [flags]\n</code></pre> Options <pre><code>  -h, --help   help for bad\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-good","title":"lakectl bisect good","text":"<p>Set current commit as 'good' commit that is known to be before the bug was introduced</p> <pre><code>lakectl bisect good [flags]\n</code></pre> Options <pre><code>  -h, --help   help for good\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-help","title":"lakectl bisect help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type bisect help [path to command] for full details.</p> <pre><code>lakectl bisect help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-log","title":"lakectl bisect log","text":"<p>Print out the current bisect state</p> <pre><code>lakectl bisect log [flags]\n</code></pre> Options <pre><code>  -h, --help   help for log\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-reset","title":"lakectl bisect reset","text":"<p>Clean up the bisection state</p> <pre><code>lakectl bisect reset [flags]\n</code></pre> Options <pre><code>  -h, --help   help for reset\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-run","title":"lakectl bisect run","text":"<p>Bisecting based on command status code</p> <pre><code>lakectl bisect run &lt;command&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help   help for run\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-start","title":"lakectl bisect start","text":"<p>Start a bisect session</p> <pre><code>lakectl bisect start &lt;bad ref URI&gt; &lt;good ref URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help   help for start\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-view","title":"lakectl bisect view","text":"<p>Current bisect commits</p> <pre><code>lakectl bisect view [flags]\n</code></pre> Options <pre><code>  -h, --help   help for view\n</code></pre>"},{"location":"reference/cli/#lakectl-cat-hook-output","title":"lakectl cat-hook-output","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Cat actions hook output</p> <pre><code>lakectl cat-hook-output &lt;repository URI&gt; &lt;run_id&gt; &lt;hook_id&gt; [flags]\n</code></pre> Examples <pre><code>lakectl cat-hook-output lakefs://my-repo 20230719152411arS0z6I my_hook_name\n</code></pre> Options <pre><code>  -h, --help   help for cat-hook-output\n</code></pre>"},{"location":"reference/cli/#lakectl-cat-sst","title":"lakectl cat-sst","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Explore lakeFS .sst files</p> <pre><code>lakectl cat-sst &lt;sst-file&gt; [flags]\n</code></pre> Options <pre><code>      --amount int    how many records to return, or -1 for all records (default -1)\n  -f, --file string   path to an sstable file, or \"-\" for stdin\n  -h, --help          help for cat-sst\n</code></pre>"},{"location":"reference/cli/#lakectl-docs","title":"lakectl docs","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <pre><code>lakectl docs [outfile] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for docs\n</code></pre>"},{"location":"reference/cli/#lakectl-find-merge-base","title":"lakectl find-merge-base","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Find the commits for the merge operation</p> <pre><code>lakectl find-merge-base &lt;source ref URI&gt; &lt;destination ref URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help   help for find-merge-base\n</code></pre>"},{"location":"reference/cli/#lakectl-refs-dump","title":"lakectl refs-dump","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Dumps refs (branches, commits, tags) to the underlying object store</p> <pre><code>lakectl refs-dump &lt;repository URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help                     help for refs-dump\n  -o, --output string            output filename (default stdout)\n      --poll-interval duration   poll status check interval (default 3s)\n      --timeout duration         timeout for polling status checks (default 1h0m0s)\n</code></pre>"},{"location":"reference/cli/#lakectl-refs-restore","title":"lakectl refs-restore","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Restores refs (branches, commits, tags) from the underlying object store to a bare repository</p> Synopsis <p>restores refs (branches, commits, tags) from the underlying object store to a bare repository.</p> <p>This command is expected to run on a bare repository (i.e. one created with 'lakectl repo create-bare'). Since a bare repo is expected, in case of transient failure, delete the repository and recreate it as bare and retry.</p> <pre><code>lakectl refs-restore &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>aws s3 cp s3://bucket/_lakefs/refs_manifest.json - | lakectl refs-restore lakefs://my-bare-repository --manifest -\n</code></pre> Options <pre><code>  -h, --help                     help for refs-restore\n      --manifest refs-dump       path to a refs manifest json file (as generated by refs-dump). Alternatively, use \"-\" to read from stdin\n      --poll-interval duration   poll status check interval (default 3s)\n      --timeout duration         timeout for polling status checks (default 1h0m0s)\n</code></pre>"},{"location":"reference/cli/#lakectl-usage","title":"lakectl usage","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Usage reports from lakeFS</p> Options <pre><code>  -h, --help   help for usage\n</code></pre>"},{"location":"reference/cli/#lakectl-usage-help","title":"lakectl usage help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type usage help [path to command] for full details.</p> <pre><code>lakectl usage help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-usage-summary","title":"lakectl usage summary","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Summary reports from lakeFS</p> <pre><code>lakectl usage summary [flags]\n</code></pre> Options <pre><code>  -h, --help   help for summary\n</code></pre>"},{"location":"reference/configuration/","title":"lakeFS Server Configuration","text":"<p>Configuring lakeFS is done using a YAML configuration file and/or environment variable. The configuration file's location can be set with the '--config' flag. If not specified, the first file found in the following order will be used:</p> <ol> <li><code>./config.yaml</code></li> <li><code>$HOME/lakefs/config.yaml</code></li> <li><code>/etc/lakefs/config.yaml</code></li> <li><code>$HOME/.lakefs.yaml</code></li> </ol> <p>Configuration items can each be controlled by an environment variable. The variable name will have a prefix of <code>LAKEFS_</code>, followed by the name of the configuration, replacing every <code>.</code> with a <code>_</code>.</p> <p>Example</p> <p><code>LAKEFS_LOGGING_LEVEL</code> controls <code>logging.level</code>.</p> <p>This reference uses <code>.</code> to denote the nesting of values.</p>"},{"location":"reference/configuration/#reference","title":"Reference","text":"<ul> <li><code>listen_address</code> <code>(string : \"0.0.0.0:8000\")</code> - A <code>&lt;host&gt;:&lt;port&gt;</code> structured string representing the address to listen on</li> </ul>"},{"location":"reference/configuration/#logging","title":"logging","text":"<ul> <li><code>logging.format</code> <code>(one of [\"json\", \"text\"] : \"text\")</code> - Format to output log message in</li> <li><code>logging.level</code> <code>(one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"INFO\")</code> - Logging level to output</li> <li> <p><code>logging.audit_log_level</code> <code>(one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"DEBUG\")</code> - Audit logs level to output.</p> <p>Note</p> <p>In case you configure this field to be lower than the main logger level, you won't be able to get the audit logs</p> </li> <li> <p><code>logging.output</code> <code>(string : \"-\")</code> - A path or paths to write logs to. A <code>-</code> means the standard output, <code>=</code> means the standard error.</p> </li> <li><code>logging.file_max_size_mb</code> <code>(int : 100)</code> - Output file maximum size in megabytes.</li> <li><code>logging.files_keep</code> <code>(int : 0)</code> - Number of log files to keep, default is all.</li> </ul>"},{"location":"reference/configuration/#actions","title":"actions","text":"<ul> <li><code>actions.enabled</code> <code>(bool : true)</code> - Setting this to false will block hooks from being executed.</li> <li><code>actions.lua.net_http_enabled</code> <code>(bool : false)</code> - Setting this to true will load the <code>net/http</code> package.</li> <li><code>actions.env.enabled</code> <code>(bool : true)</code> - Environment variables accessible by hooks, disabled values evaluated to empty strings</li> <li><code>actions.env.prefix</code> <code>(string : \"LAKEFSACTION_\")</code> - Access to environment variables is restricted to those with the prefix. When environment access is enabled and no prefix is provided, all variables are accessible.</li> </ul>"},{"location":"reference/configuration/#database","title":"database","text":"<p>Configuration section for the lakeFS key-value store database.</p> <ul> <li> <p><code>database.type</code> <code>(string [\"postgres\"|\"dynamodb\"|\"cosmosdb\"|\"redis\"|\"local\"] : )</code> - lakeFS database type</p> <p>Note</p> <p>The <code>redis</code> database type is available in lakeFS Enterprise</p> </li> </ul> <code>database.postgres</code><code>database.dynamodb</code><code>database.cosmosdb</code><code>database.local</code> <ul> <li><code>database.postgres.connection_string</code> <code>(string : \"postgres://localhost:5432/postgres?sslmode=disable\")</code> - PostgreSQL connection string to use</li> <li><code>database.postgres.max_open_connections</code> <code>(int : 25)</code> - Maximum number of open connections to the database</li> <li><code>database.postgres.max_idle_connections</code> <code>(int : 25)</code> - Maximum number of connections in the idle connection pool</li> <li><code>database.postgres.connection_max_lifetime</code> <code>(duration : 5m)</code> - Sets the maximum amount of time a connection may be reused <code>(valid units: ns|us|ms|s|m|h)</code></li> </ul> <ul> <li><code>database.dynamodb.table_name</code> <code>(string : \"kvstore\")</code> - Table used to store the data</li> <li> <p><code>database.dynamodb.scan_limit</code> <code>(int : 1025)</code> - Maximal number of items per page during scan operation</p> <p>Note</p> <p>Refer to the following AWS documentation for further information</p> </li> <li> <p><code>database.dynamodb.endpoint</code> <code>(string : )</code> - Endpoint URL for database instance</p> </li> <li><code>database.dynamodb.aws_region</code> <code>(string : )</code> - AWS Region of database instance</li> <li><code>database.dynamodb.aws_profile</code> <code>(string : )</code> - AWS named profile to use</li> <li><code>database.dynamodb.aws_access_key_id</code> <code>(string : )</code> - AWS access key ID</li> <li> <p><code>database.dynamodb.aws_secret_access_key</code> <code>(string : )</code> - AWS secret access key</p> <p>Note</p> <p><code>endpoint</code> <code>aws_region</code> <code>aws_access_key_id</code> <code>aws_secret_access_key</code> are not required and used mainly for experimental purposes when working with DynamoDB with different AWS credentials.</p> </li> <li> <p><code>database.dynamodb.health_check_interval</code> <code>(duration : 0s)</code> - Interval to run health check for the DynamoDB instance (won't run if equal to 0).</p> </li> <li><code>database.dynamodb.max_attempts</code> <code>(int : 10)</code> - The maximum number of attempts to perform on a DynamoDB request</li> <li><code>database.dynamodb.max_connections</code> <code>(int : 0)</code> - The maximum number of connections to DynamoDB. 0 means no limit.</li> <li><code>database.dynamodb.credentials_cache_expiry_window</code> <code>(duration : 200s)</code> - The expiry window for cached AWS credentials. This controls how long before credentials expire that the SDK will attempt to refresh them.</li> <li><code>database.dynamodb.credentials_cache_expiry_window_jitter_fraction</code> <code>(float : 0.5)</code> - The jitter fraction (0.0-1.0) for credentials cache expiry. This adds randomness to prevent thundering herd effects when refreshing credentials. A value of 0.5 means up to 50% jitter.</li> </ul> <ul> <li><code>database.cosmosdb.key</code> <code>(string : \"\")</code> - If specified, will be used to authenticate to the CosmosDB account. Otherwise, Azure SDK default authentication (with env vars) will be used.</li> <li><code>database.cosmosdb.endpoint</code> <code>(string : \"\")</code> - CosmosDB account endpoint, e.g. <code>https://&lt;account&gt;.documents.azure.com/</code>.</li> <li><code>database.cosmosdb.database</code> <code>(string : \"\")</code> - CosmosDB database name.</li> <li><code>database.cosmosdb.container</code> <code>(string : \"\")</code> - CosmosDB container name.</li> <li><code>database.cosmosdb.throughput</code> <code>(int32 : )</code> - CosmosDB container's RU/s. If not set - the default CosmosDB container throughput is used.</li> <li><code>database.cosmosdb.autoscale</code> <code>(bool : false)</code> - If set, CosmosDB container throughput is autoscaled (See CosmosDB docs for minimum throughput requirement). Otherwise, uses \"Manual\" mode (Docs).</li> </ul> <ul> <li><code>database.local.path</code> <code>(string : \"~/lakefs/metadata\")</code> - Local path on the filesystem to store embedded KV metadata, like branches and uncommitted entries</li> <li><code>database.local.sync_writes</code> <code>(bool: true)</code> - Ensure each write is written to the disk. Disable to increase performance</li> <li><code>database.local.prefetch_size</code> <code>(int: 256)</code> - How many items to prefetch when iterating over embedded KV records</li> <li><code>database.local.enable_logging</code> <code>(bool: false)</code> - Enable trace logging for local driver</li> </ul>"},{"location":"reference/configuration/#auth","title":"auth","text":"<ul> <li><code>auth.login_duration</code> <code>(time duration : \"168h\")</code> - The duration the login token is valid for.</li> <li><code>auth.login_max_duration</code> <code>(time duration : \"336h\")</code> - The maximum duration user can ask for a login token.</li> </ul>"},{"location":"reference/configuration/#authcache","title":"auth.cache","text":"<ul> <li><code>auth.cache.enabled</code> <code>(bool : true)</code> - Whether to cache access credentials and user policies in-memory. Can greatly improve throughput when enabled.</li> <li><code>auth.cache.size</code> <code>(int : 1024)</code> - How many items to store in the auth cache. Systems with a very high user count should use a larger value at the expense of ~1kb of memory per cached user.</li> <li><code>auth.cache.ttl</code> <code>(time duration : \"20s\")</code> - How long to store an item in the auth cache. Using a higher value reduces load on the database, but will cause changes longer to take effect for cached users.</li> <li><code>auth.cache.jitter</code> <code>(time duration : \"3s\")</code> - A random amount of time between 0 and this value is added to each item's TTL. This is done to avoid a large bulk of keys expiring at once and overwhelming the database.</li> </ul>"},{"location":"reference/configuration/#authencrypt","title":"auth.encrypt","text":"<ul> <li> <p><code>auth.encrypt.secret_key</code> <code>(string : \"\" - required)</code> - A random (cryptographically safe) generated string that is used for encryption and HMAC signing.</p> <p>Warning</p> <p>It is best to keep this somewhere safe such as KMS or Hashicorp Vault, and provide it to the system at run time.</p> </li> </ul>"},{"location":"reference/configuration/#authapi","title":"auth.api","text":"<ul> <li><code>auth.api.endpoint</code> <code>(string: \"\")</code> - URL to external Authorization Service described at authorization.yml; (e.g., \"https://external.service/api/v1\").</li> <li><code>auth.api.token</code> <code>(string: \"\")</code> - API token used to authenticate requests to api endpoint (e.g., eyJhbGciOiJIUzI1NiIsInR5...).</li> <li><code>auth.api.health_check_timeout</code> <code>(time duration : \"20s\")</code> - Timeout duration for external auth API health check.</li> <li><code>auth.api.skip_health_check</code> <code>(bool : false)</code> - Skip external auth API health check.</li> </ul>"},{"location":"reference/configuration/#authauthentication_api","title":"auth.authentication_api","text":"<ul> <li><code>auth.authentication_api.endpoint</code> <code>(string : \"\")</code> - URL to external Authentication Service described at authentication.yml.</li> <li><code>auth.authentication_api.external_principals_enabled</code> <code>(bool : false)</code> - If true, external principals API will be enabled, e.g auth service and login api's.</li> </ul>"},{"location":"reference/configuration/#authremote_authenticator","title":"auth.remote_authenticator","text":"<ul> <li><code>auth.remote_authenticator.enabled</code> <code>(bool : false)</code> - If specified, also authenticate users via this Remote Authenticator server.</li> <li><code>auth.remote_authenticator.endpoint</code> <code>(string : \"\" - required)</code> - Endpoint URL of the remote authentication service (e.g. https://my-auth.example.com/auth).</li> <li><code>auth.remote_authenticator.default_user_group</code> <code>(string : \"Viewers\")</code> - Create users in this group (i.e <code>Viewers</code>, <code>Developers</code>, etc).</li> <li><code>auth.remote_authenticator.request_timeout</code> <code>(time duration : 10s)</code> - If specified, timeout for remote authentication requests.</li> </ul>"},{"location":"reference/configuration/#authoidc","title":"auth.oidc","text":"<ul> <li><code>auth.oidc.default_initial_groups</code> <code>(string[] : [])</code> - By default, OIDC users will be assigned to these groups.</li> <li><code>auth.oidc.initial_groups_claim_name</code> <code>(string : \"\")</code> - Use this claim from the ID token to provide the initial group for new users. This will take priority if <code>auth.oidc.default_initial_groups</code> is also set.</li> <li><code>auth.oidc.friendly_name_claim_name</code> <code>(string : \"\")</code> - If specified, the value from the claim with this name will be used as the user's display name.</li> <li><code>auth.oidc.persist_friendly_name</code> <code>(bool : false)</code> - If set to <code>true</code>, the friendly name is persisted to the KV store and can be displayed in the user list. This is meant to be used in conjunction with <code>auth.oidc.friendly_name_claim_name</code>.</li> <li><code>auth.oidc.validate_id_token_claims</code> <code>(map[string]string : {})</code> - When a user tries to access lakeFS, validate that the ID token contains these claims with the corresponding values.</li> </ul>"},{"location":"reference/configuration/#authcookie_auth_verification","title":"auth.cookie_auth_verification","text":"<ul> <li><code>auth.cookie_auth_verification.validate_id_token_claims</code> <code>(map[string]string : {})</code> - When a user tries to access lakeFS, validate that the ID token contains these claims with the corresponding values.</li> <li><code>auth.cookie_auth_verification.default_initial_groups</code> <code>(string[] : [])</code> - By default, users will be assigned to these groups.</li> <li><code>auth.cookie_auth_verification.initial_groups_claim_name</code> <code>(string : \"\")</code> - Use this claim from the ID token to provide the initial group for new users. This will take priority if <code>auth.cookie_auth_verification.default_initial_groups</code> is also set.</li> <li><code>auth.cookie_auth_verification.friendly_name_claim_name</code> <code>(string : \"\")</code> - If specified, the value from the claim with this name will be used as the user's display name.</li> <li><code>auth.cookie_auth_verification.persist_friendly_name</code> <code>(bool : false)</code> - If set to <code>true</code>, the friendly name is persisted to the KV store and can be displayed in the user list. This is meant to be used in conjunction with <code>auth.cookie_auth_verification.friendly_name_claim_name</code>.</li> <li><code>auth.cookie_auth_verification.external_user_id_claim_name</code> - <code>(string : \"\")</code> - If specified, the value from the claim with this name will be used as the user's id name.</li> <li><code>auth.cookie_auth_verification.auth_source</code> - <code>(string : \"\")</code> - If specified, user will be labeled with this auth source.</li> </ul>"},{"location":"reference/configuration/#authui_config","title":"auth.ui_config","text":"<ul> <li> <p><code>auth.ui_config.rbac</code> <code>(string: \"none\")</code> - \"none\", \"simplified\", \"external\" or \"internal\".</p> <ul> <li><code>\"none\"</code> - lakeFS runs without an RBAC authorization service. No additional users, groups, or policies can be created.</li> <li><code>\"simplified\"</code> - lakeFS uses ACL as the authorization service. You must implement your own ACL service and connect it to lakeFS. See the implementation guide and ACL overview for details.</li> <li><code>\"external\"</code> - For lakeFS cloud - lakeFS cloud integrates with an external RBAC authorization service.</li> <li><code>\"internal\"</code> - Available in the Enterprise lakeFS version. Uses the built-in RBAC authorization service. More information is available here.</li> </ul> </li> <li> <p><code>auth.ui_config.login_url</code> <code>(string : \"\")</code> - An absolute or relative URL to your IdP\u2019s login page, used to authenticate to lakeFS via SSO with OIDC or SAML.</p> </li> <li><code>auth.ui_config.login_failed_message</code> <code>(string : \"The credentials don't match.\")</code> - Custom error message displayed when authentication fails on the login form (defaults to \"The credentials don't match.\" if not specified).</li> <li><code>auth.ui_config.logout_url</code> <code>(string : \"\")</code>- URL to redirect users to when they logout from lakeFS (defaults to \"/logout\" if not specified).</li> <li><code>auth.ui_config.use_login_placeholders</code> <code>(bool: false)</code> - If set to true, the login page will show placeholders for the Access Key ID and Secret Access Key (Username and Password) (defaults to \"false\" if not specified).</li> </ul>"},{"location":"reference/configuration/#blockstore","title":"blockstore","text":"<ul> <li><code>blockstore.type</code> <code>(one of [\"local\", \"s3\", \"gs\", \"azure\", \"mem\"] : required)</code>. Block adapter to use. This controls where the underlying data will be stored</li> <li><code>blockstore.default_namespace_prefix</code> <code>(string : )</code> - Use this to help your users choose a storage namespace for their repositories.    If specified, the storage namespace will be filled with this default value as a prefix when creating a repository from the UI.    The user may still change it to something else.</li> <li><code>blockstore.signing.secret_key</code> <code>(string : required)</code> - A random generated string that is used for HMAC signing when using get/link physical address</li> </ul> <code>blockstore.local</code><code>blockstore.s3</code><code>blockstore.azure</code><code>blockstore.gs</code> <p>Note</p> <p>local blockstore is supported for POSIX compliant filesystems only.</p> <ul> <li><code>blockstore.local.path</code> <code>(string: \"~/lakefs/data/block\")</code> - When using the local Block Adapter, which directory to store files in</li> <li><code>blockstore.local.import_enabled</code> <code>(bool: false)</code> - Enable import for local Block Adapter, relevant only if you are using shared location</li> <li><code>blockstore.local.import_hidden</code> <code>(bool: false)</code> - When enabled import will scan and import any file or folder that starts with a dot character.</li> <li><code>blockstore.local.allowed_external_prefixes</code> <code>([]string: [])</code> - List of absolute path prefixes used to match any access for external location (ex: /var/data/). Empty list mean no access to external location.</li> </ul> <ul> <li><code>blockstore.s3.region</code> <code>(string : \"us-east-1\")</code> - Default region for lakeFS to use when interacting with S3.</li> <li><code>blockstore.s3.profile</code> <code>(string : )</code> - If specified, will be used as a named credentials profile</li> <li><code>blockstore.s3.credentials_file</code> <code>(string : )</code> - If specified, will be used as a credentials file</li> <li><code>blockstore.s3.credentials.access_key_id</code> <code>(string : )</code> - If specified, will be used as a static set of credential</li> <li><code>blockstore.s3.credentials.secret_access_key</code> <code>(string : )</code> - If specified, will be used as a static set of credential</li> <li><code>blockstore.s3.credentials.session_token</code> <code>(string : )</code> - If specified, will be used as a static session token</li> <li><code>blockstore.s3.endpoint</code> <code>(string : )</code> - If specified, custom endpoint for the AWS S3 API (https://s3_compatible_service_endpoint:port)</li> <li><code>blockstore.s3.force_path_style</code> <code>(bool : false)</code> - When true, use path-style S3 URLs (https:/// instead of https://.) <li><code>blockstore.s3.discover_bucket_region</code> <code>(bool : true)</code> - (Can be turned off if the underlying S3 bucket doesn't support the GetBucketRegion API).</li> <li><code>blockstore.s3.skip_verify_certificate_test_only</code> <code>(bool : false)</code> - Skip certificate verification while connecting to the storage endpoint. Should be used only for testing.</li> <li><code>blockstore.s3.server_side_encryption</code> <code>(string : )</code> - Server side encryption format used (Example on AWS using SSE-KMS while passing \"aws:kms\")</li> <li><code>blockstore.s3.server_side_encryption_kms_key_id</code> <code>(string : )</code> - Server side encryption KMS key ID</li> <li><code>blockstore.s3.pre_signed_expiry</code> <code>(time duration : \"15m\")</code> - Expiry of pre-signed URL.</li> <li><code>blockstore.s3.pre_signed_endpoint</code> <code>(string : )</code> - Custom endpoint for pre-signed URLs.</li> <li><code>blockstore.s3.disable_pre_signed</code> <code>(bool : false)</code> - Disable use of pre-signed URL.</li> <li><code>blockstore.s3.disable_pre_signed_ui</code> <code>(bool : true)</code> - Disable use of pre-signed URL in the UI.</li> <li><code>blockstore.s3.disable_pre_signed_multipart</code> <code>(bool : )</code> - Disable use of pre-signed multipart upload experimental, enabled on s3 block adapter with presign support.</li> <li><code>blockstore.s3.client_log_request</code> <code>(bool : false)</code> - Set SDK logging bit to log requests</li> <li><code>blockstore.s3.client_log_retries</code> <code>(bool : false)</code> - Set SDK logging bit to log retries</li> <ul> <li><code>blockstore.azure.storage_account</code> <code>(string : )</code> - If specified, will be used as the Azure storage account</li> <li><code>blockstore.azure.storage_access_key</code> <code>(string : )</code> - If specified, will be used as the Azure storage access key</li> <li><code>blockstore.azure.pre_signed_expiry</code> <code>(time duration : \"15m\")</code> - Expiry of pre-signed URL.</li> <li><code>blockstore.azure.disable_pre_signed</code> <code>(bool : false)</code> - Disable use of pre-signed URL.</li> <li><code>blockstore.azure.disable_pre_signed_ui</code> <code>(bool : true)</code> - Disable use of pre-signed URL in the UI.</li> <li> <p><code>blockstore.azure.china_cloud</code> <code>(bool : false)</code> - Enable for using lakeFS on Azure China Cloud.</p> <p>Deprecated</p> <p>Please use <code>blockstore.azure.domain</code></p> </li> <li> <p><code>blockstore.azure.domain</code> <code>(string : blob.core.windows.net)</code> - Enables support of different Azure cloud domains.</p> <p>Current supported domains (in Beta stage): [<code>blob.core.chinacloudapi.cn</code>, <code>blob.core.usgovcloudapi.net</code>]</p> </li> </ul> <ul> <li><code>blockstore.gs.credentials_file</code> <code>(string : )</code> - If specified will be used as a file path of the JSON file that contains your Google service account key</li> <li><code>blockstore.gs.credentials_json</code> <code>(string : )</code> - If specified will be used as JSON string that contains your Google service account key (when credentials_file is not set)</li> <li><code>blockstore.gs.data_credentials_file</code> <code>(string : )</code> - If specified will be used as a file path of the JSON file that contains your Google service account key for data operations experimental</li> <li><code>blockstore.gs.data_credentials_json</code> <code>(string : )</code> - If specified will be used as JSON string that contains your Google service account key for data operations experimental</li> <li><code>blockstore.gs.pre_signed_expiry</code> <code>(time duration : \"15m\")</code> - Expiry of pre-signed URL.</li> <li><code>blockstore.gs.disable_pre_signed</code> <code>(bool : false)</code> - Disable use of pre-signed URL.</li> <li><code>blockstore.gs.disable_pre_signed_ui</code> <code>(bool : true)</code> - Disable use of pre-signed URL in the UI.</li> <li><code>blockstore.gs.server_side_encryption_customer_supplied</code> <code>(string : )</code> - Server side encryption with AES key in hex format, exclusive with key ID below</li> <li><code>blockstore.gs.server_side_encryption_kms_key_id</code> <code>(string : )</code> - Server side encryption KMS key ID, exclusive with above</li> </ul>"},{"location":"reference/configuration/#graveler","title":"graveler","text":"<ul> <li><code>graveler.ensure_readable_root_namespace</code> <code>(bool: true)</code> - When creating a new repository use this to verify that lakeFS has access to the root of the underlying storage namespace. Set <code>false</code> only if lakeFS should not have access (i.e pre-sign mode only).</li> <li><code>graveler.max_batch_delay</code> <code>(duration : 3ms)</code> - Controls the server batching period for references store operations.</li> <li><code>graveler.background.rate_limit</code> <code>(int : 0)</code> - Requests per seconds limit on background work performed (default: 0 - unlimited), like deleting committed staging tokens.</li> </ul>"},{"location":"reference/configuration/#gravelerrepository_cache","title":"graveler.repository_cache","text":"<ul> <li><code>graveler.repository_cache.size</code> <code>(int : 1000)</code> - How many items to store in the repository cache.</li> <li><code>graveler.repository_cache.ttl</code> <code>(time duration : \"5s\")</code> - How long to store an item in the repository cache.</li> <li><code>graveler.repository_cache.jitter</code> <code>(time duration : \"2s\")</code> - A random amount of time between 0 and this value is added to each item's TTL.</li> </ul>"},{"location":"reference/configuration/#gravelercommit_cache","title":"graveler.commit_cache","text":"<ul> <li><code>graveler.commit_cache.size</code> <code>(int : 50000)</code> - How many items to store in the commit cache.</li> <li><code>graveler.commit_cache.ttl</code> <code>(time duration : \"10m\")</code> - How long to store an item in the commit cache.</li> <li><code>graveler.commit_cache.jitter</code> <code>(time duration : \"2s\")</code> - A random amount of time between 0 and this value is added to each item's TTL.</li> </ul>"},{"location":"reference/configuration/#committed","title":"committed","text":"<ul> <li><code>committed.block_storage_prefix</code> (<code>string</code> : <code>_lakefs</code>) - Prefix for metadata file storage   in each repository's storage namespace</li> <li><code>committed.sstable.memory.cache_size_bytes</code> (<code>int</code> : <code>400_000_000</code>) - maximal size of   in-memory cache used for each SSTable reader.</li> </ul>"},{"location":"reference/configuration/#committedlocal_cache","title":"committed.local_cache","text":"<p>An object describing the local (on-disk) cache of metadata from permanent storage.</p> <ul> <li><code>committed.local_cache.size_bytes</code> (<code>int</code> : <code>1073741824</code>) - bytes for local cache to use on disk.  The cache may use more storage for short periods of time.</li> <li><code>committed.local_cache.dir</code> (<code>string</code>, <code>~/lakefs/data/cache</code>) - directory to store local cache.</li> <li><code>committed.local_cache.range_proportion</code> (<code>float</code> : <code>0.9</code>) - proportion of local cache to   use for storing ranges (leaves of committed metadata storage).</li> <li><code>committed.local_cache.range.open_readers</code> (<code>int</code> : <code>500</code>) - maximal number of unused open   SSTable readers to keep for ranges.</li> <li><code>committed.local_cache.range.num_shards</code> (<code>int</code> : <code>30</code>) - sharding factor for open SSTable   readers for ranges.  Should be at least <code>sqrt(committed.local_cache.range.open_readers)</code>.</li> <li><code>committed.local_cache.metarange_proportion</code> (<code>float</code> : <code>0.1</code>) - proportion of local cache   to use for storing metaranges (roots of committed metadata storage).</li> <li><code>committed.local_cache.metarange.open_readers</code> (<code>int</code> : <code>50</code>) - maximal number of unused open   SSTable readers to keep for metaranges.</li> <li><code>committed.local_cache.metarange.num_shards</code> (<code>int</code> : <code>10</code>) - sharding factor for open   SSTable readers for metaranges.  Should be at least   <code>sqrt(committed.local_cache.metarange.open_readers)</code>.</li> </ul>"},{"location":"reference/configuration/#committedpermanent","title":"committed.permanent","text":"<ul> <li><code>committed.permanent.min_range_size_bytes</code> (<code>int</code> : <code>0</code>) - Smallest allowable range in   metadata.  Increase to somewhat reduce random access time on committed metadata, at the cost   of increased committed metadata storage cost.</li> <li><code>committed.permanent.max_range_size_bytes</code> (<code>int</code> : <code>20971520</code>) - Largest allowable range in   metadata.  Should be close to the size at which fetching from remote storage becomes linear.</li> <li><code>committed.permanent.range_raggedness_entries</code> (<code>int</code> : <code>50_000</code>) - Average number of object   pointers to store in each range (subject to <code>min_range_size_bytes</code> and   <code>max_range_size_bytes</code>).</li> </ul>"},{"location":"reference/configuration/#email","title":"email","text":"<ul> <li><code>email.smtp_host</code> <code>(string)</code> - A string representing the URL of the SMTP host.</li> <li><code>email.smtp_port</code> (<code>int</code>) - An integer representing the port of the SMTP service (465, 587, 993, 25 are some standard ports)</li> <li><code>email.use_ssl</code> (<code>bool : false</code>) - Use SSL connection with SMTP host.</li> <li><code>email.username</code> <code>(string)</code> - A string representing the username of the specific account at the SMTP. It's recommended to provide this value at runtime from a secret vault of some sort.</li> <li><code>email.password</code> <code>(string)</code> - A string representing the password of the account. It's recommended to provide this value at runtime from a secret vault of some sort.</li> <li><code>email.local_name</code> <code>(string)</code> - A string representing the hostname sent to the SMTP server with the HELO command. By default, \"localhost\" is sent.</li> <li><code>email.sender</code> <code>(string)</code> - A string representing the email account which is set as the sender.</li> <li><code>email.limit_every_duration</code> <code>(duration : 1m)</code> - The average time between sending emails. If zero is entered, there is no limit to the amount of emails that can be sent.</li> <li><code>email.burst</code> <code>(int: 10)</code> - Maximal burst of emails before applying <code>limit_every_duration</code>. The zero value means no burst and therefore no emails can be sent.</li> <li><code>email.lakefs_base_url</code> <code>(string : \"http://localhost:8000\")</code> - A string representing the base lakeFS endpoint to be directed to when emails are sent inviting users, reseting passwords etc.</li> </ul>"},{"location":"reference/configuration/#gateways","title":"gateways","text":"<ul> <li><code>gateways.s3.domain_name</code> <code>(string : \"s3.local.lakefs.io\")</code> - a FQDN   representing the S3 endpoint used by S3 clients to call this server   (<code>*.s3.local.lakefs.io</code> always resolves to 127.0.0.1, useful for   local development, if using virtual-host addressing.</li> <li><code>gateways.s3.region</code> <code>(string : \"us-east-1\")</code> - AWS region we're pretending to be in, it should match the region configuration used in AWS SDK clients</li> <li><code>gateways.s3.fallback_url</code> <code>(string)</code> - If specified, requests with a non-existing repository will be forwarded to this URL. This can be useful for using lakeFS side-by-side with S3, with the URL pointing at an S3Proxy instance.</li> <li><code>gateways.s3.verify_unsupported</code> <code>(bool : true)</code> - The S3 gateway errors on unsupported requests, but when disabled, defers to target-based handlers.</li> </ul>"},{"location":"reference/configuration/#tls","title":"tls","text":"<ul> <li><code>tls.enabled</code> <code>(bool :false)</code> - Enable TLS listening. The <code>listen_address</code> will be used to serve HTTPS requests. (mainly for local development)</li> <li><code>tls.cert_file</code> <code>(string : )</code> - Server certificate file path used while serve HTTPS (.cert or .crt file - signed certificates).</li> <li><code>tls.key_file</code> <code>(string : )</code> - Server secret key file path used whie serve HTTPS (.key file - private key).</li> </ul>"},{"location":"reference/configuration/#stats","title":"stats","text":"<ul> <li><code>stats.enabled</code> <code>(bool : true)</code> - Whether to periodically collect anonymous usage statistics</li> <li><code>stats.flush_interval</code> <code>(duration : 30s)</code> - Interval used to post anonymous statistics collected</li> <li><code>stats.flush_size</code> <code>(int : 100)</code> - A size (in records) of anonymous statistics collected in which we post</li> </ul>"},{"location":"reference/configuration/#installation","title":"installation","text":"<ul> <li><code>installation.user_name</code> <code>(string : )</code> - When specified, an initial admin user will be created when the server is first run. Works only when <code>database.type</code> is set to local. Requires <code>installation.access_key_id</code> and <code>installation.secret_access_key</code>.</li> <li><code>installation.access_key_id</code> <code>(string : )</code> - Admin's initial access key id (used once in the initial setup process)</li> <li><code>installation.secret_access_key</code> <code>(string : )</code> - Admin's initial secret access key (used once in the initial setup process)</li> <li><code>installation.allow_inter_region_storage</code> <code>(bool : true)</code> - Allow storage in a different region than the one the server is running in.</li> </ul>"},{"location":"reference/configuration/#usage_report","title":"usage_report","text":"<ul> <li><code>usage_report.enabled</code> <code>(bool : true)</code> - Store API and Gateway usage reports into key-value store.</li> <li><code>usage_report.flush_interval</code> <code>(duration : 5m)</code> - Sets interval for flushing in-memory usage data to key-value store.</li> </ul>"},{"location":"reference/configuration/#ui","title":"ui","text":"<ul> <li><code>ui.enabled</code> <code>(bool: true)</code> - Whether to serve the embedded UI from the binary</li> </ul>"},{"location":"reference/configuration/#security","title":"security","text":"<ul> <li><code>security.audit_check_interval</code> <code>(duration : 24h)</code> - Duration in which we check for security audit.</li> </ul>"},{"location":"reference/configuration/#garbage-collection","title":"garbage collection","text":"<ul> <li><code>ugc.prepare_max_file_size</code> <code>(int: 125829120)</code> - Uncommitted garbage collection prepare request, limit the produced file maximum size</li> <li><code>ugc.prepare_interval</code> <code>(duraction: 1m)</code> - Uncommitted garbage collection prepare request, limit produce time to interval</li> </ul>"},{"location":"reference/configuration/#using-environment-variables","title":"Using Environment Variables","text":"<p>All the configuration variables can be set or overridden using environment variables.</p> <p>To set an environment variable, prepend <code>LAKEFS_</code> to its name, convert it to upper case, and replace <code>.</code> with <code>_</code>:</p> <p>For example, <code>logging.format</code> becomes <code>LAKEFS_LOGGING_FORMAT</code>, <code>blockstore.s3.region</code> becomes <code>LAKEFS_BLOCKSTORE_S3_REGION</code>, etc.</p> <p>To set a value into a <code>map[string]string</code> type field, use the syntax <code>key1=value1,key2=value2,...</code></p>"},{"location":"reference/configuration/#example-configurations","title":"Example Configurations","text":"<p>Local Development with PostgreSQL database</p> <pre><code>---\nlisten_address: \"0.0.0.0:8000\"\n\ndatabase:\ntype: \"postgres\"\npostgres:\n    connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\"\n\nlogging:\nformat: text\nlevel: DEBUG\noutput: \"-\"\n\nauth:\nencrypt:\n    secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\"\n\nblockstore:\ntype: local\nlocal:\n    path: \"~/lakefs/dev/data\"\n\ngateways:\ns3:\n    region: us-east-1\n</code></pre> <p>AWS Deployment with DynamoDB database</p> <pre><code>---\nlogging:\nformat: json\nlevel: WARN\noutput: \"-\"\n\ndatabase:\ntype: \"dynamodb\"\ndynamodb:\n    table_name: \"kvstore\"\n\nauth:\nencrypt:\n    secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\"\n\nblockstore:\ntype: s3\ns3:\n    region: us-east-1 # optional, fallback in case discover from bucket is not supported\n    credentials_file: /secrets/aws/credentials\n    profile: default\n</code></pre> <p>Google Storage</p> <pre><code>---\nlogging:\nformat: json\nlevel: WARN\noutput: \"-\"\n\ndatabase:\ntype: \"postgres\"\npostgres:\n    connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\"\n\nauth:\nencrypt:\n    secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\"\n\nblockstore:\ntype: gs\ngs:\n    credentials_file: /secrets/lakefs-service-account.json\n</code></pre> <p>MinIO</p> <pre><code>---\nlogging:\nformat: json\nlevel: WARN\noutput: \"-\"\n\ndatabase:\ntype: \"postgres\"\npostgres:\n    connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\"\n\nauth:\nencrypt:\n    secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\"\n\nblockstore:\ntype: s3\ns3:\n    force_path_style: true\n    endpoint: http://localhost:9000\n    discover_bucket_region: false\n    credentials:\n        access_key_id: minioadmin\n        secret_access_key: minioadmin\n</code></pre> <p>Azure blob storage</p> <pre><code>---\nlogging:\nformat: json\nlevel: WARN\noutput: \"-\"\n\ndatabase:\ntype: \"cosmosdb\"\ncosmosdb:\n    key: \"ExampleReadWriteKeyMD7nkPOWgV7d4BUjzLw==\"\n    endpoint: \"https://lakefs-account.documents.azure.com:443/\"\n    database: \"lakefs-db\"\n    container: \"lakefs-container\"\n\nauth:\nencrypt:\n    secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\"\n\nblockstore:\ntype: azure\nazure:\n    storage_account: exampleStorageAcount\n    storage_access_key: ExampleAcessKeyMD7nkPOWgV7d4BUjzLw==\n</code></pre>"},{"location":"reference/monitor/","title":"Monitoring using Prometheus","text":""},{"location":"reference/monitor/#example-prometheusyml","title":"Example prometheus.yml","text":"<p>lakeFS exposes metrics through the same port used by the lakeFS service, using the standard <code>/metrics</code> path.</p> <p>An example could look like this:</p> <p><code>prometheus.yml</code></p> <pre><code>scrape_configs:\n- job_name: lakeFS\n  scrape_interval: 10s\n  metrics_path: /metrics\n  static_configs:\n  - targets:\n    - lakefs.example.com:8000\n</code></pre>"},{"location":"reference/monitor/#metrics-exposed-by-lakefs","title":"Metrics exposed by lakeFS","text":"<p>By default, Prometheus exports metrics with OS process information like memory and CPU. It also includes Go-specific metrics such as details about GC and a number of goroutines. You can learn about these default metrics in this post.</p> <p>In addition, lakeFS exposes the following metrics to help monitor your deployment:</p> Name in Prometheus Description Labels <code>api_requests_total</code> lakeFS API requests (counter) code: http statusmethod: http method <code>lakefs_concurrent_requests</code> Number of concurrent requests being processed by lakeFS (gauge) service: \"api\" or \"gateway\"operation: name of operation <code>api_request_duration_seconds</code> Durations of lakeFS API requests (histogram) operation: name of API operationcode: http status <code>gateway_request_duration_seconds</code> lakeFS S3-compatible endpoint request (histogram) operation: name of gateway operationcode: http status <code>blockstore_concurrent_operations</code> Number of concurrent blockstore operations (gauge) operation: blockstore operation nameblockstore_type: type of blockstore (s3, gs, azure, etc) <code>s3_operation_duration_seconds</code> Outgoing S3 operations (histogram) operation: operation nameerror: \"true\" if error, \"false\" otherwise <code>gs_operation_duration_seconds</code> Outgoing Google Storage operations (histogram) operation: operation nameerror: \"true\" if error, \"false\" otherwise <code>azure_operation_duration_seconds</code> Outgoing Azure storage operations (histogram) operation: operation nameerror: \"true\" if error, \"false\" otherwise <code>kv_request_duration_seconds</code> Durations of KV requests(histogram) operation: name of KV operationtype: KV type(dynamodb, postgres, etc) <code>dynamo_request_duration_seconds</code> Time spent doing DynamoDB requests operation: DynamoDB operation name <code>dynamo_consumed_capacity_total</code> The capacity units consumed by operation operation: DynamoDB operation name <code>dynamo_failures_total</code> The total number of errors while working for kv store operation: DynamoDB operation name <code>pgxpool_acquire_count</code> PostgreSQL cumulative count of successful acquires from the pool db_name default to the kv table name (kv) <code>pgxpool_acquire_duration_ns</code> PostgreSQL total duration of all successful acquires from the pool in nanoseconds db_name default to the kv table name (kv) <code>pgxpool_acquired_conns</code> PostgreSQL number of currently acquired connections in the pool db_name default to the kv table name (kv) <code>pgxpool_canceled_acquire_count</code> PostgreSQL cumulative count of acquires from the pool that were canceled by a context db_name default to the kv table name (kv) <code>pgxpool_constructing_conns</code> PostgreSQL number of conns with construction in progress in the pool db_name default to the kv table name (kv) <code>pgxpool_empty_acquire</code> PostgreSQL cumulative count of successful acquires from the pool that waited for a resource to be released or constructed because the pool was empty db_name default to the kv table name (kv) <code>pgxpool_idle_conns</code> PostgreSQL number of currently idle conns in the pool db_name default to the kv table name (kv) <code>pgxpool_max_conns</code> PostgreSQL maximum size of the pool db_name default to the kv table name (kv) <code>pgxpool_total_conns</code> PostgreSQL total number of resources currently in the pool db_name default to the kv table name (kv)"},{"location":"reference/monitor/#example-queries","title":"Example queries","text":"<p>Note</p> <p>when using Prometheus functions like rate or increase, results are extrapolated and may not be exact.</p> <p>99th percentile of API request latencies</p> <pre><code>sum by (operation)(histogram_quantile(0.99, rate(api_request_duration_seconds_bucket[1m])))\n</code></pre> <p>50th percentile of S3-compatible API latencies</p> <pre><code>sum by (operation)(histogram_quantile(0.5, rate(gateway_request_duration_seconds_bucket[1m])))\n</code></pre> <p>Number of errors in outgoing S3 requests</p> <pre><code>sum by (operation) (increase(s3_operation_duration_seconds_count{error=\"true\"}[1m]))\n</code></pre> <p>Number of open connections to the database</p> <pre><code>go_sql_stats_connections_open\n</code></pre>"},{"location":"reference/monitor/#example-grafana-dashboard","title":"Example Grafana dashboard","text":""},{"location":"reference/mount/","title":"Mount (Everest)","text":"<p>Info</p> <p>Available in lakeFS Cloud and lakeFS Enterprise</p> <p>For the latest updates and changes to lakeFS Mount features, see the changelog.</p> <p>Everest is a complementary binary to lakeFS that allows you to virtually mount a remote lakeFS repository onto a local directory or within a Kubernetes environment. Once mounted, you can access data as if it resides on your local filesystem, using any tool, library, or framework.</p>"},{"location":"reference/mount/#use-cases","title":"Use Cases","text":"<ul> <li>Simplified Data Loading: Use your existing tools to read and write files directly from the filesystem with no need for custom data loaders or SDKs.</li> <li>Seamless Scalability: Scale from a few local files to billions without changing your tools or workflow. Use the same code from experimentation to production.</li> <li>Enhanced Performance: Everest supports billions of files and offers fast, lazy data fetching, making it ideal for optimizing GPU utilization and other performance-sensitive tasks.</li> </ul>"},{"location":"reference/mount/#getting-started","title":"Getting Started","text":"<p>This guide will walk you through setting up and using Everest to mount a lakeFS repository on your local machine.</p> <p>New to Everest?</p> <p>After completing this getting started guide, we recommend reading the Core Concepts section to understand caching, consistency, and performance characteristics.</p>"},{"location":"reference/mount/#prerequisites","title":"Prerequisites","text":"<ul> <li>lakeFS Cloud account or lakeFS Enterprise Version <code>1.25.0</code> or higher.</li> <li>Supported OS: macOS (with NFS V3), Linux and Windows (using CFAPI).</li> <li>Get the Everest Binary: Everest is a self-contained binary with no installation required. Please contact us to get access.</li> </ul> <p>Windows Support</p> <p>Everest for Windows is now available. Currently, only read operations are supported. See Everest for Windows</p>"},{"location":"reference/mount/#authentication-configuration","title":"Authentication &amp; Configuration","text":"<p>Everest uses the same configuration and authentication methods as <code>lakectl</code>. It discovers credentials and the server endpoint in the following order:</p> <ol> <li>Command-Line Flags: <code>--lakectl-access-key-id</code>, <code>--lakectl-secret-access-key</code>, and <code>--lakectl-server-url</code>.</li> <li>Environment Variables: <code>LAKECTL_*</code> or <code>EVEREST_LAKEFS_*</code> prefixed variables.</li> <li>Configuration File: <code>~/.lakectl.yaml</code> (or the file specified by <code>--lakectl-config</code>).</li> </ol> Authentication Methods <p>Everest will attempt to authenticate in the following order:</p> <ol> <li>Session Token: From <code>EVEREST_LAKEFS_CREDENTIALS_SESSION_TOKEN</code> or <code>LAKECTL_CREDENTIALS_SESSION_TOKEN</code>. If the token is expired, authentication will fail.</li> <li>lakeFS Key Pair: Standard access key ID and secret access key (credentials are picked up from lakectl configuration if Everest-specific credentials are not provided).</li> <li> <p>IAM Authentication: If your lakeFS environment is configured for AWS IAM Role Login, Everest (\u2265 v0.4.0) can authenticate using your AWS environment (e.g., <code>AWS_PROFILE</code>). IAM authentication is only attempted when no static credentials are set. To enable this, configure your .lakectl.yaml with <code>provider_type: aws_iam</code>. The token is seamlessly refreshed as long as the AWS session remains valid.</p> <p>To configure IAM authentication using environment variables, use the <code>EVEREST_LAKEFS_*</code> or <code>LAKECTL_*</code> prefix: <pre><code>export EVEREST_LAKEFS_CREDENTIALS_PROVIDER_TYPE=aws_iam\n# or\nexport LAKECTL_CREDENTIALS_PROVIDER_TYPE=aws_iam\n</code></pre></p> </li> </ol> <p>lakectl Version Compatibility</p> <p>If you configure the IAM provider using the same <code>lakectl.yaml</code> file that you use for the lakectl CLI, you must upgrade lakectl to version <code>\u2265 v1.57.0</code>. Otherwise, lakectl will raise errors when using it.</p> <p>Troubleshooting IAM Presign Requests</p> <p>To troubleshoot presign request issues with IAM authentication, you can enable debug logging for presign requests using the environment variable: <pre><code>export EVEREST_LAKEFS_CREDENTIALS_PROVIDER_AWS_IAM_CLIENT_LOG_PRE_SIGNING_REQUEST=true\n</code></pre></p>"},{"location":"reference/mount/#create-your-first-mount","title":"Create Your First Mount","text":"<p>Let's mount a prefix from a lakeFS repository to a local directory. In read-only mode, Everest mounts a specific commit ID. If you provide a branch name, it will resolve to the HEAD commit at the time of mounting.</p> <ol> <li> <p>Mount the repository:     This command mounts the <code>datasets/pets/</code> prefix from the <code>main</code> branch of the <code>image-repo</code> repository into a new local directory named <code>./pets</code>.</p> <pre><code>everest mount \"lakefs://image-repo/main/datasets/pets/\" \"./pets\"\n</code></pre> </li> <li> <p>Explore the data:     You can now use standard filesystem commands to interact with your data. Files are downloaded lazily only when you access their content.</p> <pre><code># List files - this only fetches metadata\nls -l \"./pets/dogs/\"\n\n# Find files\nfind ./pets -name \"*.small.jpg\"\n\n# Open a file - this triggers a download\nopen -a Preview \"./pets/dogs/golden_retrievers/cute.jpg\"\n</code></pre> </li> <li> <p>Unmount the directory:     When you are finished, unmount the directory.</p> <pre><code>everest umount \"./pets\"\n</code></pre> </li> </ol>"},{"location":"reference/mount/#core-concepts","title":"Core Concepts","text":"<p>This section will help you understand how Everest manages performance, consistency, and caching in both local and Kubernetes deployments.</p>"},{"location":"reference/mount/#cache-behavior","title":"Cache Behavior","text":"<p>Everest uses a local cache to improve performance when accessing files from lakeFS. Understanding how the cache works will help you optimize performance for your specific use case.</p> How Caching Works <p>When you access a file through a mounted lakeFS path, Everest follows this process:</p> <ol> <li>Lazy Fetching: Files are only downloaded when their content is accessed (e.g., reading a file, not just listing it with <code>ls</code>).</li> <li>Cache Storage: When an object is not found in the local cache, Everest fetches the data from the object store and stores it in the cache for subsequent access.</li> <li>Cache Reuse: Subsequent reads of the same file are served directly from the cache, eliminating network requests and improving performance. Cached can't be shared between different instances of mount.</li> </ol> Default Cache Behavior <p>By default, Everest creates a temporary cache directory when you run <code>everest mount</code>. This directory is automatically cleared when the mount is terminated via <code>everest umount</code>.</p> <p>Key points:</p> <ul> <li>Each new mount creates a fresh cache directory.</li> <li>By default cache location is managed by Everest and cleaned up automatically.</li> <li>The cache is ephemeral and does not persist between mount sessions. Unless you specify the cache directory.</li> </ul> Persistent Cache <p>To reuse cache data across multiple mount sessions, you can specify a custom cache directory using the <code>--cache-dir</code> flag:</p> <pre><code>everest mount lakefs://image-repo/main/datasets/ ./datasets --cache-dir ~/.everest-cache\n</code></pre> <p>Benefits of persistent cache:</p> <ul> <li>Faster startup times when remounting the same data.</li> <li>Reduced bandwidth usage by reusing previously downloaded files.</li> <li>Useful for iterative workflows where you repeatedly mount and unmount the same repository.</li> </ul> Cache Management <p>Everest manages cached data based on the commit ID of the mounted reference:</p> <ul> <li>Commit-Based Caching: Each commit ID has its own cache namespace. This ensures that cached data always corresponds to the correct version of your files.</li> <li>Cache Invalidation on Commit: When you commit changes in write mode using <code>everest commit</code>, the mount point's source commit ID is updated to the new HEAD of the branch. As a result, the cache associated with the old commit ID is no longer used, and new data will be cached under the new commit ID.</li> </ul> <p>Optimizing Cache Size</p> <p>Set <code>--cache-size</code> to match the amount of data you plan to read or write. A larger cache reduces the need to evict and re-fetch files, improving performance for workloads that access many files.</p>"},{"location":"reference/mount/#consistency-data-behavior","title":"Consistency &amp; Data Behavior","text":"File System Consistency <p>Everest mount provides strong read-after-write consistency within a single mount point. Once a write operation completes, the data is guaranteed to be available for subsequent read operations on that same mount.</p> lakeFS Consistency <p>Local changes are reflected in lakeFS only after they are committed using the <code>everest commit</code> command. Until then:</p> <ul> <li>Changes are only visible within your local mount point</li> <li>Other users or mounts will not see your changes</li> <li>If two users mount the same branch, they will not see each other's changes until those changes are committed</li> </ul> Sync Operation <p>When you run <code>everest diff</code> or <code>everest commit</code>, Everest performs a sync operation that uploads all local changes to a temporary location in lakeFS for processing. This ensures your changes are safely transferred before being committed to the branch.</p> <p>See the Write-Mode Operations section for more details on working with writable mounts.</p>"},{"location":"reference/mount/#performance-considerations","title":"Performance Considerations","text":"<p>Everest achieves high-performance data access through:</p> <ul> <li>Direct Object Store Access: By default, Everest uses pre-signed URLs to read and write data directly to and from the underlying object store, bypassing the lakeFS server for data transfer. Only metadata operations go through the lakeFS server.</li> <li>Lazy Metadata Loading: Directory listings are fetched on-demand, allowing you to work with repositories containing billions of files without upfront overhead.</li> <li>Cache Sizing: Setting an appropriate <code>--cache-size</code> prevents frequent eviction and re-fetching. As a rule of thumb, size your cache to accommodate your working set.</li> <li>Network Bandwidth: Since data is fetched directly from object storage, ensure your network connection has adequate bandwidth for your workload.</li> </ul> <p>Optimizing for ML Workloads</p> <p>For training jobs, consider using a persistent cache directory (<code>--cache-dir</code>) and sizing the cache to fit your entire dataset. This eliminates repeated downloads across training epochs.</p>"},{"location":"reference/mount/#working-with-data-local-mount","title":"Working with Data (Local Mount)","text":""},{"location":"reference/mount/#read-only-operations","title":"Read-Only Operations","text":"<p>Read-only mode is the default and is ideal for data exploration, analysis, and feeding data into local applications without the risk of accidental changes.</p> <p>For information about how data is cached and accessed, see the Cache Behavior section.</p> <p>Working with Data Locally</p> <p>Mount a repository and use your favorite tools directly on the data.</p> <pre><code>everest mount lakefs://image-repo/main/datasets/pets/ ./pets\n\n# Run a python script\npytorch_train.py --input ./pets\n\n# Query data with DuckDB\nduckdb \"SELECT * FROM read_parquet('pets/labels.parquet')\"\n\neverest umount ./pets\n</code></pre>"},{"location":"reference/mount/#write-mode-operations","title":"Write-Mode Operations","text":"<p>By enabling write mode (--write-mode), you can modify, add, and delete files locally and then commit those changes back to the lakeFS branch. When running in write mode, the lakeFS URI must point to a branch, not a commit ID or a tag.</p> Example of changing data locally <ol> <li> <p>Mount in write mode:     Use the <code>--write-mode</code> flag to enable writes.</p> <pre><code>everest mount lakefs://image-repo/main/datasets/pets/ ./pets --write-mode\n</code></pre> </li> <li> <p>Modify files:     Make any changes you need using standard shell commands.</p> <pre><code># Add a new file\necho \"new data\" &gt; ./pets/birds/parrot/cute.jpg\n\n# Update an existing file\necho \"new data\" &gt;&gt; ./pets/dogs/golden_retrievers/cute.jpg\n\n# Delete a file\nrm ./pets/cats/persian/cute.jpg\n</code></pre> </li> <li> <p>Review your changes:     The <code>diff</code> command shows the difference between your local state and the branch's state at the time of mounting.</p> <pre><code>everest diff ./pets\n# Output:\n# + added datasets/pets/birds/parrot/cute.jpg\n# ~ modified datasets/pets/dogs/golden_retrievers/cute.jpg\n# - removed datasets/pets/cats/persian/cute.jpg\n</code></pre> </li> <li> <p>Commit your changes:     The <code>commit</code> command uploads your local changes and commits them to the source branch in lakeFS.</p> <p><pre><code>everest commit ./pets -m \"Updated pet images\"\n</code></pre> After committing, your local mount will be synced to the new HEAD of the branch. Running <code>diff</code> again will show no changes.</p> </li> <li> <p>Unmount when finished: <pre><code>everest umount ./pets\n</code></pre></p> </li> </ol> <p>Write Mode Limitations</p> <p>Write mode has some limitations on supported operations. See Write Mode Limitations for details on unsupported operations and modified behaviors.</p>"},{"location":"reference/mount/#everest-for-windows","title":"Everest for Windows","text":"<p>Everest mount is available for Windows, in read-only mode.</p>"},{"location":"reference/mount/#everest-behavior-on-windows-operation-system","title":"Everest Behavior On Windows Operation System","text":"<p>CFAPI uses an OS-managed caching system optimized for cloud storage:</p> <ul> <li>Placeholder Files: Files initially appear as stubs containing only metadata, without actual content</li> <li>On-Demand Hydration: When accessed, files are \"hydrated\" - their content is fetched from lakeFS</li> <li>Local Cache: Subsequent reads are served directly from the local cache without reaching lakeFS</li> <li>Full Download: Currently, accessing any part of a file, triggers a full download</li> <li>Automatic Eviction: The OS \"dehydrates\" (clears content) under storage pressure. All data is deleted after unmount</li> </ul>"},{"location":"reference/mount/#requirements","title":"Requirements","text":"<ul> <li>Everest Mount supports the windows's native Cloud Filter API. No need in additional installations.</li> <li>CFAPI Support Starts at Windows 10, version 1709.</li> <li>Make sure your Everest version is &gt; <code>0.6.0</code></li> </ul>"},{"location":"reference/mount/#skip-scan-in-microsoft-protection-preferences-windows-defender","title":"Skip Scan in Microsoft Protection Preferences (Windows Defender)","text":"<p>AV will try to fully scan all files contents,  please make sure to must disable firewall for the mounted path. This can be done via the UI or in Powershell With an admin user:</p> <pre><code>Add-MpPreference -ExclusionPath \"Path\\To\\Mount-Dir\"\n</code></pre> <p>Verify exclusions:</p> <pre><code>Get-MpPreference | Select-Object -ExpandProperty ExclusionPath\n</code></pre>"},{"location":"reference/mount/#exclude-mount-directory-from-windows-search-indexing","title":"Exclude Mount Directory from Windows Search Indexing","text":"<p>Windows Search automatically tries to index the newly available files recursively walking through the directory structure. For large repositories this can take a long time and impact performance, so it's recommended to exclude the mount directory from indexing.</p> <p>How To Exclude Your Mount Folder (Recommended)</p> <ol> <li>Open Settings &gt; Privacy &amp; security &gt; Searching Windows</li> <li>Under \"Excluded Folders\", click \"Add an excluded folder\"</li> <li>Select your mount directory (e.g., C:\\Users\\me\\mounted)</li> <li>The indexer will stop trying to index that location</li> </ol>"},{"location":"reference/mount/#everest-on-kubernetes-csi-driver","title":"Everest on Kubernetes (CSI Driver)","text":"<p>Private Preview</p> <p>The CSI Driver is in private preview. Please contact us to get access. The driver currently provides only read-only access.</p> <p>The lakeFS CSI (Container Storage Interface) Driver allows Kubernetes Pods to mount and interact with data in a lakeFS repository as if it were a local filesystem.</p> <p>In this section:</p> <ul> <li>How it Works - Understanding the CSI driver architecture</li> <li>Status and Limitations - Supported platforms and current limitations</li> <li>Prerequisites - Requirements for deploying the CSI driver</li> <li>Deploy the CSI Driver - Installation instructions using Helm</li> <li>Use in Pods - How to mount lakeFS URIs in your Kubernetes workloads</li> <li>Troubleshooting - Common issues and debugging steps</li> </ul>"},{"location":"reference/mount/#how-it-works","title":"How it Works","text":"<p>The CSI driver, installed in your cluster, orchestrates mount operations on each Kubernetes node. It does not execute <code>mount</code> commands directly. Instead, it communicates via a Unix socket with a <code>systemd</code> service running on the host. This service is responsible for executing the <code>everest mount</code> and <code>umount</code> commands, making lakeFS URIs available to Pods as persistent volumes.</p>"},{"location":"reference/mount/#status-and-limitations","title":"Status and Limitations","text":"<ul> <li>Tested OS: BottleRocket-OS, Amazon Linux 2, RHEL 8.</li> <li>Kubernetes: Version <code>&gt;=1.23.0</code>.</li> <li>Provisioning: Static provisioning only.</li> <li>Access Modes: <code>ReadOnlyMany</code> is supported.</li> <li>Security Context: Setting Pod <code>securityContext</code> (e.g., <code>runAsUser</code>) is not currently supported.</li> </ul>"},{"location":"reference/mount/#prerequisites_1","title":"Prerequisites","text":"<ol> <li>lakeFS Enterprise Version <code>1.25.0</code> or higher.</li> <li>A Kubernetes cluster (<code>&gt;=1.23.0</code>) with Helm installed.</li> <li>Network access from the cluster pods to your lakeFS server.</li> <li>Access to the <code>treeverse/everest-lakefs-csi-driver</code> Docker Hub image.</li> </ol>"},{"location":"reference/mount/#deploy-the-csi-driver","title":"Deploy the CSI Driver","text":"<p>The driver is deployed using a Helm chart.</p> <ol> <li> <p>Add the lakeFS Helm repository: <pre><code>helm repo add lakefs https://charts.lakefs.io\nhelm repo update lakefs\n</code></pre>     Verify the chart is available and see the latest version:     <pre><code>helm search repo lakefs/everest-lakefs-csi-driver\n</code></pre>     To see all available chart versions, use the <code>-l</code> flag:     <pre><code>helm search repo lakefs/everest-lakefs-csi-driver -l\n</code></pre></p> </li> <li> <p>Configure <code>values.yaml</code>:     Create a <code>values.yaml</code> file to configure the driver. At a minimum, you must provide credentials for Docker Hub and your lakeFS server.     You can view the complete list of configuration options by running <code>helm show values lakefs/everest-lakefs-csi-driver --version &lt;version&gt;</code>.</p> <p><code>values.yaml</code> example</p> <pre><code># Docker Hub credentials to pull the CSI driver image\nimagePullSecret:\n  token: &lt;dockerhub-token&gt;\n  username: &lt;dockerhub-user&gt;\n\n# Default lakeFS credentials for Everest to use when mounting volumes\nlakeFSAccessSecret:\n  keyId: &lt;lakefs-key-id&gt;\n  accessKey: &lt;lakefs-access-key&gt;\n  endpoint: &lt;lakefs-endpoint&gt;\n\nnode:\n  # Logging verbosity (0-4 is normal, 5 is most verbose)\n  logLevel: 4\n  # (Advanced) Only set if you have issues with the Everest binary installation on the node.\n  # This path must end with a \"/\"\n  # everestInstallPath: /opt/everest-mount/bin/\n\n# (Advanced) Additional environment variables for the CSI driver pod\n# extraEnvVars:\n#   - name: CSI_DRIVER_MOUNT_TIMEOUT\n#     value: \"30s\"\n</code></pre> </li> <li> <p>Install the chart: <pre><code>helm install -f values.yaml lakefs lakefs/everest-lakefs-csi-driver --version &lt;chart-version&gt;\n</code></pre></p> </li> </ol>"},{"location":"reference/mount/#use-in-pods","title":"Use in Pods","text":"<p>To use the driver, you create a <code>PersistentVolume</code> (PV) and a <code>PersistentVolumeClaim</code> (PVC) to mount a lakeFS URI into your Pod.</p> <ul> <li>Static Provisioning: You must set <code>storageClassName: \"\"</code> in your PVC. To ensure a PVC is bound to a specific PV, you can use a <code>claimRef</code> in the PV definition to create a one-to-one mapping.</li> <li>Mount URI: The <code>lakeFSMountUri</code> is the only required attribute in the PV spec.</li> <li>Mount Options: Additional <code>everest mount</code> flags can be passed via <code>mountOptions</code> in the PV spec.</li> </ul> Examples <p>The following examples demonstrate how to mount a lakeFS URI in different Kubernetes scenarios.</p> Single Pod and MountMultiple Pods, One Mount (Deployment)Multiple Mounts, Single PodStatefulSet (Advanced)Mount Options <p>This example mounts a single lakeFS URI into one Pod. <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: everest-pv\nspec:\n  capacity:\n    storage: 100Gi # Required by Kubernetes, but ignored by Everest\n  accessModes:\n    - ReadOnlyMany\n  csi:\n    driver: csi.everest.lakefs.io\n    volumeHandle: everest-csi-driver-volume-1 # Must be unique\n    volumeAttributes:\n      # Replace with your lakeFS mount URI\n      lakeFSMountUri: lakefs://&lt;repo&gt;/&lt;ref&gt;/&lt;path&gt;\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: everest-claim\nspec:\n  accessModes:\n    - ReadOnlyMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 5Gi # Required by Kubernetes, but ignored by Everest\n  volumeName: everest-pv\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: everest-app\nspec:\n  containers:\n    - name: app\n      image: rockylinux/rockylinux\n      command: [\"/bin/sh\", \"-c\", \"ls /data/; tail -f /dev/null\"]\n      volumeMounts:\n        - name: my-lakefs-data\n          mountPath: /data\n  volumes:\n    - name: my-lakefs-data\n      persistentVolumeClaim:\n        claimName: everest-claim\n</code></pre></p> <p>A Deployment where multiple Pods share the same lakeFS mount. Each Pod gets its own independent mount. <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: multiple-pods-one-pv\nspec:\n  capacity:\n    storage: 100Gi\n  accessModes:\n    - ReadOnlyMany\n  csi:\n    driver: csi.everest.lakefs.io\n    volumeHandle: everest-csi-driver-volume-2 # Must be unique\n    volumeAttributes:\n      lakeFSMountUri: lakefs://&lt;repo&gt;/&lt;ref&gt;/&lt;path&gt;\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: multiple-pods-one-claim\nspec:\n  accessModes:\n    - ReadOnlyMany\n  storageClassName: \"\"\n  resources:\n    requests:\n      storage: 5Gi\n  volumeName: multiple-pods-one-pv\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: multi-pod-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: multi-pod-app\n  template:\n    metadata:\n      labels:\n        app: multi-pod-app\n    spec:\n      containers:\n      - name: app\n        image: rockylinux/rockylinux\n        command: [\"/bin/sh\", \"-c\", \"ls /data/; tail -f /dev/null\"]\n        volumeMounts:\n        - name: lakefs-storage\n          mountPath: /data\n      volumes:\n      - name: lakefs-storage\n        persistentVolumeClaim:\n          claimName: multiple-pods-one-claim\n</code></pre></p> <p>A single Pod with two different lakeFS URIs mounted to two different paths. <pre><code># Define two PVs and two PVCs, one for each mount.\n# PV 1\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: multi-mount-pv-1\nspec:\n  capacity: { storage: 100Gi }\n  accessModes: [ReadOnlyMany]\n  csi:\n    driver: csi.everest.lakefs.io\n    volumeHandle: everest-csi-driver-volume-3 # Must be unique\n    volumeAttributes:\n      lakeFSMountUri: lakefs://&lt;repo&gt;/&lt;ref&gt;/&lt;path1&gt;\n---\n# PVC 1\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: multi-mount-claim-1\nspec:\n  accessModes: [ReadOnlyMany]\n  storageClassName: \"\"\n  resources: { requests: { storage: 5Gi } }\n  volumeName: multi-mount-pv-1\n---\n# PV 2\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: multi-mount-pv-2\nspec:\n  capacity: { storage: 100Gi }\n  accessModes: [ReadOnlyMany]\n  csi:\n    driver: csi.everest.lakefs.io\n    volumeHandle: everest-csi-driver-volume-4 # Must be unique\n    volumeAttributes:\n      lakeFSMountUri: lakefs://&lt;repo&gt;/&lt;ref&gt;/&lt;path2&gt;\n---\n# PVC 2\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: multi-mount-claim-2\nspec:\n  accessModes: [ReadOnlyMany]\n  storageClassName: \"\"\n  resources: { requests: { storage: 5Gi } }\n  volumeName: multi-mount-pv-2\n---\n# Pod\napiVersion: v1\nkind: Pod\nmetadata:\n  name: multi-mount-pod\nspec:\n  containers:\n    - name: app\n      image: rockylinux/rockylinux\n      command: [\"/bin/sh\", \"-c\", \"echo 'Path 1:'; ls /data1; echo 'Path 2:'; ls /data2; tail -f /dev/null\"]\n      volumeMounts:\n        - name: lakefs-data-1\n          mountPath: /data1\n        - name: lakefs-data-2\n          mountPath: /data2\n  volumes:\n    - name: lakefs-data-1\n      persistentVolumeClaim:\n        claimName: multi-mount-claim-1\n    - name: lakefs-data-2\n      persistentVolumeClaim:\n        claimName: multi-mount-claim-2\n</code></pre></p> <p>Due to the nuances of how StatefulSets manage PersistentVolumeClaims, it is often simpler to use a <code>Deployment</code>.</p> <ul> <li>Deletion: When you delete a StatefulSet, its PVCs are not automatically deleted. You must delete them manually.</li> <li>Replicas &gt; 1: Using more than one replica requires manually creating a corresponding number of <code>PersistentVolume</code> resources, as static provisioning does not automatically create them.</li> </ul> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: sts-simple-mount\n  labels:\n    app: sts-app-simple-everest\nspec:\n  capacity:\n    storage: 100Gi # ignored, required\n  accessModes:\n    - ReadOnlyMany\n  csi:\n    driver: csi.everest.lakefs.io\n    volumeHandle: everest-csi-driver-volume-5 # Must be unique\n    volumeAttributes:\n      lakeFSMountUri: &lt;LAKEFS_MOUNT_URI&gt;\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sts-app-simple-everest\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sts-app-simple-everest\n  template:\n    metadata:\n      labels:\n        app: sts-app-simple-everest\n    spec:\n      containers:\n        - name: app\n          image: rockylinux/rockylinux\n          command: [\"/bin/sh\", \"-c\", \"ls /data/; tail -f /dev/null\"]\n          volumeMounts:\n            - name: sts-simple-mount\n              mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: sts-simple-mount\n    spec:\n      selector:\n        matchLabels:\n          app: sts-app-simple-everest\n      storageClassName: \"\" # required for static provisioning\n      accessModes: [ \"ReadOnlyMany\" ]\n      resources:\n        requests:\n          storage: 5Gi # ignored, required\n</code></pre> <p>This example demonstrates how to pass various <code>everest mount</code> flags via <code>mountOptions</code> in the <code>PersistentVolume</code> spec.</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: options-demo-pv\nspec:\n  capacity:\n    storage: 100Gi # ignored, required\n  accessModes:\n    - ReadOnlyMany\n  # everest mount flags are passed here\n  mountOptions:\n    # set cache size in bytes\n    - cache-size 10000000\n    # set log level to trace for debugging (very noisy!)\n    - log-level trace\n    # WARN: Overriding credentials should only be used in advanced cases.\n    # It is more secure to rely on the default credentials configured in the CSI driver.\n    - lakectl-access-key-id &lt;LAKEFS_ACCESS_KEY_ID&gt;\n    - lakectl-secret-access-key &lt;LAKEFS_SECRET_ACCESS_KEY&gt;\n    - lakectl-server-url &lt;LAKEFS_ENDPOINT&gt;\ncsi:\n  driver: csi.everest.lakefs.io\n  volumeHandle: everest-csi-driver-volume-6 # Must be unique\n  volumeAttributes:\n    lakeFSMountUri: &lt;LAKEFS_MOUNT_URI&gt;\n---\n# PVC and Pod definitions follow...\n</code></pre>"},{"location":"reference/mount/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Check logs from the CSI driver pods and the application pod that failed to mount.</li> <li>Inspect the events and status of the <code>PV</code> and <code>PVC</code> (<code>kubectl get pv</code>, <code>kubectl get pvc</code>, <code>kubectl describe ...</code>).</li> <li>Advanced: SSH into the Kubernetes node to inspect the <code>systemd</code> service logs for the specific mount operation-<ol> <li>Find the failed mount service:     <pre><code>systemctl list-units --type=service | grep everest-lakefs-mount\n# Example output:\n# everest-lakefs-mount-0.0.8-everest-123.service loaded active running CSI driver FUSE daemon\n</code></pre></li> <li>Get the status and view the exact command that was executed:     <pre><code>systemctl status everest-lakefs-mount-0.0.8-everest-123.service\n</code></pre></li> <li>View the logs for the service:     <pre><code>journalctl -f -u everest-lakefs-mount-0.0.8-everest-123.service\n</code></pre></li> <li>View full resolved unit:     <pre><code>systemctl cat everest-lakefs-mount-0.0.8-everest-123.service\n</code></pre></li> </ol> </li> </ul>"},{"location":"reference/mount/#command-line-reference","title":"Command-Line Reference","text":"<p>This section provides detailed documentation for all Everest CLI commands. For conceptual information about how Everest works, see the Core Concepts section.</p>"},{"location":"reference/mount/#everest-mount","title":"<code>everest mount</code>","text":"<p>Mounts a lakeFS URI to a local directory.</p> <pre><code>everest mount &lt;lakefs_uri&gt; &lt;mount_directory&gt; [flags]\n</code></pre> <p>Tips:</p> <ul> <li>Since the server runs in the background, use <code>--log-output /path/to/file</code> to view logs.</li> <li>The optimal cache size is the size of the data you are going to read/write.</li> <li>To reuse the cache between restarts of the same mount, set the <code>--cache-dir</code> flag.</li> <li>In read-only mode, if you provide a branch or tag, Everest will resolve and mount the HEAD commit. For a stable mount, use a specific commit ID in the URI.</li> </ul> <p>Flags:</p> <ul> <li><code>--write-mode</code>: Enable write mode (default: <code>false</code>).</li> <li><code>--cache-dir</code>: Directory to cache files.</li> <li><code>--cache-size</code>: Size of the local cache in bytes.</li> <li><code>--cache-create-provided-dir</code>: If <code>cache-dir</code> is provided and does not exist, create it.</li> <li><code>--listen</code>: Address for the mount server to listen on.</li> <li><code>--no-spawn</code>: Do not spawn a new server; assume one is already running.</li> <li><code>--protocol</code>: Protocol to use (default: <code>nfs</code>), for Windows use --cfapi.</li> <li><code>--log-level</code>: Set logging level.</li> <li><code>--log-format</code>: Set logging output format.</li> <li><code>--log-output</code>: Set logging output(s).</li> <li><code>--presign</code>: Use pre-signed URLs for direct object store access (default: <code>true</code>).</li> </ul> `everest umount` <p>Unmounts a lakeFS directory.</p> <pre><code>everest umount &lt;mount_directory&gt;\n</code></pre> `everest diff` (Write Mode Only) <p>Shows the difference between the local mount directory and the source branch.</p> <pre><code>everest diff [mount_directory]\n</code></pre> `everest commit` (Write Mode Only) <p>Commits local changes to the source lakeFS branch. The new commit is merged to the original branch using a <code>source-wins</code> strategy. After the commit succeeds, the mounted directory's source commit is updated to the new HEAD of the branch.</p> <p>Warning</p> <p>Writes to a mount directory during a commit operation may be lost.</p> <pre><code>everest commit [mount_directory] -m &lt;commit_message&gt;\n</code></pre> `everest mount-server` (Advanced) <p>Starts the mount server without performing the OS-level mount. This is intended for advanced use cases where you want to manage the server process and the OS mount command separately.</p> <pre><code>everest mount-server &lt;remote_mount_uri&gt; [flags]\n</code></pre> <p>Flags:</p> <ul> <li><code>--cache-dir</code>: Directory to cache read files and metadata.</li> <li><code>--cache-create-provided-dir</code>: Create the cache directory if it does not exist.</li> <li><code>--listen</code>: Address to listen on.</li> <li><code>--protocol</code>: Protocol to use (nfs | fuse | cfapi).</li> <li><code>--callback-addr</code>: Callback address to report back to.</li> <li><code>--log-level</code>: Set logging level.</li> <li><code>--log-format</code>: Set logging output format.</li> <li><code>--log-output</code>: Set logging output(s).</li> <li><code>--cache-size</code>: Size of the local cache in bytes.</li> <li><code>--parallelism</code>: Number of parallel downloads for metadata.</li> <li><code>--presign</code>: Use presign for downloading.</li> <li><code>--write-mode</code>: Enable write mode (default: false).</li> <li><code>--root</code>: Directory to mount on the the filesystem (Windows only)</li> </ul>"},{"location":"reference/mount/#advanced-topics","title":"Advanced Topics","text":""},{"location":"reference/mount/#write-mode-limitations","title":"Write Mode Limitations","text":"<p>Windows Support</p> <p>Currently, Everest for Windows supportd only read-mode operations</p> <p>When using write mode (<code>--write-mode</code>), be aware of the following limitations and modified behaviors. For more details on write mode operations, see the Write-Mode Operations section.</p> Unsupported Operations <ul> <li>Rename: File and directory rename operations are not supported.</li> <li>Temporary Files: Temporary files are not supported.</li> <li>Hard/Symbolic Links: Hard links and symbolic links are not supported.</li> <li>POSIX File Locks: POSIX file locks (<code>lockf</code>) are not supported.</li> <li>POSIX Permissions: POSIX permissions are not supported. Default permissions are assigned to files and directories.</li> </ul> Modified Behavior <ul> <li>Metadata Operations: Modifying file metadata (<code>chmod</code>, <code>chown</code>, <code>chgrp</code>, time attributes) results in a no-op. The file metadata will not be changed.</li> <li>Directory Removal: Calling <code>remove</code> on a directory is not supported. Use appropriate directory removal commands (e.g. <code>rm -r</code>) instead, and on the next commit, the directory will be deleted.</li> </ul> Functionality Limitations <ul> <li>Empty Directories: Newly created empty directories will not reflect as directory markers in lakeFS.</li> <li>Path Conflicts: lakeFS allows having two path keys where one is a \"directory\" prefix of the other (e.g., both <code>animals/cat.png</code> and <code>animals</code> as an empty object are valid in lakeFS). However, since a filesystem cannot contain both a file and a directory with the same name, this will lead to undefined behavior depending on the filesystem type.</li> </ul>"},{"location":"reference/mount/#integration-with-git","title":"Integration with Git","text":"<p>It is safe to mount a lakeFS path inside a Git repository. Everest automatically creates a virtual <code>.gitignore</code> file in the mount directory. This file instructs Git to ignore all mounted content except for a single file: <code>.everest/source</code>.</p> <p>By committing the <code>.everest/source</code> file, which contains the <code>lakefs://</code> URI, you ensure that anyone who clones your Git repository and uses Everest will mount the exact same version of the data, making your project fully reproducible.</p> <p>Reproducible Data Science Projects</p> <p>This feature is particularly useful for data science projects where you want to version both your code (in Git) and your data (in lakeFS). Team members can clone the repository and automatically mount the correct data version.</p>"},{"location":"reference/mount/#faq","title":"FAQ","text":""},{"location":"reference/mount/#how-does-data-access-work-does-it-stream-through-the-lakefs-server","title":"How does data access work? Does it stream through the lakeFS server?","text":"<p>No. By default (<code>--presign=true</code>), Everest uses pre-signed URLs to read and write data directly to and from the underlying object store, ensuring high performance. Metadata operations still go through the lakeFS server.</p> <p>For more details, see Performance Considerations.</p>"},{"location":"reference/mount/#what-happens-if-the-lakefs-branch-is-updated-after-i-mount-it","title":"What happens if the lakeFS branch is updated after I mount it?","text":"<p>In read-only mode, your mount points to the commit that was at the HEAD of the branch at the time of mounting. It will not reflect subsequent commits to that branch unless you unmount and remount. In write mode, after a successful <code>commit</code>, the mount is updated to the new HEAD of the branch.</p>"},{"location":"reference/mount/#when-are-files-downloaded","title":"When are files downloaded?","text":"<p>Everest uses a lazy fetching strategy. Files are only downloaded when their content is accessed (e.g., with <code>cat</code>, <code>open</code>, or reading in a script). Metadata-only operations like <code>ls</code> do not trigger downloads.</p> <p>Downloaded files are cached locally for performance. See Cache Behavior for details on how caching works and how to configure it.</p>"},{"location":"reference/mount/#what-are-the-rbac-permissions-required-for-mounting","title":"What are the RBAC permissions required for mounting?","text":"<p>You can use lakeFS's Role-Based Access Control to manage access.</p> <p>Minimal Read-Only Permissions:</p> <pre><code>{\n  \"id\": \"MountReadOnlyPolicy\",\n  \"statement\": [\n    {\n      \"action\": [\"fs:ReadObject\"],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:fs:::repository/&lt;repo&gt;/object/&lt;prefix&gt;/*\"\n    },\n    {\n      \"action\": [\"fs:ListObjects\", \"fs:ReadCommit\", \"fs:ReadBranch\", \"fs:ReadTag\", \"fs:ReadRepository\"],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:fs:::repository/&lt;repo&gt;\"\n    },\n    { \"action\": [\"fs:ReadConfig\"], \"effect\": \"allow\", \"resource\": \"*\" }\n  ]\n}\n</code></pre> <p>Minimal Write-Mode Permissions:</p> <pre><code>{\n  \"id\": \"MountWritePolicy\",\n  \"statement\": [\n    {\n      \"action\": [\"fs:ReadObject\", \"fs:WriteObject\", \"fs:DeleteObject\"],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:fs:::repository/&lt;repo&gt;/object/&lt;prefix&gt;/*\"\n    },\n    {\n      \"action\": [\n        \"fs:ListObjects\", \"fs:ReadCommit\", \"fs:ReadBranch\", \"fs:ReadRepository\",\n        \"fs:CreateCommit\", \"fs:CreateBranch\", \"fs:DeleteBranch\", \"fs:RevertBranch\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:fs:::repository/&lt;repo&gt;\"\n    },\n    { \"action\": [\"fs:ReadConfig\"], \"effect\": \"allow\", \"resource\": \"*\" }\n  ]\n}\n</code></pre>"},{"location":"reference/mount/#why-use-lakefs-mount-instead-of-lakectl-local","title":"Why use lakeFS Mount instead of <code>lakectl local</code>?","text":"<p>While both tools work with local data, they serve different needs. Use <code>lakectl local</code> for Git-like workflows where you need to pull and push entire directories. Use lakeFS Mount when you need immediate, on-demand access to a large repository without downloading it first, making it ideal for exploration, training ML models, or any task that benefits from lazy loading.</p>"},{"location":"reference/s3/","title":"S3-Supported API","text":"<p>The S3 Gateway emulates a subset of the API exposed by S3. This subset includes all API endpoints relevant to data systems.</p> <p>Tip</p> <p>For more information, see architecture.</p> <p>lakeFS supports the following API operations:</p> <ol> <li>Identity and authorization<ol> <li>SIGv2</li> <li>SIGv4</li> </ol> </li> <li>Bucket operations:<ol> <li>ListBuckets</li> <li>HEAD bucket</li> </ol> </li> <li>Object operations:<ol> <li>DeleteObject</li> <li>DeleteObjects</li> <li>GetObject<ol> <li>Support for caching headers, ETag</li> <li>Support for range requests</li> <li>Support for reading user metadata.</li> <li>No support for SSE</li> <li>No support for SelectObject operations</li> </ol> </li> <li>HeadObject</li> <li>PutObject<ol> <li>Support multi-part uploads</li> <li>Support for writing user metadata.</li> <li>No support for storage classes</li> <li>No object level tagging</li> </ol> </li> <li>CopyObject</li> </ol> </li> <li>Object Listing:<ol> <li>ListObjects</li> <li>ListObjectsV2</li> <li>Delimiter support (for <code>\"/\"</code> only)</li> </ol> </li> <li>Multipart Uploads:<ol> <li>AbortMultipartUpload</li> <li>CompleteMultipartUpload</li> <li>CreateMultipartUpload</li> <li>ListParts Currently supported only on AWS S3. Link to tracked issue</li> <li>ListMultipartUploads Currently supported only on AWS S3. Link to tracked issue</li> <li>Upload Part</li> <li>UploadPartCopy</li> </ol> </li> </ol>"},{"location":"reference/spark-client/","title":"lakeFS Spark Metadata Client","text":"<p>Utilize the power of Spark to interact with the metadata on lakeFS. Possible use cases include:</p> <ul> <li>Creating a DataFrame for listing the objects in a specific commit or branch.</li> <li>Computing changes between two commits.</li> <li>Exporting your data for consumption outside lakeFS.</li> <li>Bulk operations on the underlying storage.</li> </ul>"},{"location":"reference/spark-client/#getting-started","title":"Getting Started","text":"<p>Note</p> <p>Please note that Spark 2.x is no longer supported with the lakeFS metadata client.</p> <p>The Spark metadata client is compiled for Spark 3.1.2 with Hadoop 3.2.1, but can work for other Spark versions and higher Hadoop versions.</p> PySpark, spark-shell, spark-submit, spark-sqlDatabricks <p>Start Spark Shell / PySpark with the <code>--packages</code> flag, for instance:</p> <pre><code>spark-shell --packages io.lakefs:lakefs-spark-client_2.12:0.17.0\n</code></pre> <p>Alternatively use the assembled jar (an \"\u00dcberjar\") on S3, from <code>s3://treeverse-clients-us-east/lakefs-spark-client/0.17.0/lakefs-spark-client-assembly-0.17.0.jar</code> by passing its path to <code>--jars</code>.</p> <p>The assembled jar is larger but shades several common libraries.  Use it if Spark complains about bad classes or missing methods.</p> <p>Include this assembled jar (an \"\u00dcberjar\") from S3, from <code>s3://treeverse-clients-us-east/lakefs-spark-client/0.17.0/lakefs-spark-client-assembly-0.17.0.jar</code>.</p>"},{"location":"reference/spark-client/#configuration","title":"Configuration","text":"<p>To read metadata from lakeFS, the client should be configured with your lakeFS endpoint and credentials, using the following Hadoop configurations:</p> Configuration Description <code>spark.hadoop.lakefs.api.url</code> lakeFS API endpoint, e.g: <code>http://lakefs.example.com/api/v1</code> <code>spark.hadoop.lakefs.api.access_key</code> The access key to use for fetching metadata from lakeFS <code>spark.hadoop.lakefs.api.secret_key</code> Corresponding lakeFS secret key"},{"location":"reference/spark-client/#examples","title":"Examples","text":"<p>Get a DataFrame for listing all objects in a commit</p> <pre><code>import io.treeverse.clients.LakeFSContext\n\nval commitID = \"a1b2c3d4\"\nval df = LakeFSContext.newDF(spark, \"example-repo\", commitID)\ndf.show\n/* output example:\n+------------+--------------------+--------------------+-------------------+----+\n|        key |             address|                etag|      last_modified|size|\n+------------+--------------------+--------------------+-------------------+----+\n|     file_1 |791457df80a0465a8...|7b90878a7c9be5a27...|2021-03-05 11:23:30|  36|\n|     file_2 |e15be8f6e2a74c329...|95bee987e9504e2c3...|2021-03-05 11:45:25|  36|\n|     file_3 |f6089c25029240578...|32e2f296cb3867d57...|2021-03-07 13:43:19|  36|\n|     file_4 |bef38ef97883445c8...|e920efe2bc220ffbb...|2021-03-07 13:43:11|  13|\n+------------+--------------------+--------------------+-------------------+----+\n*/\n</code></pre>"},{"location":"security/","title":"lakeFS Security Reference","text":""},{"location":"security/#understanding-your-data-security","title":"Understanding Your Data Security","text":"<p>At lakeFS, we understand the critical nature of data security. Thousands of organizations worldwide rely on lakeFS to manage their data with confidence. Here's a few concepts we follow to ensure your data remains secure:</p> <p>Data Stays in Place: The data you version control remains within your existing object storage. lakeFS creates metadata for your data without moving it. New data is stored in the bucket you designate within your object storage.</p> <p>lakeFS Servers Stores only Metadata: The lakeFS Server (also in the case of lakeFS Cloud) only stores the metadata used for version control operations (i.e. diff, merge). It does not store any of your actual data.</p> <p>Minimal Permissions: lakeFS requires minimal permissions to manage your data. We can even version data we cannot directly access by utilizing presigned URLs.</p> <p>Learn more about some of the featured that help keep lakeFS Secure:</p> <ul> <li>Authentication - An overview of the authentication and authorization mechanisms available in lakeFS, including built-in and external services, API and S3 Gateway authentication, and user permissions management.</li> <li>Remote Authenticator - This feature allows organizations to leverage their existing identity infrastructure while using lakeFS, providing a flexible and secure authentication mechanism.</li> <li>Role-Based Access Control (RBAC) - RBAC with lakeFS provides a flexible and granular approach to managing access and permissions, similar to other cloud-based systems like AWS IAM.</li> <li>Presigned URL - This feature allows for more flexible and direct data access in lakeFS, particularly useful for scenarios where bypassing the lakeFS server for data retrieval or storage is beneficial.</li> <li>Single Sign On (SSO) - lakeFS provides administrators with the necessary information to set up SSO for both lakeFS Cloud and Enterprise editions, covering various authentication protocols and identity providers.</li> <li>Short Lived Token (STS like) - This feature allows lakeFS to leverage temporary credentials for secure and flexible authentication, integrating seamlessly with existing identity providers.</li> <li>Login to lakeFS with AWS IAM - This feature enhances the integration between lakeFS and AWS by supporting authenticating users programmatically using AWS IAM roles instead of using static lakeFS access and secret keys.</li> </ul>"},{"location":"security/#soc2-compliance","title":"SOC2 Compliance","text":"<p>lakeFS Cloud is SOC2 compliant, demonstrating our commitment to stringent security standards.</p>"},{"location":"security/#more-questions-contact-us","title":"More questions? Contact us","text":"<p>For details on supported versions, security updates, and vulnerability reporting, please refer to our security policy on GitHub.</p> <p>If you have additional questions regarding lakeFS Security, talk to an expert.</p>"},{"location":"security/ACL-server-implementation/","title":"ACL Server Implementation","text":""},{"location":"security/ACL-server-implementation/#overview","title":"Overview","text":"<p>This guide explains how to implement an ACL (Access Control List) server and configure lakeFS OSS to work with it. This is intended for contributors who want to understand or extend the ACL authentication mechanism in lakeFS.</p> <p>Contents:</p> <ol> <li>Required APIs for implementing an ACL server.</li> <li>How to configure lakeFS OSS to connect to your ACL server.</li> <li>How to run lakeFS OSS with your ACL server.</li> </ol>"},{"location":"security/ACL-server-implementation/#what-is-acl","title":"What is ACL?","text":"<p>Access Control List (ACL) in lakeFS manages permissions by associating a set of permissions directly with a specific object or a group of objects. In the context of the lakeFS authorization API, ACLs are represented within policies. These policies can then be attached to users or groups to grant them the specified permissions.</p>"},{"location":"security/ACL-server-implementation/#implementation-and-setup","title":"Implementation and Setup","text":"<p>Follow these steps to implement an ACL server compatible with lakeFS.</p>"},{"location":"security/ACL-server-implementation/#1-implementation","title":"1. Implementation","text":"<p>To implement the ACL server, you need to implement a subset of the APIs defined in the authorization.yaml specification. Not all APIs in the specification are required \u2014 only those listed below, grouped into the following categories:</p> <ul> <li>Credentials</li> <li>Users</li> <li>Groups</li> <li>Policies</li> </ul> <p>Implement all APIs under these categories.</p> <p>Info</p> <p>For detailed descriptions of the different schemas and each API, including their input and output parameters, refer to each API in the authorization.yaml specification.</p>"},{"location":"security/ACL-server-implementation/#credentials-apis","title":"Credentials APIs","text":"<p>These APIs are used to manage credentials (access key ID and secret access key) for users.</p> <p>Implement the following endpoints under the <code>credentials</code> tag in the authorization.yaml specification:</p> <ul> <li> <p><code>GET /auth/users/{userId}/credentials</code>:</p> <ul> <li>Description: Returns a list of all access_key_ids and their creation dates for a specific user.</li> <li>Input: <code>userId</code> (path parameter), pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>CredentialsList</code> object containing a list of <code>Credentials</code> objects and pagination information.</li> <li>Implementation Notes: The results should be sorted by <code>access_key_id</code>.</li> <li> <p>Output Schema (<code>CredentialsList</code>):</p> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/Credentials\"\n</code></pre> </li> <li> <p>Output Schema (<code>Credentials</code>):</p> <pre><code>type: object\nproperties:\n    access_key_id:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds\n</code></pre> </li> </ul> </li> <li> <p><code>POST /auth/users/{userId}/credentials</code>:</p> <ul> <li>Description: Creates new credentials for a specific user.</li> <li>Input: <code>userId</code> (path parameter), optional query parameters (<code>access_key</code>, <code>secret_key</code>).</li> <li>Output: A <code>CredentialsWithSecret</code> object containing the generated or provided access key ID, secret access key, creation date, and username.</li> <li>Implementation Notes: If <code>access_key</code> or <code>secret_key</code> are empty, the server should generate random values. The <code>username</code> field in the response is required.</li> <li> <p>Output Schema (<code>CredentialsWithSecret</code>):</p> <pre><code>type: object\nproperties:\n    access_key_id:\n        type: string\n    secret_access_key:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    user_name:\n        type: string\n        description: A unique identifier for the user.\n</code></pre> </li> </ul> </li> <li> <p><code>DELETE /auth/users/{userId}/credentials/{accessKeyId}</code>:</p> <ul> <li>Description: Deletes credentials for a specific user.</li> <li>Input: <code>userId</code> (path parameter), <code>accessKeyId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the user and credentials exist before deleting.</li> </ul> </li> <li> <p><code>GET /auth/users/{userId}/credentials/{accessKeyId}</code>:</p> <ul> <li>Description: Returns a specific user's credentials details (excluding the secret key).</li> <li>Input: <code>userId</code> (path parameter), <code>accessKeyId</code> (path parameter).</li> <li>Output: A <code>Credentials</code> object containing the access key ID and creation date.</li> <li>Implementation Notes: Ensure the user and credentials exist. The secret access key should not be returned by this endpoint.</li> <li> <p>Output Schema (<code>Credentials</code>):</p> <pre><code>type: object\nproperties:\n    access_key_id:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds\n</code></pre> </li> </ul> </li> <li> <p><code>GET /auth/credentials/{accessKeyId}</code>:</p> <ul> <li>Description: Returns the credentials details associated with a specific accessKeyId (including the secret key).</li> <li>Input: <code>accessKeyId</code> (path parameter).</li> <li>Output: A <code>CredentialsWithSecret</code> object containing all credential details.</li> <li>Implementation Notes: This endpoint is used by lakeFS to authenticate requests using access key IDs and secret access keys. The <code>username</code> field in the response is required.</li> <li> <p>Output Schema (<code>CredentialsWithSecret</code>):</p> <pre><code>type: object\nproperties:\n    access_key_id:\n        type: string\n    secret_access_key:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    user_name:\n        type: string\n        description: A unique identifier for the user.\n</code></pre> </li> </ul> </li> </ul>"},{"location":"security/ACL-server-implementation/#users-apis","title":"Users APIs","text":"<p>These APIs are used to manage users.</p> <p>Implement the following endpoints under the <code>users</code> tag in the authorization.yaml specification:</p> <ul> <li> <p><code>GET /auth/users</code>:</p> <ul> <li>Description: Returns a list of all users.</li> <li>Input: Pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>UserList</code> object containing a list of <code>User</code> objects and pagination information.</li> <li>Implementation Notes: The results must be sorted by the username. The <code>external_id</code> and <code>encryptedPassword</code> fields in the <code>User</code> object are not used internally by lakeFS in the ACL implementation.</li> <li> <p>Output Schema (<code>UserList</code>):</p> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/User\"\n</code></pre> </li> <li> <p>Output Schema (<code>User</code>):</p> <pre><code>type: object\nproperties:\n    username:\n        type: string\n        description: A unique identifier for the user.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    friendly_name:\n        type: string\n        description: A name for the user that is not necessarily unique.\n    email:\n        type: string\n    source:\n        type: string\n        description: User source. Based on implementation.\n</code></pre> </li> </ul> </li> <li> <p><code>POST /auth/users</code>:</p> <ul> <li>Description: Creates a new user.</li> <li>Input: Request body containing <code>UserCreation</code> object (<code>username</code>, optional <code>email</code>, <code>friendlyName</code>, <code>source</code>, <code>external_id</code>, <code>invite</code>).</li> <li>Output: A <code>User</code> object representing the created user.</li> <li>Implementation Notes: The <code>username</code> must be unique. The <code>external_id</code> and <code>encryptedPassword</code> fields in the input and output are not used internally by lakeFS in the ACL implementation. If <code>invite</code> is true, an invitation email should be sent (if supported by the implementation).</li> <li> <p>Input Schema (<code>UserCreation</code>):</p> <pre><code>type: object\nproperties:\n    username:\n        type: string\n        minLength: 1\n        description: A unique identifier for the user.\n    email:\n        type: string\n        description: If provided, the email is set to the same value as the username.\n    friendlyName:\n        type: string\n    source:\n        type: string\n        description: User source. Based on implementation.\n    invite:\n        type: boolean\n        description: A boolean that determines whether an invitation email should be sent.\n</code></pre> </li> <li> <p>Output Schema (<code>User</code>):</p> <pre><code>type: object\nproperties:\n    username:\n        type: string\n        description: A unique identifier for the user.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    friendly_name:\n        type: string\n        description: A name for the user that is not necessarily unique.\n    email:\n        type: string\n    source:\n        type: string\n        description: User source. Based on implementation.\n</code></pre> </li> </ul> </li> <li> <p><code>GET /auth/users/{userId}</code>:</p> <ul> <li>Description: Returns the details of a specific user.</li> <li>Input: <code>userId</code> (path parameter).</li> <li>Output: A <code>User</code> object representing the user.</li> <li>Implementation Notes: Ensure the user exists. The <code>external_id</code> and <code>encryptedPassword</code> fields in the <code>User</code> object are not used internally by lakeFS in the ACL implementation.</li> <li> <p>Output Schema (<code>User</code>):</p> <pre><code>type: object\nproperties:\n    username:\n        type: string\n        description: A unique identifier for the user.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    friendly_name:\n        type: string\n        description: A name for the user that is not necessarily unique.\n    email:\n        type: string\n    source:\n        type: string\n        description: User source. Based on implementation.\n</code></pre> </li> </ul> </li> <li> <p><code>DELETE /auth/users/{userId}</code>:</p> <ul> <li>Description: Deletes a user.</li> <li>Input: <code>userId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the user exists. When a user is deleted, their associated credentials, group memberships, and policy attachments should also be removed.</li> </ul> </li> <li> <p><code>GET /auth/users/{userId}/groups</code>:</p> <ul> <li>Description: Returns the list of groups that a specific user is associated with.</li> <li>Input: <code>userId</code> (path parameter), pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>GroupList</code> object containing a list of <code>Group</code> objects and pagination information.</li> <li>Implementation Notes: The results must be sorted by the group name.</li> <li> <p>Output Schema (<code>GroupList</code>):</p> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/Group\"\n</code></pre> </li> <li> <p>Output Schema (<code>Group</code>):</p> <pre><code>type: object\nproperties:\n    id:\n        type: string\n        description: A unique identifier of the group.\n    name:\n        type: string\n        description: A unique identifier for the group, represented by a human-readable name.\n    description:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n</code></pre> </li> </ul> </li> <li> <p><code>GET /auth/users/{userId}/policies</code>:</p> <ul> <li>Description: Returns the list of policies associated with a specific user.</li> <li>Input: <code>userId</code> (path parameter), pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>), optional query parameter <code>effective</code> (boolean).</li> <li>Output: A <code>PolicyList</code> object containing a list of <code>Policy</code> objects and pagination information.</li> <li>Implementation Notes: If <code>effective</code> is true, return all distinct policies attached to the user directly or through their groups. If <code>effective</code> is false (default), return only policies directly attached to the user.</li> <li> <p>Output Schema (<code>PolicyList</code>):</p> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/Policy\"\n</code></pre> </li> <li> <p>Output Schema (<code>Policy</code>):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> </ul> </li> <li> <p><code>PUT /auth/users/{userId}/policies/{policyId}</code>:</p> <ul> <li>Description: Attaches a policy to a specific user.</li> <li>Input: <code>userId</code> (path parameter), <code>policyId</code> (path parameter).</li> <li>Output: No output on success (HTTP 201).</li> <li>Implementation Notes: Ensure the user and policy exist.</li> </ul> </li> <li> <p><code>DELETE /auth/users/{userId}/policies/{policyId}</code>:</p> <ul> <li>Description: Detaches a policy from a specific user.</li> <li>Input: <code>userId</code> (path parameter), <code>policyId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the user and policy attachment exist.</li> </ul> </li> </ul>"},{"location":"security/ACL-server-implementation/#groups-apis","title":"Groups APIs","text":"<p>These APIs are used to manage groups.</p> <p>Implement the following endpoints under the <code>groups</code> tag:</p> <ul> <li> <p><code>GET /auth/groups</code>:</p> <ul> <li>Description: Returns a list of groups.</li> <li>Input: Pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>GroupList</code> object containing a list of <code>Group</code> objects and pagination information.</li> <li>Implementation Notes: The results must be sorted by the group name.</li> <li>Output Schema (<code>GroupList</code>):</li> </ul> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/Group\"\n</code></pre> <ul> <li> <p>Output Schema (<code>Group</code>):</p> <pre><code>type: object\nproperties:\n    id:\n        type: string\n        description: A unique identifier of the group.\n    name:\n        type: string\n        description: A unique identifier for the group, represented by a human-readable name.\n    description:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n</code></pre> </li> </ul> </li> <li> <p><code>POST /auth/groups</code>:</p> <ul> <li>Description: Creates a new group.</li> <li>Input: Request body containing <code>GroupCreation</code> object (<code>id</code>, optional <code>description</code>).</li> <li>Output: A <code>Group</code> object representing the created group.</li> <li>Implementation Notes: The <code>id</code> must be a unique, human-readable name for the group. This endpoint is called during setup to create initial groups.</li> <li> <p>Input Schema (<code>GroupCreation</code>):</p> <pre><code>type: object\nrequired:\n    - id\nproperties:\n    id:\n        type: string\n        description: A unique identifier for the group, represented by a human-readable name.\n    description:\n        type: string\n</code></pre> </li> <li> <p>Output Schema (<code>Group</code>):</p> <pre><code>type: object\nproperties:\n    id:\n        type: string\n        description: A unique identifier of the group.\n    name:\n        type: string\n        description: A unique identifier for the group, represented by a human-readable name.\n    description:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n</code></pre> </li> </ul> </li> <li> <p><code>GET /auth/groups/{groupId}</code>:</p> <ul> <li>Description: Returns the details of a specific group.</li> <li>Input: <code>groupId</code> (path parameter).</li> <li>Output: A <code>Group</code> object representing the group.</li> <li>Implementation Notes: Ensure the group exists.</li> <li> <p>Output Schema (<code>Group</code>):</p> <pre><code>type: object\nproperties:\n    id:\n        type: string\n        description: A unique identifier of the group.\n    name:\n        type: string\n        description: A unique identifier for the group, represented by a human-readable name.\n    description:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n</code></pre> </li> </ul> </li> <li> <p><code>DELETE /auth/groups/{groupId}</code>:</p> <ul> <li>Description: Deletes a group.</li> <li>Input: <code>groupId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the group exists. When a group is deleted, its associated user memberships and policy attachments should also be removed.</li> </ul> </li> <li> <p><code>GET /auth/groups/{groupId}/members</code>:</p> <ul> <li>Description: Returns the list of users associated with a specific group.</li> <li>Input: <code>groupId</code> (path parameter), pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>UserList</code> object containing a list of <code>User</code> objects and pagination information.</li> <li>Implementation Notes: The results must be sorted by the username. The <code>external_id</code> and <code>encryptedPassword</code> fields in the <code>User</code> object are not used internally by lakeFS in the ACL implementation.</li> <li> <p>Output Schema (<code>UserList</code>):</p> <p><pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/User\"\n</code></pre>     - Output Schema (<code>User</code>):</p> <pre><code>type: object\nproperties:\n    username:\n        type: string\n        description: A unique identifier for the user.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    friendly_name:\n        type: string\n        description: A name for the user that is not necessarily unique.\n    email:\n        type: string\n    source:\n        type: string\n        description: User source. Based on implementation.\n</code></pre> </li> </ul> </li> <li> <p><code>PUT /auth/groups/{groupId}/members/{userId}</code>:</p> <ul> <li>Description: Adds a specific user to a specific group.</li> <li>Input: <code>groupId</code> (path parameter), <code>userId</code> (path parameter).</li> <li>Output: No output on success (HTTP 201).</li> <li>Implementation Notes: Ensure the group and user exist.</li> </ul> </li> <li> <p><code>DELETE /auth/groups/{groupId}/members/{userId}</code>:</p> <ul> <li>Description: Removes a specific user from a specific group.</li> <li>Input: <code>groupId</code> (path parameter), <code>userId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the group and user membership exist.</li> </ul> </li> <li> <p><code>GET /auth/groups/{groupId}/policies</code>:</p> <ul> <li>Description: Returns the list of policies attached to a specific group.</li> <li>Input: <code>groupId</code> (path parameter), pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>PolicyList</code> object containing a list of <code>Policy</code> objects and pagination information.</li> <li>Implementation Notes: The results must be sorted by the policy name.</li> <li> <p>Output Schema (<code>PolicyList</code>):</p> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/Policy\"\n</code></pre> </li> <li> <p>Output Schema (<code>Policy</code>):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> </ul> </li> <li> <p><code>PUT /auth/groups/{groupId}/policies/{policyId}</code>:</p> <ul> <li>Description: Attaches a policy to a specific group.</li> <li>Input: <code>groupId</code> (path parameter), <code>policyId</code> (path parameter).</li> <li>Output: No output on success (HTTP 201).</li> <li>Implementation Notes: Ensure the group and policy exist.</li> </ul> </li> <li> <p><code>DELETE /auth/groups/{groupId}/policies/{policyId}</code>:</p> <ul> <li>Description: Detaches a policy from a specific group.</li> <li>Input: <code>groupId</code> (path parameter), <code>policyId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the group and policy attachment exist.</li> </ul> </li> </ul>"},{"location":"security/ACL-server-implementation/#policies-apis","title":"Policies APIs","text":"<p>These APIs are used to manage policies, which contain the ACL information.</p> <p>Implement the following endpoints under the <code>policies</code> tag:</p> <ul> <li> <p><code>GET /auth/policies</code>:</p> <ul> <li>Description: Returns a list of policies.</li> <li>Input: Pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>PolicyList</code> object containing a list of <code>Policy</code> objects and pagination information.</li> <li>Implementation Notes: The results must be sorted by the policy name.</li> <li> <p>Output Schema (<code>PolicyList</code>):</p> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/Policy\"\n</code></pre> </li> <li> <p>Output Schema (<code>Policy</code> - relevant fields for ACL):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> </ul> </li> <li> <p><code>POST /auth/policies</code>:</p> <ul> <li>Description: Creates a new policy.</li> <li>Input: Request body containing a <code>Policy</code> object (<code>name</code>, <code>statement</code> and <code>acl</code>).</li> <li>Output: A <code>Policy</code> object representing the created policy.</li> <li>Implementation Notes: The <code>name</code> must be a unique, human-readable name for the policy. The <code>acl</code> property in the <code>Policy</code> object is used to define the permissions. The <code>statement</code> property is not used in the ACL implementation. This endpoint is called during setup to create default policies.</li> <li> <p>Input Schema (<code>Policy</code> - relevant fields for ACL):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> <li> <p>Output Schema (<code>Policy</code>):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> </ul> </li> <li> <p><code>GET /auth/policies/{policyId}</code>:</p> <ul> <li>Description: Returns the details of a specific policy.</li> <li>Input: <code>policyId</code> (path parameter).</li> <li>Output: A <code>Policy</code> object representing the policy.</li> <li>Implementation Notes: Ensure the policy exists. The <code>statement</code> property is not used in the ACL implementation.</li> <li> <p>Output Schema (<code>Policy</code>):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> </ul> </li> <li> <p><code>PUT /auth/policies/{policyId}</code>:</p> <ul> <li>Description: Updates an existing policy.</li> <li>Input: <code>policyId</code> (path parameter), request body containing the updated <code>Policy</code> object.</li> <li>Output: A <code>Policy</code> object representing the updated policy.</li> <li>Implementation Notes: Ensure the policy exists and the provided <code>policyId</code> matches the <code>name</code> in the request body. The request is to update the <code>acl</code> property. The <code>statement</code> property is not used in the ACL implementation.</li> <li> <p>Input Schema (<code>Policy</code>):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> <li> <p>Output Schema (<code>Policy</code>):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> </ul> </li> <li> <p><code>DELETE /auth/policies/{policyId}</code>:</p> <ul> <li>Description: Deletes a policy.</li> <li>Input: <code>policyId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the policy exists. When a policy is deleted, its attachments to users and groups should also be removed.</li> </ul> </li> </ul>"},{"location":"security/ACL-server-implementation/#2-setup","title":"2. Setup","text":""},{"location":"security/ACL-server-implementation/#key-steps-in-the-initial-setup","title":"Key Steps in the Initial Setup","text":"<p>When deploying an ACL server for the first time, it is essential to establish a set of standard user groups and assign each group a default set of permissions (policies). This process ensures that the system starts with a clear structure for access control, making it easy to manage users and their roles securely and consistently.</p> <p>Define Standard Groups</p> <p>Define Standard Groups</p> <p>Establish a set of base groups that represent the typical roles in your system, such as Admins (full privileges), Writers (read/write access), and Readers (read-only access). Each group should be mapped to a specific policy that defines the permissions for its members.</p> <p>You can reference the lakeFS contrib ACL implementation to see practical examples of how standard groups are defined and structured, along with their associated permission policies.</p>"},{"location":"security/ACL-server-implementation/#lakefs-configuration","title":"lakeFS Configuration","text":"<p>Update your lakeFS configuration file (<code>config.yaml</code>) to include:</p> <pre><code>auth:\n  encrypt:\n    secret_key: \"some_string\"\n  ui_config:\n    rbac: \"simplified\"\n  api:\n    endpoint: {ENDPOINT_TO_YOUR_ACL_SERVER} # e.g., http://localhost:9006/api/v1\n    token: {ACL_SERVER_TOKEN} # Used as authentication bearer calling the ACL server\n</code></pre> <p>Note</p> <p>The <code>auth.api.token</code> parameter is optional. If unspecified, lakeFS uses the <code>auth.encrypt.secret_key</code> as the secret to generate JWT. If specified, provide a JWT token or via the environment variable <code>LAKEFS_AUTH_API_TOKEN</code>.</p>"},{"location":"security/access-control-lists/","title":"Access Control Lists (ACLs)","text":"<p>Deprecated</p> <p>ACLs were removed from core lakeFS.</p> <p>For a more robust authorization solution, please see Role-Based Access Control, available in lakeFS Cloud and lakeFS Enterprise.  </p> <p>The following documentation is aimed for users with existing installations who wish to continue working with ACLs. </p>"},{"location":"security/access-control-lists/#basic-auth-functionality","title":"Basic Auth Functionality","text":"<p>New lakeFS versions will provide basic auth functionality featuring a single Admin user with a single set of credentials. Existing lakeFS installations that have a single user and a single set of credentials will migrate seamlessly to the new version. Installations that have more than one user / credentials will require to run a command and choose which set of user + credentials to migrate (more details here)</p>"},{"location":"security/access-control-lists/#credentials-replacement","title":"Credentials Replacement","text":"<p>In a single user setup, replacing credentials can be done as follows:</p> <ol> <li> <p>Delete the existing user:</p> <pre><code>lakectl auth users delete --id &lt;user-id&gt;\n</code></pre> </li> <li> <p>Shut down the lakeFS server - Required for invalidating the old credentials on the server</p> </li> <li> <p>Create a new user, with the same name and new credentials:</p> <pre><code>lakefs superuser --user-name &lt;user-id&gt;\n</code></pre> <p>This will generate a new set of credentials, and will print it out to the screen:</p> <pre><code>credentials:\n  access_key_id: *** (omitted)\n  secret_access_key: *** (omitted)\n</code></pre> </li> <li> <p>Re-run lakeFS server</p> </li> </ol> <p>Warning</p> <p>Calling the <code>superuser</code> command with pre-defined <code>--access-key-id</code> and <code>--secret-access-key</code> is possible, but should be done with caution. Make sure that <code>--secret-access-key</code> is not empty, as providing an access key without a secret key will trigger an ACL import flow (see Migration of existing user).</p> <p>In case you already deleted the user by following step (1), this import operation will fail and result in an  unrecoverable state, from which a clean installation is the only way out.</p>"},{"location":"security/access-control-lists/#acls","title":"ACLs","text":"<p>ACL server was moved out of core lakeFS and into a new package under <code>contrib/auth/acl</code>. Though we decided to move ACLs out, we are committed to making sure existing users who still need the use of ACLs can continue using this feature. In order to do that, users will need to run the separate ACL server as part of their lakeFS deployment environment and configure lakeFS to work with it.</p>"},{"location":"security/access-control-lists/#acl-server-configuration","title":"ACL server Configuration","text":"<p>Under the <code>contrib/auth/acl</code> you will be able to find an ACL server reference.</p> <p>Warning</p> <p>This implementation is a reference and is not fit for production use.</p> <p>For a more robust authorization solution, please see Role-Based Access Control, available in lakeFS Cloud and lakeFS Enterprise. </p> <p>The configuration of the ACL server is similar to lakeFS configuration, here's an example of an <code>.aclserver.yaml</code> config file:</p> <pre><code>---\nlisten_address: \"[ACL_SERVER_LISTEN_ADDRESS]\"\ndatabase:\n    type: \"postgres\"\n    postgres:\n    connection_string: \"[DATABASE_CONNECTION_STRING]\"\n\nencrypt:\n    # This should be the same encryption key as in lakeFS\n    secret_key: \"[ENCRYPTION_SECRET_KEY]\"\n</code></pre> <p>It is possible to use environment variables to configure the server as in lakeFS. Use the <code>ACLSERVER_</code> prefix to do so.  </p> <p>Info</p> <p>For full configuration reference see: this</p>"},{"location":"security/access-control-lists/#lakefs-configuration","title":"lakeFS Configuration","text":"<p>For the ACL server to work, configure the following values in lakeFS:  </p> <ul> <li><code>auth.ui_config.rbac</code>: <code>simplified</code> </li> <li><code>auth.api.endpoint</code>: <code>[ACL_SERVER_LISTEN_ADDRESS]</code></li> </ul>"},{"location":"security/access-control-lists/#migration-of-existing-user","title":"Migration of existing user","text":"<p>For installation with multiple users / credentials, upgrading to the new lakeFS version requires choosing which user + credentials will be used for the single user mode. This is done via the <code>lakefs superuser</code> command. For example, if you have a user with username <code>&lt;my-username&gt;</code> and credential key <code>&lt;my-access-key-id&gt;</code> use the following command to migrate that user:</p> <pre><code>lakefs superuser --user-name &lt;my-username&gt; --access-key-id &lt;my-access-key-id&gt;\n</code></pre> <p>After running the command you will be able to access the installation using the user's access key id and its respective secret access key.</p>"},{"location":"security/authentication/","title":"Authentication","text":""},{"location":"security/authentication/#user-authentication","title":"User Authentication","text":"<p>lakeFS authenticates users from a built-in authentication database.</p>"},{"location":"security/authentication/#built-in-database","title":"Built-in database","text":"<p>The built-in authentication database is always present and active. You can use the Web UI at Administration / Users to create users.</p> <p>Users have an access key <code>AKIA...</code> and an associated secret access key. These credentials are valid for logging into the Web UI or authenticating programmatic requests to the API Server or the S3 Gateway.</p>"},{"location":"security/authentication/#remote-authenticator-service","title":"Remote Authenticator Service","text":"<p>lakeFS server supports external authentication, the feature can be configured by providing an HTTP endpoint to an external authentication service. This integration can be especially useful if you already have an existing authentication system in place, as it allows you to reuse that system instead of maintaining a new one.</p> <p>Info</p> <p>To configure a Remote Authenticator see the configuration fields.</p>"},{"location":"security/authentication/#api-server-authentication","title":"API Server Authentication","text":"<p>Authenticating against the API server is done using a key-pair, passed via Basic Access Authentication.</p> <p>All HTTP requests must carry an <code>Authorization</code> header with the following structure:</p> <pre><code>Authorization: Basic &lt;base64 encoded access_key_id:secret_access_key&gt;\n</code></pre> <p>For example, assuming my access_key_id is <code>my_access_key_id</code> and my secret_access_key is <code>my_secret_access_key</code>, we'd send the following header with every request:</p> <pre><code>Authorization: Basic bXlfYWNjZXNzX2tleV9pZDpteV9zZWNyZXRfYWNjZXNzX2tleQ==\n</code></pre>"},{"location":"security/authentication/#s3-gateway-authentication","title":"S3 Gateway Authentication","text":"<p>To provide API compatibility with Amazon S3, authentication with the S3 Gateway supports both SIGv2 and SIGv4. Clients such as the AWS SDK that implement these authentication methods should work without modification.</p> <p>See this example for authenticating with the AWS CLI.</p>"},{"location":"security/authentication/#oidc-support","title":"OIDC support","text":"<p>Deprecated</p> <p>This feature is deprecated. For single sign-on with lakeFS, try lakeFS Cloud</p> <p>OpenID Connect (OIDC) is a simple identity layer on top of the OAuth 2.0 protocol. You can configure lakeFS to enable OIDC to manage your lakeFS users externally. Essentially, once configured, this enables you the benefit of OpenID connect, such as a single sign-on (SSO), etc.</p>"},{"location":"security/authentication/#configuring-lakefs-server-for-oidc","title":"Configuring lakeFS server for OIDC","text":"<p>To support OIDC, add the following to your lakeFS configuration:</p> <pre><code>auth:\n  oidc:\n    enabled: true\n    client_id: example-client-id\n    client_secret: exampleSecretValue\n    callback_base_url: https://lakefs.example.com       # The scheme, domain (and port) of your lakeFS installation\n    url: https://my-account.oidc-provider-example.com\n    default_initial_groups: [\"Developers\"]\n    friendly_name_claim_name: name                      #  Optional: use the value from this claim as the user's display name\n    persist_friendly_name: true                         #  Optional: persist friendly name to KV store so it can be displayed in the user list\n</code></pre> <p>Your login page will now include a link to sign in using the OIDC provider. When a user first logs in through the provider, a corresponding user is created in lakeFS.</p>"},{"location":"security/authentication/#friendly-name-persistence","title":"Friendly Name Persistence","text":"<p>When the <code>persist_friendly_name</code> configuration property is set to <code>true</code> and <code>friendly_name_claim_name</code> is set to a valid claim name, which exists in the incoming <code>id_token</code>, the friendly name will be persisted to the KV store. This will allow users with access to the lakeFS administration section to see friendly names in the users list, when listing group members, and when adding/removing group members. The friendly name stored in KV is updated with each successful login, if the incoming value is different than the stored value. This means it will be kept up-to-date with changes to the user's profile or if <code>friendly_name_claim_name</code> is re-configured.</p> <p>Notes</p> <ol> <li>As always, you may choose to provide these configurations using environment variables.</li> <li>You may already have other configuration values under the auth key, so make sure you combine them correctly.</li> </ol>"},{"location":"security/authentication/#user-permissions","title":"User permissions","text":"<p>Authorization is managed via lakeFS groups and policies.</p> <p>By default, an externally managed user is assigned to the lakeFS groups configured in the default_initial_groups property above. For a user to be assigned to other groups, add the initial_groups claim to their ID token claims. The claim should contain a comma-separated list of group names.</p> <p>Once the user has been created, you can manage their permissions from the Administration pages in the lakeFS UI or using lakectl.</p>"},{"location":"security/authentication/#using-a-different-claim-name","title":"Using a different claim name","text":"<p>To supply the initial groups using another claim from your ID token, you can use the <code>auth.oidc.initial_groups_claim_name</code> lakeFS configuration. For example, to take the initial groups from the roles claim, add:</p> <pre><code>auth:\n  oidc:\n    # ... Other OIDC configurations\n    initial_groups_claim_name: roles\n</code></pre>"},{"location":"security/authorization-yaml/","title":"Authorization API","text":""},{"location":"security/external-principals-aws/","title":"Authenticate to lakeFS with AWS IAM Roles","text":"<p>Info</p> <p>Available in lakeFS Cloud and lakeFS Enterprise If you're using the open-source version you can check the pluggable APIs.</p>","boost":2},{"location":"security/external-principals-aws/#overview","title":"Overview","text":"<p>lakeFS supports authenticating users programmatically using AWS IAM roles instead of using static lakeFS access and secret keys. The method enables you to bound IAM principal ARNs to lakeFS users. A single lakeFS user may have many AWS principal ARNs attached to it. When a client is authenticated to a lakeFS server with an AWS session, the actions performed by the client are on behalf of the user attached to the ARN.</p>","boost":2},{"location":"security/external-principals-aws/#using-session-names","title":"Using Session Names","text":"<p>The bound ARN can be attached to a single lakeFS user with or without SessionName, serving different users. For example, consider the following mapping: </p> Principal ARN lakeFS User <code>arn:aws:sts::123456:assumed-role/Dev</code> <code>foo</code> <code>arn:aws:sts::123456:assumed-role/Dev/john@acme.com</code> <code>john</code> <p>if the bound ARN were <code>arn:aws:sts::123456:assumed-role/Dev/&lt;SessionName&gt;</code> it would allow any principal assuming <code>Dev</code> role in AWS account <code>123456</code> to login to it. If the <code>SessionName</code> is <code>john@acme.com</code> then lakeFS would return token for <code>john</code> user</p>","boost":2},{"location":"security/external-principals-aws/#how-aws-authentication-works","title":"How AWS authentication works","text":"<p>The AWS STS API includes a method, <code>sts:GetCallerIdentity</code>, which allows you to validate the identity of a client. The client signs a GetCallerIdentity query using the AWS Signature v4 algorithm and sends it to the lakeFS server.</p> <p>The <code>GetCallerIdentity</code> query consists of four pieces of information: the request URL, the request body, the request headers and the request method. The AWS signature is computed over those fields. The lakeFS server reconstructs the query using this information and forwards it on to the AWS STS service. Depending on the response from the STS service, the server authenticates the client.</p> <p>Notably, clients don't need network-level access themselves to talk to the AWS STS API endpoint; they merely need access to the credentials to sign the request. However, it means that the lakeFS server does need network-level access to send requests to the STS endpoint.</p> <p>Each signed AWS request includes the current timestamp to mitigate the risk of replay attacks. In addition, lakeFS allows you to require an additional header, <code>X-LakeFS-Server-ID</code> (added by default), to be present to mitigate against different types of replay attacks (such as a signed <code>GetCallerIdentity</code> request stolen from a dev lakeFS instance and used to authenticate to a prod lakeFS instance).</p> <p>It's also important to note that Amazon does NOT appear to include any sort of authorization around calls to GetCallerIdentity. For example, if you have an IAM policy on your credential that requires all access to be MFA authenticated, non-MFA authenticated credentials will still be able to authenticate to lakeFS using this method.</p>","boost":2},{"location":"security/external-principals-aws/#server-configuration","title":"Server Configuration","text":"<p>Info</p> <p>lakeFS Helm chart supports the configuration below since version 1.5.0</p> <p>To enable AWS IAM authentication in lakeFS Enterprise:</p> <ol> <li>Enable external principals in lakeFS configuration</li> <li>Configure external AWS authentication settings</li> </ol> <p>Helm Configuration (<code>values.yaml</code>):</p> <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths:\n        - /\n\nlakefsConfig: |\n  auth:\n    # Configure external AWS authentication\n    external_aws_auth:\n      enabled: true\n      # the maximum age in seconds for the GetCallerIdentity request\n      #get_caller_identity_max_age: 60\n      # headers that must be present by the client when doing login request\n      required_headers:\n        # same host as the lakeFS server ingress\n        X-LakeFS-Server-ID: &lt;lakefs.ingress.domain&gt;\n</code></pre> <p>Note</p> <p>By default, lakeFS clients will add the parameter <code>X-LakeFS-Server-ID: &lt;lakefs.ingress.domain&gt;</code> to the initial login request for STS.</p> <p>Direct Configuration File (<code>lakefs.yaml</code>):</p> <pre><code>auth:\n  external_aws_auth:\n    enabled: true\n    # Optional: max age for GetCallerIdentity requests (default: 24h)\n    get_caller_identity_max_age: 3600\n    # Required headers for login requests\n    required_headers:\n      X-LakeFS-Server-ID: &lt;lakefs.ingress.domain&gt;\n    # Optional headers that may be present\n    optional_headers:\n      X-Custom-Header: value\n</code></pre>","boost":2},{"location":"security/external-principals-aws/#administration-of-iam-roles-in-lakefs","title":"Administration of IAM Roles in lakeFS","text":"<p>Administration refers to the management of the IAM roles that are allowed to authenticate to lakeFS. Operations such as attaching and detaching IAM roles to a user, listing the roles attached to a user, and listing the users attached to a role. This can be done through lakectl, the lakeFS External Principals API, or generated clients.</p>","boost":2},{"location":"security/external-principals-aws/#using-lakectl","title":"Using lakectl","text":"<pre><code># Attach IAM role to a lakeFS user (omit --id to use current user)\nlakectl auth users aws-iam attach [--id &lt;lakefs-user&gt;] \\\n    --principal-id 'arn:aws:sts::&lt;id&gt;:assumed-role/&lt;role A&gt;/&lt;optional session name&gt;'\nlakectl auth users aws-iam attach [--id &lt;lakefs-user&gt;] \\\n    --principal-id 'arn:aws:sts::&lt;id&gt;:assumed-role/&lt;role B&gt;'\n\n# Detach an IAM role from a user (omit --id to use current user)\nlakectl auth users aws-iam detach [--id &lt;lakefs-user&gt;] \\\n    --principal-id 'arn:aws:sts::&lt;id&gt;:assumed-role/&lt;role A&gt;'\n\n# List IAM roles attached to a user (omit --id to use current user)\nlakectl auth users aws-iam list [--id &lt;lakefs-user&gt;]\n\n# Lookup which lakeFS users are attached to an IAM role\nlakectl auth users aws-iam lookup --principal-id 'arn:aws:sts::&lt;id&gt;:assumed-role/&lt;role&gt;'\n</code></pre>","boost":2},{"location":"security/external-principals-aws/#using-python-sdk","title":"Using Python SDK","text":"<pre><code>import lakefs_sdk as lakefs\n\nconfiguration = lakefs.Configuration(host = \"...\",username=\"...\",password=\"...\")\nusername = \"&lt;lakefs-user&gt;\"\napi = lakefs.ApiClient(configuration)\nauth_api = lakefs.AuthApi(api)\n\n# attach the role(s)to a lakeFS user\nauth_api.create_user_external_principal(\n    user_id=username, principal_id='arn:aws:sts::&lt;id&gt;:assumed-role/&lt;role A&gt;/&lt;optional session name&gt;')\nauth_api.create_user_external_principal(\n    user_id=username, principal_id='arn:aws:sts::&lt;id&gt;:assumed-role/&lt;role B&gt;')\n\n# list the roles attached to the user\nresp = auth_api.list_user_external_principals(user_id=username)\nfor p in resp.results:\n    # do something\n</code></pre>","boost":2},{"location":"security/external-principals-aws/#get-lakefs-api-token","title":"Get lakeFS API Token","text":"<p>The login to lakeFS is done by calling the login API with the <code>GetCallerIdentity</code> request signed by the client. Currently, the login operation is supported out of the box in:</p> <ul> <li>lakectl</li> <li>lakeFS Hadoop FileSystem version 0.2.4, see Spark usage</li> <li>python</li> <li>Everest mount</li> </ul> <p>For other use cases authenticated to lakeFS via login endpoint, this will require building the request input.</p>","boost":2},{"location":"security/external-principals-aws/#login-with-lakectl","title":"Login with lakectl","text":"","boost":2},{"location":"security/external-principals-aws/#prerequisites","title":"prerequisites","text":"<ol> <li>lakeFS must be configured to allow external principals to authenticate. The relevant IAM role should be attached to the appropriate lakeFS user. </li> <li>lakectl must be configured to use IAM authentication.</li> </ol>","boost":2},{"location":"security/external-principals-aws/#lakectl-configuration","title":"lakectl configuration","text":"<p>To use IAM authentication, the following configuration fields are available:</p> <ul> <li><code>credentials.provider.type</code> <code>(string: '')</code> - Settings this <code>aws_iam</code> will expect <code>aws_iam</code> block and try to use IAM.</li> <li><code>credentials.provider.aws_iam.token_ttl_second</code> <code>(duration: 60m)</code> - Optional: lakeFS token duration.</li> <li><code>credentials.provider.aws_iam.url_presign_ttl_seconds</code> <code>(duration: 15m)</code> - Optional: AWS STS's presigned URL validation duration.  </li> <li><code>credentials.provider.aws_iam.refresh_interval</code> <code>(duration: 15m)</code> - Optional: Amount of time before token expiration that Everest will try to fetch a new session token instead of using the current one.  </li> <li><code>credentials.provider.aws_iam.token_request_headers</code>: Map of required headers and their values to be signed by the AWS STS request as configured in your lakeFS server. If nothing is set the default behavior is adding <code>x-lakefs-server-id:&lt;lakeFS host&gt;</code>. If your lakeFS server doesn't require any headers (less secure) you can set this empty by setting <code>{}</code> empty map in your config. </li> </ul> <p>These configuration fields can be set via <code>.lakectl.yaml</code>: </p> <p>Example</p> <pre><code>credentials:\nprovider:\n    type: aws_iam          # Required\n    aws_iam:\n      token_ttl_seconds: 60m              # Optional, default: 1h\n      url_presign_ttl_seconds: 15m        # Optional, default: 15m\n      refresh_interval: 5m                # Optional, default: 5m\n      token_request_headers:              # Optional, if omitted then will set x-lakefs-server-id: &lt;lakeFS host&gt; by default, to override default set to '{}'\n      # x-lakefs-server-id: &lt;lakeFS host&gt;     Added by default if token_request_headers is not set  \n      custome-key:  custome-val\nserver:\nendpoint_url: &lt;lakeFS endpoint url&gt;\n</code></pre>","boost":2},{"location":"security/external-principals-aws/#token-caching","title":"Token Caching","text":"<p>To optimize lakectl's IAM authentication, a simple token caching mechanism was introduced. Instead of performing a login request to AWS to get a JWT for lakeFS, lakectl will save the token to <code>$HOMEDIR/.lakectl/cache/lakectl_token_cache.json</code>. The token will be used in the next lakectl operations up until one hour after writing. Later than that, lakectl will try and retrieve a new token.</p> <p>Note</p> <p>Cache implementation does not support multiple AWS profiles.  When switching profiles and for general troubleshooting, try and delete the cache first.   </p>","boost":2},{"location":"security/external-principals-aws/#login-with-python","title":"Login with python","text":"","boost":2},{"location":"security/external-principals-aws/#prerequisites_1","title":"prerequisites","text":"<ol> <li>lakeFS should be configured to allow external principals to authenticate, and the used IAM role should be attached to the relevant lakeFS user</li> <li>The Python SDK requires additional packages to be installed to generate a lakeFS client with the assumed role. To install the required packages, run the following command:</li> </ol> <pre><code>  pip install \"lakefs[aws-iam]\"\n</code></pre> <p>There are two ways in which external principals can be used to authenticate to lakeFS:</p> <ol> <li> <p>If no other authentication flow is provided, and the <code>credentials.provider.type</code> configuration is set to <code>aws_iam</code> in <code>.lakectl.yaml</code>, the client will use the machine's AWS role to authenticate with lakeFS:</p> <p><pre><code>credentials:\n    provider:\n    type: aws_iam\n    aws_iam:\n        token_ttl_seconds: 3600      # TTL for the temporary token (default: 3600)\n        url_presign_ttl_seconds: 60  # TTL for presigned URLs (default: 60)\n        token_request_headers:       # Optional headers for token requests\n        HeaderName: HeaderValue\n</code></pre> Or using environment variables: <pre><code>export LAKECTL_CREDENTIALS_PROVIDER_TYPE=\"aws_iam\"\nexport LAKECTL_CREDENTIALS_PROVIDER_AWS_IAM_TOKEN_TTL_SECONDS=\"3600\"\nexport LAKECTL_CREDENTIALS_PROVIDER_AWS_IAM_PRESIGNED_URL_TTL_SECONDS=\"60\"\nexport LAKECTL_CREDENTIALS_PROVIDER_AWS_IAM_TOKEN_REQUEST_HEADERS='{\"HeaderName\":\"HeaderValue\"}'\n</code></pre> To use the client, merely <code>import lakefs</code> and use it as you would normally do: <pre><code>import lakefs\n\nfor branch in lakefs.repository(\"example-repo\").branches():\nprint(branch)\n</code></pre></p> <p>Warning</p> <p>Please note, using the IAM provider configurations will not work with the lakectl command line tool, and will stop you from running it.</p> </li> <li> <p>Generate a lakeFS client with the assumed role by initiating a boto3 session with the desired role and call <code>lakefs.client.frow_aws_role</code>:</p> <pre><code>import lakefs\nimport boto3    \n\nsession = boto3.Session()\nmy_client = lakefs.client.from_aws_role(session=session, ttl_seconds=7200, host=\"&lt;lakefs-host&gt;\")\n\n# list repositories\nrepos = lakefs.repositories(client=my_client)\nfor r in repos:\n    print(r)\n</code></pre> </li> </ol>","boost":2},{"location":"security/presigned-url/","title":"Configuring lakeFS to use presigned URLs","text":"<p>With lakeFS, you can access data directly from the storage and not through lakeFS using a presigned URL. Based on the user's access to an object in the object store, the presigned URL will get read or write access.</p> <p>The presign support is enabled for block adapter that supports it (AWS, GCP, Azure), and can be disabled by the configuration (<code>blockstore.&lt;blockstore_type&gt;.disable_pre_signed</code>). Note that the UI support is disabled by default.</p> <p>It is possible to override the default pre-signed URL endpoint in AWS by setting the configuration (<code>blockstore.s3.pre_signed_endpoint</code>). This is useful, for example, when you wish to define a VPC endpoint access for the pre-signed URL.</p>"},{"location":"security/presigned-url/#using-presigned-urls-in-the-ui","title":"Using presigned URLs in the UI","text":"<p>To use presigned URLs in the UI:</p> <ol> <li>Enable the presigned URL support UI in the lakeFS configuration: set <code>blockstore.&lt;blockstore_type&gt;.disable_pre_signed_ui=false</code>.</li> <li>Add CORS (Cross-Origin Resource Sharing) permissions to the bucket for the UI to fetch objects using a presigned URL (instead of through lakeFS).</li> </ol> <p>Warning</p> <p>Currently DuckDB fetching data from lakeFS does not support fetching data using presigned URL.</p>"},{"location":"security/presigned-url/#examples","title":"Examples","text":"<p>AWS S3</p> <pre><code>  [\n    {\n        \"AllowedHeaders\": [\n            \"*\"\n        ],\n        \"AllowedMethods\": [\n            \"GET\",\n            \"PUT\",\n            \"HEAD\"\n        ],\n        \"AllowedOrigins\": [\n            \"lakefs.endpoint\"\n        ],\n        \"ExposeHeaders\": [\n            \"ETag\"\n        ]\n    }\n  ]\n</code></pre> <p>Google Storage</p> <pre><code>  [\n    {\n        \"origin\": [\"lakefs.endpoint\"],\n        \"responseHeader\": [\"ETag\"],\n        \"method\": [\"PUT\", \"GET\", \"HEAD\"],\n        \"maxAgeSeconds\": 3600\n    }\n  ]\n</code></pre> <p>Azure blob storage</p> <pre><code>  &lt;Cors&gt;\n      &lt;CorsRule&gt;  \n          &lt;AllowedOrigins&gt;lakefs.endpoint&lt;/AllowedOrigins&gt;  \n          &lt;AllowedMethods&gt;PUT,GET,HEAD&lt;/AllowedMethods&gt;  \n          &lt;AllowedHeaders&gt;*&lt;/AllowedHeaders&gt;  \n          &lt;ExposedHeaders&gt;ETag,x-ms-*&lt;/ExposedHeaders&gt;  \n          &lt;MaxAgeInSeconds&gt;3600&lt;/MaxAgeInSeconds&gt;  \n      &lt;/CorsRule&gt;  \n  &lt;/Cors&gt;\n</code></pre>"},{"location":"security/rbac/","title":"Role-Based Access Control (RBAC)","text":"<p>Info</p> <p>Available in lakeFS Cloud and lakeFS Enterprise</p>","boost":2},{"location":"security/rbac/#rbac-model","title":"RBAC Model","text":"<p>Access to resources is managed very much like AWS IAM.</p> <p>There are five basic components to the system:</p> <ol> <li>Users - Representing entities that access and use the system. A user is given one or more Access Credentials for authentication.</li> <li>Actions - Representing a logical action within the system - reading a file, creating a repository, etc.</li> <li>Resources - A unique identifier representing a specific resource in the system - a repository, an object, a user, etc.</li> <li>Policies - Representing a set of Actions, a Resource and an effect: whether or not these actions are <code>allowed</code> or <code>denied</code> for the given resource(s).</li> <li>Groups - A named collection of users. Users can belong to multiple groups.</li> </ol> <p>Controlling access is done by attaching Policies, either directly to Users, or to Groups they belong to.</p>","boost":2},{"location":"security/rbac/#authorization-process","title":"Authorization process","text":"<p>Every action in the system - be it an API request, UI interaction, S3 Gateway call, or CLI command - requires a set of actions to be allowed for one or more resources.</p> <p>When a user makes a request to perform that action, the following process takes place:</p> <ol> <li>Authentication - the credentials passed in the request are evaluated and the user's identity is extracted.</li> <li>Action permission resolution - lakeFS then calculates the set of allowed actions and resources that this request requires.</li> <li>Effective policy resolution - the user's policies (either attached directly or through group memberships) are calculated.</li> <li>Policy/Permission evaluation - lakeFS will compare the given user policies with the request actions and determine whether or not the request is allowed to continue.</li> </ol>","boost":2},{"location":"security/rbac/#policy-precedence","title":"Policy Precedence","text":"<p>Each policy attached to a user or a group has an <code>Effect</code> - either <code>allow</code> or <code>deny</code>. During evaluation of a request, <code>deny</code> would take precedence over any other <code>allow</code> policy.</p> <p>This helps us compose policies together. For example, we could attach a very permissive policy to a user and use <code>deny</code> rules to then selectively restrict what that user can do.</p>","boost":2},{"location":"security/rbac/#policy-conditions","title":"Policy Conditions","text":"<p>lakeFS policies support optional conditions that provide additional control over when a policy statement is in effect.</p> <p>Conditions are specified using condition operators that evaluate to true or false based on the request context. When a condition evaluates to false, the policy statement does not apply.</p>","boost":2},{"location":"security/rbac/#condition-structure","title":"Condition Structure","text":"<p>Conditions are added to policy statements using the optional <code>condition</code> field:</p> <pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\"fs:ReadObject\"],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:fs:::repository/myrepo/object/*\",\n      \"condition\": {\n        \"IpAddress\": {\n          \"SourceIp\": [\"203.0.113.0/24\", \"198.51.100.25/32\"]\n        }\n      }\n    }\n  ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#supported-condition-operators","title":"Supported Condition Operators","text":"","boost":2},{"location":"security/rbac/#ipaddress-notipaddress","title":"IpAddress / NotIpAddress","text":"<p>The <code>IpAddress</code> condition operator matches the client's source IP address against one or more IP addresses or CIDR blocks. The <code>NotIpAddress</code> condition operator matches when the client's source IP address does NOT match any of the specified IP addresses or CIDR blocks (negation of <code>IpAddress</code>).</p> <p>Supported Fields: - <code>SourceIp</code> - The IP address of the client making the request</p> <p>Value Format: - Single IP address: <code>\"203.0.113.5\"</code> - CIDR notation: <code>\"203.0.113.0/24\"</code> - Array of IPs and/or CIDRs: <code>[\"203.0.113.0/24\", \"198.51.100.25/32\"]</code></p> <p>Example - Allow access only from specific IP range:</p> <pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\"fs:*\"],\n      \"effect\": \"allow\",\n      \"resource\": \"*\",\n      \"condition\": {\n        \"IpAddress\": {\n          \"SourceIp\": [\"10.0.0.0/8\", \"172.16.0.0/12\"]\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>Example - Deny access from specific IPs:</p> <pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\"fs:*\"],\n      \"effect\": \"deny\",\n      \"resource\": \"*\",\n      \"condition\": {\n        \"IpAddress\": {\n          \"SourceIp\": [\"192.0.2.0/24\"]\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>Example - Deny access from IPs NOT in allowed ranges:</p> <pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\"fs:*\"],\n      \"effect\": \"deny\",\n      \"resource\": \"*\",\n      \"condition\": {\n        \"NotIpAddress\": {\n          \"SourceIp\": [\"10.0.0.0/8\", \"172.16.0.0/12\", \"192.168.0.0/16\"]\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>IP Address Extraction:</p> <p>lakeFS extracts the client IP address from the request using the following priority:</p> <ol> <li><code>X-Forwarded-For</code> header (first IP in the list) - for requests through proxies/load balancers</li> <li><code>X-Real-IP</code> header - as a fallback</li> <li>Remote address from the TCP connection</li> </ol> <p>This ensures that IP-based conditions work correctly even when lakeFS is deployed behind a load balancer or reverse proxy.</p>","boost":2},{"location":"security/rbac/#resource-naming-arns","title":"Resource naming - ARNs","text":"<p>lakeFS uses ARN identifier - very similar in structure to those used by AWS.  The resource segment of the ARN supports wildcards: use <code>*</code> to match 0 or more characters, or <code>?</code> to match exactly one character.  </p> <p>Here are a some examples of valid ARNs within lakeFS and their meaning:</p> ARN Meaning <code>arn:lakefs:auth:::user/jane.doe</code> A specific user <code>arn:lakefs:auth:::user/*</code> All users <code>arn:lakefs:fs:::repository/myrepo/*</code> All resources under <code>myrepo</code> <code>arn:lakefs:fs:::repository/myrepo/object/foo/bar/baz</code> A single object ARN <code>arn:lakefs:fs:::repository/myrepo/object/*</code> All objects in <code>myrepo</code> <code>arn:lakefs:fs:::repository/*</code> All repositories <code>arn:lakefs:fs:::*</code> All resources under the fs ARN prefix <p>Additionally, the current user's ID is interpolated in runtime into the ARN using the <code>${user}</code> placeholder.</p> <p>This allows us to create fine-grained policies affecting only a specific subset of resources.</p> <p>See below for a full reference of ARNs and actions.</p>","boost":2},{"location":"security/rbac/#actions-and-permissions","title":"Actions and Permissions","text":"<p>For the full list of actions and their required permissions, see the following table:</p> Action name required action Resource API endpoint S3 gateway operation List Repositories <code>fs:ListRepositories</code> <code>*</code> GET <code>/repositories</code> ListBuckets Get Repository <code>fs:ReadRepository</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}</code> HeadBucket Get Commit <code>fs:ReadCommit</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}/commits/{commitId}</code> - Create Commit <code>fs:CreateCommit</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> POST <code>/repositories/{repositoryId}/branches/{branchId}/commits</code> - Get Commit log <code>fs:ReadBranch</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> GET <code>/repositories/{repositoryId}/branches/{branchId}/commits</code> - Create Repository <code>fs:CreateRepository</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> POST <code>/repositories</code> - Namespace Attach to Repository <code>fs:AttachStorageNamespace</code> <code>arn:lakefs:fs:::namespace/{storageNamespace}</code> POST <code>/repositories</code> - Import From Source <code>fs:ImportFromStorage</code> <code>arn:lakefs:fs:::namespace/{storageNamespace}</code> POST <code>/repositories/{repositoryId}/branches/{branchId}/import</code> - Cancel Import <code>fs:ImportCancel</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> DELETE <code>/repositories/{repositoryId}/branches/{branchId}/import</code> - Delete Repository <code>fs:DeleteRepository</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> DELETE <code>/repositories/{repositoryId}</code> - List Branches <code>fs:ListBranches</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}/branches</code> ListObjects/ListObjectsV2 (with delimiter = <code>/</code> and empty` prefix) Get Branch <code>fs:ReadBranch</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> GET <code>/repositories/{repositoryId}/branches/{branchId}</code> - Create Branch <code>fs:CreateBranch</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> POST <code>/repositories/{repositoryId}/branches</code> - Delete Branch <code>fs:DeleteBranch</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> DELETE <code>/repositories/{repositoryId}/branches/{branchId}</code> - Merge branches <code>fs:CreateCommit</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{destinationBranchId}</code> POST <code>/repositories/{repositoryId}/refs/{sourceBranchId}/merge/{destinationBranchId}</code> - Diff branch uncommitted changes <code>fs:ListObjects</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}/branches/{branchId}/diff</code> - Diff refs <code>fs:ListObjects</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}/refs/{leftRef}/diff/{rightRef}</code> - Stat object <code>fs:ReadObject</code> <code>arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey}</code> GET <code>/repositories/{repositoryId}/refs/{ref}/objects/stat</code> HeadObject Get Object <code>fs:ReadObject</code> <code>arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey}</code> GET <code>/repositories/{repositoryId}/refs/{ref}/objects</code> GetObject List Objects <code>fs:ListObjects</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}/refs/{ref}/objects/ls</code> ListObjects, ListObjectsV2 (no delimiter, or \"/\" + non-empty` prefix) Upload Object <code>fs:WriteObject</code> <code>arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey}</code> POST <code>/repositories/{repositoryId}/branches/{branchId}/objects</code> PutObject, CreateMultipartUpload, UploadPart`, CompleteMultipartUpload Delete Object <code>fs:DeleteObject</code> <code>arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey}</code> DELETE <code>/repositories/{repositoryId}/branches/{branchId}/objects</code> DeleteObject, DeleteObjects`, AbortMultipartUpload Revert Branch <code>fs:RevertBranch</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> PUT <code>/repositories/{repositoryId}/branches/{branchId}</code> - Get Branch Protection Rules <code>branches:GetBranchProtectionRules</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/branch_protection</code> - Set Branch Protection Rules <code>branches:SetBranchProtectionRules</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> POST <code>/repositories/{repository}/branch_protection</code> - Delete Branch Protection Rules <code>branches:SetBranchProtectionRules</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> DELETE <code>/repositories/{repository}/branch_protection</code> - Create User <code>auth:CreateUser</code> <code>arn:lakefs:auth:::user/{userId}</code> POST <code>/auth/users</code> - List Users <code>auth:ListUsers</code> <code>*</code> GET <code>/auth/users</code> - Get User <code>auth:ReadUser</code> <code>arn:lakefs:auth:::user/{userId}</code> GET <code>/auth/users/{userId}</code> - Delete User <code>auth:DeleteUser</code> <code>arn:lakefs:auth:::user/{userId}</code> DELETE <code>/auth/users/{userId}</code> - Get Group <code>auth:ReadGroup</code> <code>arn:lakefs:auth:::group/{groupId}</code> GET <code>/auth/groups/{groupId}</code> - List Groups <code>auth:ListGroups</code> <code>*</code> GET <code>/auth/groups</code> - Create Group <code>auth:CreateGroup</code> <code>arn:lakefs:auth:::group/{groupId}</code> POST <code>/auth/groups</code> - Delete Group <code>auth:DeleteGroup</code> <code>arn:lakefs:auth:::group/{groupId}</code> DELETE <code>/auth/groups/{groupId}</code> - List Policies <code>auth:ListPolicies</code> <code>*</code> GET <code>/auth/policies</code> - Create Policy <code>auth:CreatePolicy</code> <code>arn:lakefs:auth:::policy/{policyId}</code> POST <code>/auth/policies</code> - Update Policy <code>auth:UpdatePolicy</code> <code>arn:lakefs:auth:::policy/{policyId}</code> POST <code>/auth/policies</code> - Delete Policy <code>auth:DeletePolicy</code> <code>arn:lakefs:auth:::policy/{policyId}</code> DELETE <code>/auth/policies/{policyId}</code> - Get Policy <code>auth:ReadPolicy</code> <code>arn:lakefs:auth:::policy/{policyId}</code> GET <code>/auth/policies/{policyId}</code> - List Group Members <code>auth:ReadGroup</code> <code>arn:lakefs:auth:::group/{groupId}</code> GET <code>/auth/groups/{groupId}/members</code> - Add Group Member <code>auth:AddGroupMember</code> <code>arn:lakefs:auth:::group/{groupId}</code> PUT <code>/auth/groups/{groupId}/members/{userId}</code> - Remove Group Member <code>auth:RemoveGroupMember</code> <code>arn:lakefs:auth:::group/{groupId}</code> DELETE <code>/auth/groups/{groupId}/members/{userId}</code> - List User Credentials <code>auth:ListCredentials</code> <code>arn:lakefs:auth:::user/{userId}</code> GET <code>/auth/users/{userId}/credentials</code> - Create User Credentials <code>auth:CreateCredentials</code> <code>arn:lakefs:auth:::user/{userId}</code> POST <code>/auth/users/{userId}/credentials</code> - Delete User Credentials <code>auth:DeleteCredentials</code> <code>arn:lakefs:auth:::user/{userId}</code> DELETE <code>/auth/users/{userId}/credentials/{accessKeyId}</code> - Get User Credentials <code>auth:ReadCredentials</code> <code>arn:lakefs:auth:::user/{userId}</code> GET <code>/auth/users/{userId}/credentials/{accessKeyId}</code> - List User Groups <code>auth:ReadUser</code> <code>arn:lakefs:auth:::user/{userId}</code> GET <code>/auth/users/{userId}/groups</code> - List User Policies <code>auth:ReadUser</code> <code>arn:lakefs:auth:::user/{userId}</code> GET <code>/auth/users/{userId}/policies</code> - Attach Policy To User <code>auth:AttachPolicy</code> <code>arn:lakefs:auth:::user/{userId}</code> PUT <code>/auth/users/{userId}/policies/{policyId}</code> - Detach Policy From User <code>auth:DetachPolicy</code> <code>arn:lakefs:auth:::user/{userId}</code> DELETE <code>/auth/users/{userId}/policies/{policyId}</code> - List Group Policies <code>auth:ReadGroup</code> <code>arn:lakefs:auth:::group/{groupId}</code> GET <code>/auth/groups/{groupId}/policies</code> - Attach Policy To Group <code>auth:AttachPolicy</code> <code>arn:lakefs:auth:::group/{groupId}</code> PUT <code>/auth/groups/{groupId}/policies/{policyId}</code> - Detach Policy From Group <code>auth:DetachPolicy</code> <code>arn:lakefs:auth:::group/{groupId}</code> DELETE <code>/auth/groups/{groupId}/policies/{policyId}</code> - Attach External Principal to a User <code>auth:CreateUserExternalPrincipal</code> <code>arn:lakefs:auth:::user/{userId}</code> POST <code>/auth/users/{userId}/external/principals</code> - Delete External Principal Attachment from a User <code>auth:DeleteUserExternalPrincipal</code> <code>arn:lakefs:auth:::user/{userId}</code> DELETE <code>/auth/users/{userId}/external/principals</code> - Get the User attached to an External Principal <code>auth:ReadExternalPrincipal</code> <code>arn:lakefs:auth:::externalPrincipal/{principalId}</code> GET <code>/auth/external/principals</code> - Read Storage Config <code>fs:ReadConfig</code> <code>*</code> GET <code>/config/storage</code> - Get Garbage Collection Rules <code>retention:GetGarbageCollectionRules</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}/gc/rules</code> - Set Garbage Collection Rules <code>retention:SetGarbageCollectionRules</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> POST <code>/repositories/{repositoryId}/gc/rules</code> - Prepare Garbage Collection Commits <code>retention:PrepareGarbageCollectionCommits</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> POST <code>/repositories/{repositoryId}/gc/prepare_commits</code> - List Repository Action Runs <code>ci:ReadAction</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/actions/runs</code> - Get Action Run <code>ci:ReadAction</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/actions/runs/{run_id}</code> - List Action Run Hooks <code>ci:ReadAction</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/actions/runs/{run_id}/hooks</code> - Get Action Run Hook Output <code>ci:ReadAction</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/actions/runs/{run_id}/hooks/{hook_run_id}/output</code> - Get Pull Request <code>pr:ReadPullRequest</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/pulls/{pull_request}</code> - Create Pull Request <code>pr:WritePullRequest</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> POST <code>/repositories/{repository}/pulls</code> - Update Pull Request <code>pr:WritePullRequest</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> PATCH <code>/repositories/{repository}/pulls/{pull_request}</code> - Merge Pull Request <code>pr:WritePullRequest</code> + Merge Branches <code>arn:lakefs:fs:::repository/{repositoryId}</code> PUT <code>/repositories/{repository}/pulls/{pull_request}/merge</code> - List Pull Requests <code>pr:ListPullRequests</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/pulls</code> - Login as Organization Admin (Cloud only) <code>admin:Login</code> <code>*</code> POST <code>/admin/login</code> (part of lakefs cloud, not lakefs endpoint) - <p>Some APIs may require more than one action.  For instance, to create a repository (<code>POST /repositories</code>),  you need permission to <code>fs:CreateRepository</code> for the name of the repository and also <code>fs:AttachStorageNamespace</code> for the storage namespace used.</p>","boost":2},{"location":"security/rbac/#preconfigured-policies","title":"Preconfigured Policies","text":"<p>The following <code>Policies</code> are created during initial setup:</p>","boost":2},{"location":"security/rbac/#fsfullaccess","title":"FSFullAccess","text":"<pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\n        \"fs:*\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"*\"\n    }\n  ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#fsreadall","title":"FSReadAll","text":"<pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\n        \"fs:List*\",\n        \"fs:Read*\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"*\"\n    }\n  ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#fsreadwriteall","title":"FSReadWriteAll","text":"<pre><code>{\n    \"statement\": [\n        {\n            \"action\": [\n                \"fs:Read*\",\n                \"fs:List*\",\n                \"fs:WriteObject\",\n                \"fs:DeleteObject\",\n                \"fs:RevertBranch\",\n                \"fs:CreateBranch\",\n                \"fs:CreateTag\",\n                \"fs:DeleteBranch\",\n                \"fs:DeleteTag\",\n                \"fs:CreateCommit\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"*\"\n        }\n    ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#authfullaccess","title":"AuthFullAccess","text":"<pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\n        \"auth:*\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"*\"\n    }\n  ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#authmanageowncredentials","title":"AuthManageOwnCredentials","text":"<pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\n        \"auth:CreateCredentials\",\n        \"auth:DeleteCredentials\",\n        \"auth:ListCredentials\",\n        \"auth:ReadCredentials\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:auth:::user/${user}\"\n    }\n  ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#repomanagementfullaccess","title":"RepoManagementFullAccess","text":"<pre><code>{\n    \"statement\": [\n        {\n            \"action\": [\n                \"ci:*\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"*\"\n        },\n        {\n            \"action\": [\n                \"retention:*\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"*\"\n        }\n    ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#repomanagementreadall","title":"RepoManagementReadAll","text":"<pre><code>{\n    \"statement\": [\n        {\n            \"action\": [\n                \"ci:Read*\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"*\"\n        },\n        {\n            \"action\": [\n                \"retention:Get*\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"*\"\n        }\n    ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#adminfullaccess-lakefs-cloud-only","title":"AdminFullAccess (lakeFS Cloud only)","text":"<pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\n        \"admin:*\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"*\"\n    }\n  ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#additional-policies","title":"Additional Policies","text":"<p>You can create additional policies to further limit user access. Use the web UI or the lakectl auth command to create policies.</p> <p>Here is an example to define read/write access for a specific repository:</p> <pre><code>{\n    \"statement\": [\n        {\n            \"action\": [\n                \"fs:ReadRepository\",\n                \"fs:ReadCommit\",\n                \"fs:ListBranches\",\n                \"fs:ListTags\",\n                \"fs:ListObjects\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;\"\n        },\n        {\n            \"action\": [\n                \"fs:RevertBranch\",\n                \"fs:ReadBranch\",\n                \"fs:CreateBranch\",\n                \"fs:DeleteBranch\",\n                \"fs:CreateCommit\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/branch/*\"\n        },\n                {\n            \"action\": [\n                \"fs:ListObjects\",\n                \"fs:ReadObject\",\n                \"fs:WriteObject\",\n                \"fs:DeleteObject\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/object/*\"\n        },\n                {\n            \"action\": [\n                \"fs:ReadTag\",\n                \"fs:CreateTag\",\n                \"fs:DeleteTag\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/tag/*\"\n        },\n        {\n            \"action\": [\"fs:ReadConfig\"],\n            \"effect\": \"allow\",\n            \"resource\": \"*\"\n        }\n    ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#multiple-resources-statements","title":"Multiple Resources Statements","text":"<p>lakeFS supports specifying multiple resources in a single RBAC statement. This is available on lakeFS Cloud, or on lakeFS Enterprise if starting at version lakeFS v1.54.0 and Fluffy v0.12.0 In addition to a single resource. The resource field can contain a string representing a JSON-encoded list of resources.</p> <pre><code>{\n    \"statement\": [\n        {\n            \"action\": [\n                \"fs:Read*\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"[\\\"arn:lakefs:fs:::repository/repo1\\\",\\\"arn:lakefs:fs:::repository/repo2\\\"]\"\n        }\n    ]\n}\n</code></pre> <p>The list must be properly encoded as a JSON string: quote each resource, and escape those quotes as shown. Otherwise, the policy cannot be parsed.</p>","boost":2},{"location":"security/rbac/#multi-resource-policy-creation-using-python-sdk","title":"Multi-Resource Policy Creation Using Python SDK","text":"<p>Here is how you can leverage Python SDK to create a multiple resource policy:</p> <pre><code>import lakefs_sdk\nfrom lakefs_sdk.client import LakeFSClient\nfrom lakefs_sdk import models\n\nconfiguration = lakefs_sdk.Configuration(\n        host=lakefsEndPoint,\n        username=lakefsAccessKey,\n        password=lakefsSecretKey,\n)\nclt = LakeFSClient(configuration)\n\nclt.auth_api.create_policy(\n    policy=models.Policy(\n        id='FSReadTwoRepos',\n        statement=[models.Statement(\n            effect=\"deny\",\n            resource=json.dumps([\"arn:lakefs:fs:::repository/repo1\",\"arn:lakefs:fs:::repository/repo2\"]),\n            action=[\"fs:ReadRepository\"],\n        ),\n        ]\n    )\n)\n</code></pre>","boost":2},{"location":"security/rbac/#preconfigured-groups","title":"Preconfigured Groups","text":"<p>lakeFS has four preconfigured groups:</p> <ul> <li>Admins</li> <li>SuperUsers</li> <li>Developers</li> <li>Viewers</li> </ul> <p>They have the following policies granted to them:</p> Policy Admins SuperUsers Developers Viewers <code>FSFullAccess</code> \u2705 \u2705 <code>AuthFullAccess</code> \u2705 <code>RepoManagementFullAccess</code> \u2705 <code>AuthManageOwnCredentials</code> \u2705 \u2705 \u2705 <code>RepoManagementReadAll</code> \u2705 \u2705 <code>FSReadWriteAll</code> \u2705 <code>FSReadAll</code> \u2705 <code>AdminFullAccess</code> (Cloud only) \u2705","boost":2},{"location":"security/rbac/#pluggable-authentication-and-authorization","title":"Pluggable Authentication and Authorization","text":"<p>Authorization and authentication is pluggable in lakeFS. </p> <p>If lakeFS is attached to a remote authentication server (or you are using lakeFS Cloud) then the role-based access control user interface can be used. </p> <p>If you are using RBAC with your self-managed lakeFS then the lakeFS configuration element <code>auth.ui_config.rbac</code> should be set to <code>external</code>. </p> <p>An enterprise (paid) solution of lakeFS should set <code>auth.ui_config.rbac</code> as <code>internal</code>.</p>","boost":2},{"location":"security/remote-authenticator/","title":"Remote Authenticator","text":"<p>Remote Authenticator is a pluggable architecture for lakeFS which allows you to use existing organizational identity policies and infrastructure with the authentication mechanism of lakeFS. The Remote Authenticator's job is to abstract away the complexities of existing infrastructure and implement a standard interface, which lakeFS can use to resolve user identity and manage access to lakeFS. This loose coupling allows you to implement federated identity without providing lakeFS with direct access to your identity infrastructure.</p>"},{"location":"security/remote-authenticator/#architecture","title":"Architecture","text":"<p>Here's the authentication flow that lakeFS uses when configured with a remote authenticator:</p> <pre><code>sequenceDiagram\n    participant A as lakeFS Client\n    participant B as lakeFS Server\n    participant C as Remote Authenticator\n    participant D as IdP\n    A-&gt;&gt;B: Submit login form\n    B-&gt;&gt;C: POST user credentials\n    C-&gt;&gt;D: IdP request\n    D-&gt;&gt;C: IdP response\n    C-&gt;&gt;B: Auth response\n    B-&gt;&gt;A: auth JWT</code></pre>"},{"location":"security/remote-authenticator/#the-interface","title":"The Interface","text":"<p>To configure lakeFS to work with a Remote Authenticator add the following YAML to your lakeFS configuration:</p> <pre><code>auth:\n    remote_authenticator:\n        enabled: true\n        endpoint: &lt;url-to-remote-authenticator-endpoint&gt;\n        default_user_group: \"Developers\"\n    ui_config:\n        logout_url: /logout\n        login_cookie_names:\n            - internal_auth_session\n</code></pre> <p>Info</p> <p>lakeFS OSS uses a single user: only the admin user created during initialization exists. When using Remote Authenticator with OSS, authentication is limited to this admin user. Successful authentication is only possible if the Remote Authenticator returns an <code>external_user_identifier</code> that matches the admin user name created at setup.</p> <p>lakeFS Enterprise supports multiple users. With Remote Authenticator enabled, both existing and new users authenticated by the Remote Authenticator can log in to lakeFS and will be associated with the configured default user group (if specified). This allows organizations to manage access for multiple users and leverage their existing identity infrastructure.</p> <ul> <li><code>auth.remote_authenticator.enabled</code> - set lakeFS to use the remote authenticator</li> <li><code>auth.remote_authenticator.endpoint</code> - an endpoint where the remote authenticator is able to receive a POST request from lakeFS</li> <li><code>auth.remote_authenticator.default_user_group</code> - the group assigned by default to new users</li> <li><code>auth.ui_config.logout_url</code> - the URL to redirect the browser when clicking the logout link in the user menu</li> <li><code>auth.ui_config.login_cookie_names</code> - the name of the cookie(s) lakeFS will set following a successful authentication. The value is the authenticated user's JWT</li> </ul> <p>A Remote Authenticator implementation should expose a single endpoint, which expects the following JSON request:</p> <pre><code>{\n    \"username\": \"testy.mctestface@example.com\",\n    \"password\": \"Password1\"\n}\n</code></pre> <p>and returns a JSON response like this:</p> <pre><code>{\n    \"external_user_identifier\": \"TestyMcTestface\"\n}\n</code></pre>"},{"location":"security/remote-authenticator/#example-request-responses","title":"Example Request &amp; Responses","text":""},{"location":"security/remote-authenticator/#request","title":"Request","text":"<pre><code>POST https://remote-authenticator.example.com/auth\nContent-Type: application/json\n\n{\n  \"username\": \"testy.mctestface@example.com\",\n  \"password\": \"Password1\"\n}\n</code></pre>"},{"location":"security/remote-authenticator/#successful-response","title":"Successful Response","text":"<pre><code>HTTP/1.1 200 OK\nContent-Type: application/json\n\n\n{\n  \"external_user_identifier\": \"TestyMcTestface\"\n}\n</code></pre>"},{"location":"security/remote-authenticator/#unauthorized-response","title":"Unauthorized Response","text":"<pre><code>HTTP/1.1 401 Unauthorized\nContent-Type: application/json\n\n{\n  \"external_user_identifier\": \"\"\n}\n</code></pre> <p>If the Remote Authenticator returns any HTTP status in the 2xx range, lakeFS considers this a successful authentication. Any HTTP status &lt; 200 or &gt; 300 is considered a failed authentication. If the Remote Authenticator returns a non-empty value for the <code>external_user_identifier</code> property along with a success HTTP status, lakeFS will show this identifier instead of an internal lakeFS user identifier in the UI.</p>"},{"location":"security/remote-authenticator/#sample-implementation","title":"Sample Implementation","text":"<p>Here is a sample Remote Authenticator implemented using node and express and written in TypeScript. This example implementation doesn't integrate with any real IdP but illustrates the expected request/response patterns that you need to implement.</p> <pre><code>import dotenv from \"dotenv\";\nimport express, { Express, Request, Response } from \"express\";\nimport { StatusCodes } from \"http-status-codes\";\n\ntype AuthRequestBody = {\n  username: string;\n  password: string;\n};\n\ntype AuthResponseBody = {\n  external_user_identifier: string;\n};\n\nconst DEFAULT_PORT = 80;\n\ndotenv.config();\n\nconst port = process.env.PORT || DEFAULT_PORT;\nconst app: Express = express();\n\napp.post(\n  \"/auth\",\n  (req: Request&lt;AuthResponseBody, {}, AuthRequestBody&gt;, res: Response) =&gt; {\n    const { username, password } = req.body;\n    if (!username?.length || !password?.length) {\n      return res.status(StatusCodes.BAD_REQUEST).json({\n        external_user_identifier: \"\",\n      });\n    }\n\n    // \ud83d\udc47\ud83c\udffb This is where you would implement your own authentication logic\n    if (\n      username === \"testy.mctestface@example.com\" &amp;&amp;\n      password === \"Password1\"\n    ) {\n      return res.status(StatusCodes.OK).json({\n        external_user_identifier: \"TestyMcTestface\",\n      });\n    } else {\n      return res.status(StatusCodes.UNAUTHORIZED).json({\n        external_user_identifier: \"\",\n      });\n    }\n  }\n);\n\napp.listen(port, () =&gt; {\n  console.log(`Remote Authenticator listening on port ${port}`);\n});\n</code></pre> <p>To run this service on the sub-domain <code>idp.example.com</code>, use a lakeFS configuration that looks like this:</p> <pre><code>auth:\n    remote_authenticator:\n        enabled: true\n        endpoint: https://idp.example.com/auth\n        default_user_group: \"Developers\"\n    ui_config:\n        logout_url: /logout\n        login_cookie_names:\n            - internal_auth_session\n</code></pre>"},{"location":"security/sso/","title":"Single Sign On (SSO)","text":"<p>Info</p> <p>Available in lakeFS Cloud and lakeFS Enterprise If you're using the open-source version of lakeFS you can read more about the authentication options available.</p>","boost":2},{"location":"security/sso/#sso-for-lakefs-cloud","title":"SSO for lakeFS Cloud","text":"<p>lakeFS Cloud uses Auth0 for authentication and thus supports the same identity providers as Auth0 including Active Directory/LDAP, ADFS, Azure Active Directory Native, Google Workspace, OpenID Connect, Okta, PingFederate, SAML, and Azure Active Directory.</p> OktaActive Directory Federation Services (AD FS)Azure Active Directory (AD) <p>Note</p> <p>This guide is based on Okta's Create OIDC app integrations guide.</p> <p>Steps:</p> <ol> <li>Login to your Okta account</li> <li>Select Applications &gt; Applications, then Create App Integration.</li> <li>Select Create New App and enter the following:<ol> <li>For Sign-in method, choose OIDC.</li> <li>Under Application type, choose Web app.</li> <li>Select Next.</li> </ol> </li> <li>Under General Settings:<ol> <li>App integration name, enter a name for your application. (i.e lakeFS Cloud)</li> </ol> </li> <li>In the Sign-in redirect URIs field, enter https://lakefs-cloud.us.auth0.com/login (United States) or https://lakefs-cloud.eu.auth0.com/login (Europe).</li> <li>Under Sign-in redirect URIs, click Add URI, enter https://lakefs-cloud.us.auth0.com/login/callback (United States) or https://lakefs-cloud.eu.auth0.com/login/callback (Europe).</li> <li>Under Assignments, choose the wanted Controlled access. (i.e Allow everyone in your organization to access)</li> <li>Uncheck Enable immediate access with Federation Broker Mode.</li> <li>Select Save.</li> </ol> <p>Once you finish registering your application with Okta, save the Client ID, Client Secret and your Okta Domain, send this to Treeverse's team to finish the integration.</p> <p>Prerequisites:</p> <ul> <li>Client's AD FS server should be exposed publicly or to Auth0's IP ranges (either directly or using Web Application Proxy)</li> </ul> <p>Steps:</p> <ol> <li>Connect to the AD FS server</li> <li>Open AD FS' PowerShell CLI as Administrator through the server manager</li> <li> <p>Execute the following:     <pre><code>(new-object Net.WebClient -property @{Encoding = [Text.Encoding]::UTF8}).DownloadString(\"https://raw.github.com/auth0/adfs-auth0/master/adfs.ps1\") | iex\n\nAddRelyingParty \"urn:auth0:lakefs-cloud\" \"https://lakefs-cloud.us.auth0.com/login/callback\"\n</code></pre></p> <p>Note</p> <p>If your organization data is located in Europe, use <code>lakefs-cloud.eu.auth0.com</code> instead of <code>lakefs-cloud.us.auth0.com</code>.</p> </li> </ol> <p>Once you finish registering lakeFS Cloud with AD FS, save the AD FS URL and send this to Treeverse's team to finish the integration.</p> <p>Prerequisites:</p> <ul> <li>Azure account with permissions to manage applications in Azure Active Directory</li> </ul> <p>Note</p> <p>If you've already set up lakeFS Cloud with your Azure account, you can skip the Register lakeFS Cloud with Azure and Add client secret and go directly to Add a redirect URI.</p> <p>Register lakeFS Cloud with Azure</p> <p>Steps:</p> <ol> <li>Sign in to the Azure portal.</li> <li>If you have access to multiple tenants, use the Directories + subscriptions filter in the top menu to switch to the tenant in which you want to register the application.</li> <li>Search for and select Azure Active Directory.</li> <li>Under Manage, select App registrations &gt; New registration.</li> <li>Enter a display Name for your application. Users of your application might see the display name when they use the app, for example during sign-in. You can change the display name at any time and multiple app registrations can share the same name. The app registration's automatically generated Application (client) ID, not its display name, uniquely identifies your app within the identity platform.</li> <li> <p>Specify who can use the application, sometimes called its sign-in audience.</p> <p>Note</p> <p>don't enter anything for Redirect URI (optional). You'll configure a redirect URI in the next section.</p> </li> <li> <p>Select Register to complete the initial app registration.</p> </li> </ol> <p>When registration finishes, the Azure portal displays the app registration's Overview pane. You see the Application (client) ID. Also called the client ID, this value uniquely identifies your application in the Microsoft identity platform.</p> <p>Important: new app registrations are hidden to users by default. When you are ready for users to see the app on their My Apps page you can enable it. To enable the app, in the Azure portal navigate to Azure Active Directory &gt; Enterprise applications and select the app. Then on the Properties page toggle Visible to users? to Yes.</p> <p>Add a secret</p> <p>Sometimes called an application password, a client secret is a string value your app can use in place of a certificate to identity itself.</p> <p>Steps:</p> <ol> <li>In the Azure portal, in App registrations, select your application.</li> <li>Select Certificates &amp; secrets &gt; Client secrets &gt; New client secret.</li> <li>Add a description for your client secret.</li> <li>Select an expiration for the secret or specify a custom lifetime.<ol> <li>Client secret lifetime is limited to two years (24 months) or less. You can't specify a custom lifetime longer than 24 months.</li> <li>Microsoft recommends that you set an expiration value of less than 12 months.</li> </ol> </li> <li>Select Add.</li> <li>Record the secret's value for use in your client application code. This secret value is never displayed again after you leave this page.</li> </ol> <p>Add a redirect URI A redirect URI is the location where the Microsoft identity platform redirects a user's client and sends security tokens after authentication.</p> <p>You add and modify redirect URIs for your registered applications by configuring their platform settings.</p> <p>Enter https://lakefs-cloud.us.auth0.com/login/callback as your redirect URI.</p> <p>Settings for each application type, including redirect URIs, are configured in Platform configurations in the Azure portal. Some platforms, like Web and Single-page applications, require you to manually specify a redirect URI. For other platforms, like mobile and desktop, you can select from redirect URIs generated for you when you configure their other settings.</p> <p>Steps:</p> <ol> <li>In the Azure portal, in App registrations, select your application.</li> <li>Under Manage, select Authentication.</li> <li>Under Platform configurations, select Add a platform.</li> <li>Under Configure platforms, select the web option.</li> <li>Select Configure to complete the platform configuration.</li> </ol> <p>Once you finish registering lakeFS Cloud with Azure AD send the following items to the Treeverse's team:</p> <ol> <li>Client ID</li> <li>Client Secret</li> <li>Azure AD Domain</li> <li>Identity API Version (v1 for Azure AD or v2 for Microsoft Identity Platform/Entra) </li> </ol>","boost":2},{"location":"security/sso/#sso-for-lakefs-enterprise","title":"SSO for lakeFS Enterprise","text":"<p>Starting from v1.63.0, authentication in lakeFS Enterprise is handled directly by the lakeFS Enterprise service. lakeFS Enterprise supports the following identity providers:</p> <ul> <li>Active Directory Federation Services (AD FS) (using SAML)</li> <li>OpenID Connect</li> <li>LDAP</li> <li>External AWS Authentication (using IAM)</li> </ul> <p>If you're using an authentication provider that is not listed, please contact us for further assistance.</p> Active Directory Federation Services (AD FS) (using SAML)OpenID ConnectLDAPExternal AWS Authentication <p>Note</p> <p>AD FS integration uses certificates to sign and encrypt requests going out from lakeFS Enterprise and decrypt incoming requests from AD FS server.</p> <p>If you'd like to generate a self-signed certificates using OpenSSL:</p> <p><pre><code>openssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=lakefs.company.com\"\n</code></pre> In order for SAML authentication to work, configure the following values in your chart's <code>values.yaml</code> file:</p> <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths: \n        - /\n\nenterprise:\n  enabled: true\n  auth:\n    saml:\n      enabled: true\n      createCertificateSecret: true  # NEW: Auto-creates secret\n      certificate:\n        samlRsaPublicCert: |           # RENAMED: from saml_rsa_public_cert\n          -----BEGIN CERTIFICATE-----\n          ...\n          -----END CERTIFICATE-----\n        samlRsaPrivateKey: |           # RENAMED: from saml_rsa_private_key\n          -----BEGIN PRIVATE KEY-----\n          ...\n          -----END PRIVATE KEY-----\n\nimage:\n  privateRegistry:\n    enabled: true\n    secretToken: &lt;dockerhub-token&gt;\n\nlakefsConfig: |\n  blockstore:\n    type: local\n  auth:\n    logout_redirect_url: https://&lt;lakefs.ingress.domain&gt; \n    cookie_auth_verification:\n      auth_source: saml\n      # claim name to display user in the UI\n      friendly_name_claim_name: displayName\n      # claim name from IDP to use as the unique user name\n      external_user_id_claim_name: samName\n      default_initial_groups:\n        - \"Developers\"\n    providers:\n      saml:\n        post_login_redirect_url: https://&lt;lakefs.ingress.domain&gt;\n        sp_root_url: https://&lt;lakefs.ingress.domain&gt;\n        sp_sign_request: true \n        # depends on IDP\n        sp_signature_method: \"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\"\n        # url to the metadata of the IDP\n        idp_metadata_url: \"https://&lt;adfs-auth.company.com&gt;/federationmetadata/2007-06/federationmetadata.xml\"\n        # IDP SAML claims format default unspecified\n        idp_authn_name_id_format: \"urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified\"\n        # depending on IDP setup, if CA certs are self signed and not trusted by a known CA\n        #idp_skip_verify_tls_cert: true\n</code></pre> <p>In order for OIDC to work, configure the following values in your chart's <code>values.yaml</code> file:</p> <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths: \n        - /\n\nenterprise:\n  enabled: true\n  auth:\n    oidc:\n      enabled: true\n      # secret given by the OIDC provider (e.g auth0, Okta, etc)\n      client_secret: &lt;oidc-client-secret&gt;\n\nimage:\n  privateRegistry:\n    enabled: true\n    secretToken: &lt;dockerhub-token&gt;\n\nlakefsConfig: |\n  blockstore:\n    type: local\n  auth:\n    logout_redirect_url: https://oidc-provider-url.com/logout/example \n    oidc:\n      friendly_name_claim_name: &lt;some-oidc-provider-claim-name&gt;\n      default_initial_groups: [\"Developers\"]\n    providers:\n      oidc:\n        post_login_redirect_url: /\n        url: https://oidc-provider-url.com/ \n        client_id: &lt;oidc-client-id&gt;         \n        callback_base_url: https://&lt;lakefs.ingress.domain&gt;\n        # the claim name that represents the client identifier in the OIDC provider (e.g Okta)\n        logout_client_id_query_parameter: client_id\n        # query parameters used to redirect the user to the OIDC provider (e.g., Okta) after logout.\n        # If you are using `auth.ui_config.login_url_method: select` and want to redirect\n        # back to the login selection page, set the return address to lakeFS:\n        # [\"returnTo\", \"https://&lt;lakefs.ingress.domain&gt;/\"]\n        logout_endpoint_query_parameters:\n          - returnTo\n          - https://&lt;lakefs.ingress.domain&gt;/oidc/login\n</code></pre> <p>lakeFS Enterprise provides direct LDAP authentication by querying the LDAP server for user information and authenticating the user based on the provided credentials.</p> <p>Important: An administrative bind user must be configured. It should have search permissions for the LDAP server.</p> <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths:\n        - /\n\nenterprise:\n  enabled: true\n  auth:\n    ldap:\n      enabled: true\n      bindPassword: &lt;ldap bind password&gt;\n\nimage:\n  privateRegistry:\n    enabled: true\n    secretToken: &lt;dockerhub-token&gt;\n\nlakefsConfig: |\n  blockstore:\n    type: local\n  auth:\n    providers:\n      ldap:\n        server_endpoint: ldaps://ldap.company.com:636\n        bind_dn: uid=&lt;bind-user-name&gt;,ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com \n        username_attribute: uid\n        user_base_dn: ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com  \n        user_filter: (objectClass=inetOrgPerson)\n        connection_timeout_seconds: 15\n        request_timeout_seconds: 7\n        # RBAC group for first time users\n        default_user_group: \"Developers\"\n</code></pre> <p>lakeFS Enterprise supports authentication using AWS IAM credentials. This allows users to authenticate using their AWS credentials via GetCallerIdentity.</p> <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    - host: &lt;lakefs.ingress.domain&gt;\n      paths:\n        - /\n\nlakefsConfig: |\n  auth:\n    external_aws_auth:\n      enabled: true\n      # the maximum age in seconds for the GetCallerIdentity request\n      #get_caller_identity_max_age: 60\n      # headers that must be present by the client when doing login request\n      required_headers:\n        # same host as the lakeFS server ingress\n        X-LakeFS-Server-ID: &lt;lakefs.ingress.domain&gt;\n</code></pre>","boost":2},{"location":"security/sso/#troubleshooting-ldap-issues","title":"Troubleshooting LDAP issues","text":"","boost":2},{"location":"security/sso/#inspecting-logs","title":"Inspecting Logs","text":"<p>If you encounter LDAP connection errors, inspect the lakeFS container logs to get more information.</p>","boost":2},{"location":"security/sso/#authentication-issues","title":"Authentication issues","text":"<p>Auth issues (e.g. user not found, invalid credentials) can be debugged with the <code>ldapwhoami</code> CLI tool. </p> <p>The examples are based on the configuration above:</p> <p>To verify that the main bind user can connect:</p> <pre><code>ldapwhoami -H ldaps://ldap.company.com:636 -D \"uid=bind-user-name,ou=Users,o=org-id,dc=company,dc=com\" -x -W\n</code></pre> <p>To verify that a specific lakeFS user <code>dev-user</code> can connect:</p> <pre><code>ldapwhoami -H ldaps://ldap.company.com:636 -D \"uid=dev-user,ou=Users,o=org-id,dc=company,dc=com\" -x -W\n</code></pre>","boost":2},{"location":"security/sso/#user-not-found-issue","title":"User not found issue","text":"<p>Upon a login request, the bind user will search for the user in the LDAP server. If the user is not found it will be presented in the logs.</p> <p>We can search the user using ldapsearch CLI tool. </p> <p>Search ALL users in the base DN (no filters):</p> <p>Note</p> <p><code>-b</code> is the <code>user_base_dn</code>, <code>-D</code> is <code>bind_dn</code> and <code>-w</code> is <code>bind_password</code> from the lakeFS configuration.</p> <pre><code>ldapsearch -H ldaps://ldap.company.com:636 -x -b \"ou=Users,o=org-id,dc=company,dc=com\" -D \"uid=bind-user-name,ou=Users,o=org-id,dc=company,dc=com\" -w 'bind_user_pwd'\n</code></pre> <p>If the user is found, we should now use filters for the specific user the same way lakeFS does it and expect to see the user. </p> <p>For example, to reproduce the same search as lakeFS does: - user <code>dev-user</code> set from <code>uid</code> attribute in LDAP  - Configuration values: <code>user_filter: (objectClass=inetOrgPerson)</code> and <code>username_attribute: uid</code></p> <pre><code>ldapsearch -H ldaps://ldap.company.com:636 -x -b \"ou=Users,o=org-id,dc=company,dc=com\" -D \"uid=bind-user-name,ou=Users,o=org-id,dc=company,dc=com\" -w 'bind_user_pwd' \"(&amp;(uid=dev-user)(objectClass=inetOrgPerson))\"\n</code></pre>","boost":2},{"location":"security/sso/#common-configuration","title":"Common Configuration","text":"<p>All authentication methods share some common configuration options:</p> <pre><code># Enable enterprise features\nenterprise:\n  enabled: true\n\n# Common lakeFS configuration\nlakefsConfig: |\n  # Basic auth configuration\n  auth:\n    encrypt:\n      secret_key: &lt;your-encryption-secret&gt;  # Set via secrets.authEncryptSecretKey\n    login_duration: 24h                      # Session duration\n    login_max_duration: 168h                 # Maximum session duration\n</code></pre>","boost":2},{"location":"security/sso/#secrets-management","title":"Secrets Management","text":"<p>The Helm chart will automatically create Kubernetes secrets for sensitive values:</p> <pre><code>secrets:\n  # Required: encryption key for auth cookies\n  authEncryptSecretKey: &lt;your-random-secret-string&gt;\n\n  # For database connection (if using external DB)\n  databaseConnectionString: &lt;postgres-connection-string&gt;\n\n# Or use existing secrets\nsecrets:\n  existingSecret: my-lakefs-secrets\n  # Define the keys in your secret:\n  authEncryptSecretKeyName: authEncryptSecretKey\n  databaseConnectionStringKeyName: databaseConnectionString\n</code></pre> <p>Authentication provider secrets are managed separately via the <code>enterprise.auth.*</code> configuration.</p>","boost":2},{"location":"security/sso/#environment-variables","title":"Environment Variables","text":"<p>All configurations can also be set via environment variables:</p> <pre><code># SAML\nLAKEFS_AUTH_PROVIDERS_SAML_SP_ROOT_URL=https://lakefs.company.com\nLAKEFS_AUTH_COOKIE_AUTH_VERIFICATION_EXTERNAL_USER_ID_CLAIM_NAME=samName\n\n# OIDC\nLAKEFS_AUTH_PROVIDERS_OIDC_CLIENT_ID=your-client-id\nLAKEFS_AUTH_OIDC_FRIENDLY_NAME_CLAIM_NAME=name\n\n# LDAP\nLAKEFS_AUTH_PROVIDERS_LDAP_SERVER_ENDPOINT=ldaps://ldap.company.com:636\nLAKEFS_AUTH_PROVIDERS_LDAP_BIND_DN=uid=bind-user,ou=Users,o=org,dc=company,dc=com\n\n# External AWS Auth\nLAKEFS_AUTH_EXTERNAL_AWS_AUTH_ENABLED=true\n</code></pre>","boost":2},{"location":"security/sts-login/","title":"Short-lived token (STS like)","text":"<p>Info</p> <p>Available in lakeFS Cloud and lakeFS Enterprise</p> <p>Secure Token Service (STS) authentication in lakeFS enables users to authenticate to lakeFS using temporary credentials obtained from an Identity Provider (IdP) via the OpenID Connect (OIDC) Authentication workflow. This document outlines the process of setting up the STS authentication flow and using the temporary credentials to interact with lakeFS through the high-level Python SDK.</p>","boost":2},{"location":"security/sts-login/#login","title":"Login","text":"<p>Initiate a client session with temporary credentials using the high-level Python SDK:</p> <pre><code>import lakefs\n\nmy_client = lakefs.client.from_web_identity(code = '&lt;CODE_FROM_IDP&gt;', state = '&lt;STATE_FROM_IDP&gt;' , redirect_uri = '&lt;URI_USED_FOR_REDIRECT_FROM_IDP&gt;', ttl_seconds = 7200)\n</code></pre>","boost":2},{"location":"security/sts-login/#setup","title":"Setup","text":"","boost":2},{"location":"security/sts-login/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have a way to generate the code, redirect_uri, and state values that are required to initiate a new client session using the STS login feature. For a reference implementation, see the sample implementation section.</p>","boost":2},{"location":"security/sts-login/#configuration","title":"Configuration","text":"<p>To enable STS authentication, configure your lakeFS instance with the endpoint of the external Authentication Service.</p> <pre><code>auth:\n    authentication_api:\n        endpoint: &lt;url-to-remote-authenticator-endpoint&gt;\n</code></pre> <p>The endpoint value should point to the external Authentication Service described at authentication.yml.   Make sure to replace  with the actual URL of your Authentication Service.","boost":2},{"location":"security/sts-login/#sample-implementation-to-generate-the-code-redirect_uri-and-state","title":"Sample implementation to generate the code, redirect_uri, and state","text":"<p>The following code snippet demonstrates how to generate the values that are required to initiate a new client session using the STS login feature.</p> <p>Info</p> <p>Replace <code>&lt;your-authorize-endpoint&gt;</code> with the path to your IdP authorize endpoint.  </p> <p>Examples</p> <ul> <li>Auth0: The authorize endpoint will be <code>https://&lt;your-auth0-domain&gt;/authorize</code> </li> <li>Entra ID: The authorize endpoint will be <code>https://&lt;your-entra-domain&gt;/oauth2/v2.0/authorize</code></li> </ul> <pre><code>import crypto from 'crypto';\n\nimport express from 'express';\nimport axios from 'axios';\nimport url from 'url';\nimport jsonwebtoken from 'jsonwebtoken';\n\nconst app = express();\n// the local script will will spin up the server and the IdP provider will return to this endpoint the response.\nconst callback = \"http://localhost:8080/oidc/callback\"\nconst authorizeEndpoint = \"&lt;your-authorize-endpoint&gt;\"\n\n// step 1 \n// Create a code_verifier, which is a cryptographically-random, Base64-encoded key that will eventually be sent to Auth0 to request tokens.\nfunction base64URLEncode(str) {\n    return str.toString('base64')\n        .replace(/\\+/g, '-')\n        .replace(/\\//g, '_')\n        .replace(/=/g, '');\n}\nvar verifier = base64URLEncode(crypto.randomBytes(32));\nconsole.log(`verifier: ${verifier}`);\n\n// step 2 \n// Generate a code_challenge from the code_verifier that will be sent to Auth0 to request an authorization_code.\nfunction sha256(buffer) {\n    return crypto.createHash('sha256').update(buffer).digest();\n}\n\nvar challenge = base64URLEncode(sha256(verifier));\nconsole.log(`challenge: ${challenge}`);\n\n\nconst authorizeURL = `${authorizeEndpoint}?response_type=code&amp;code_challenge=${challenge}&amp;code_challenge_method=S256&amp;client_id=${auth0ClientId}&amp;redirect_uri=${callback}&amp;scope=openid&amp;state=${verifier}`\n\nconsole.log(`authorizeURL: ${authorizeURL}`)\n\n// Endpoint for OIDC callback\napp.get('/oidc/callback', async (req, res) =&gt; {\n    try {\n        const code = req.query.code;\n        const state = req.query.state;\n        console.log(`code: ${code}`);\n        console.log(`state: ${state}`);\n        // Return a success response\n        res.status(200).json({ code, state, redirect_uri: callback, python-cmd: `lakefs.client.from_web_identity(code = ${code} redirect_uri = ${callback} state = ${state}, ttl_seconds = 7200) ` });\n        return\n    } catch (err) {\n        console.error(err);\n        res.status(500).json({ message: 'Internal server error' });\n    }\n});\n\n// Start the server\nconst PORT = 8080;\napp.listen(PORT, () =&gt; {\n    console.log(`Server is running on port ${PORT}`);\n});\n</code></pre>","boost":2},{"location":"security/sts-login/#architecture","title":"Architecture","text":"<p>The STS authentication flow involves several components that facilitate secure communication between the lakeFS client, lakeFS server, the remote authenticator, and the IdP.</p> <pre><code>sequenceDiagram\n    participant A as lakeFS Client\n    participant B as lakeFS Server\n    participant C as Remote Authenticator\n    participant D as IdP\n    A-&gt;&gt;B: Call STS login endpoint\n    B-&gt;&gt;C: POST idp code state and redirect uri\n    C-&gt;&gt;D: IdP request\n    D-&gt;&gt;C: IdP response\n    C-&gt;&gt;B: Auth response\n    B-&gt;&gt;A: auth JWT</code></pre> <ul> <li>lakeFS Client: Initiates the authentication process by providing IdP credentials.</li> <li>lakeFS Server: Facilitates the authentication request between the client and the remote authenticator.</li> <li>Remote Authenticator: Acts as a bridge between lakeFS and the IdP, handling credential validation.</li> <li>IdP (Identity Provider): Validates the provided credentials and returns the authentication status.</li> </ul>","boost":2},{"location":"understand/architecture/","title":"lakeFS Architecture","text":"<p>lakeFS is distributed as a single binary encapsulating several logical services.</p> <p>The server itself is stateless, meaning you can easily add more instances to handle a bigger load.</p> <p></p>"},{"location":"understand/architecture/#object-storage","title":"Object Storage","text":"<p>lakeFS manages data stored on various object storage platforms, including:</p> <ul> <li>AWS S3</li> <li>Google Cloud Storage</li> <li>Azure Blob Storage</li> <li>MinIO</li> <li>NetApp StorageGRID</li> <li>Ceph</li> <li>Any other S3-compatible storage</li> </ul> <p>With lakeFS Enterprise, you can leverage multiple storage backend support to manage data across multiple storage locations, including on-prem, hybrid, and multi-cloud environments.</p>"},{"location":"understand/architecture/#metadata-storage","title":"Metadata Storage","text":"<p>An auxiliary Key Value storage is used for storing metadata, with supported databases including:</p> <ul> <li>PostgreSQL</li> <li>DynamoDB</li> <li>CosmosDB</li> <li>MemoryDB (or other Redis-compatible options) \ud83d\ude80</li> </ul> <p>See the installation guide and configuration reference on how to setup lakeFS with any of the above options</p> <p>Learn More</p> <p>More information about how lakeFS manages its versioning metadata is available in Versioning internals and Internal database structure</p>"},{"location":"understand/architecture/#load-balancing","title":"Load Balancing","text":"<p>Accessing lakeFS is done using HTTP. lakeFS exposes a frontend UI, an OpenAPI server, as well as an S3-compatible service (see S3 Gateway below). lakeFS uses a single port that serves all three endpoints, so for most use cases a single load balancer pointing to lakeFS server(s) would do.</p>"},{"location":"understand/architecture/#lakefs-components","title":"lakeFS Components","text":"<p>Internally, the lakeFS server is composed of a few key pieces that make up the single binary that lakeFS is distributed as:</p> <p></p>"},{"location":"understand/architecture/#s3-gateway","title":"S3 Gateway","text":"<p>The S3 Gateway is the layer in lakeFS responsible for the compatibility with S3. It implements a compatible subset of the S3 API to ensure most data systems can use lakeFS as a drop-in replacement for S3.</p> <p>See the S3 API Reference section for information on supported API operations.</p>"},{"location":"understand/architecture/#openapi-server","title":"OpenAPI Server","text":"<p>The Swagger (OpenAPI) server exposes the full set of lakeFS operations (see Reference). This includes basic CRUD operations against repositories and objects, as well as versioning related operations such as branching, merging, committing, and reverting changes to data.</p>"},{"location":"understand/architecture/#storage-adapter","title":"Storage Adapter","text":"<p>The Storage Adapter is an abstraction layer for communicating with any underlying object store. Its implementations allow compatibility with many types of underlying storage such as S3, GCS, Azure Blob Storage, or non-production usages such as the local storage adapter.</p> <p>See the roadmap for information on the future plans for storage compatibility.</p>"},{"location":"understand/architecture/#graveler","title":"Graveler","text":"<p>The Graveler handles lakeFS versioning by translating lakeFS addresses to the actual stored objects. To learn about the data model used to store lakeFS metadata, see the versioning internals page.</p>"},{"location":"understand/architecture/#authentication-authorization-service","title":"Authentication &amp; Authorization Service","text":"<p>The Auth service handles the creation, management, and validation of user credentials and RBAC policies.</p> <p>The credential scheme, along with the request signing logic, are compatible with AWS IAM (both SIGv2 and SIGv4).</p> <p>Currently, the Auth service manages its own database of users and credentials and doesn't use IAM in any way.</p>"},{"location":"understand/architecture/#hooks-engine","title":"Hooks Engine","text":"<p>The Hooks Engine enables CI/CD for data by triggering user defined Actions that will run during commit/merge.</p>"},{"location":"understand/architecture/#ui","title":"UI","text":"<p>The UI layer is a simple browser-based client that uses the OpenAPI server. It allows management, exploration, and data access to repositories, branches, commits and objects in the system.</p>"},{"location":"understand/architecture/#lakefs-clients","title":"lakeFS Clients","text":"<p>Some data applications benefit from deeper integrations with lakeFS to support different use cases or enhanced functionality provided by lakeFS clients.</p>"},{"location":"understand/architecture/#openapi-generated-sdks","title":"OpenAPI Generated SDKs","text":"<p>OpenAPI specification can be used to generate lakeFS clients for many programming languages. For example, the Python lakefs-sdk or the Java client are published with every new lakeFS release.</p>"},{"location":"understand/architecture/#lakectl","title":"lakectl","text":"<p>lakectl is a CLI tool that enables lakeFS operations using the lakeFS API from your preferred terminal.</p>"},{"location":"understand/architecture/#lakefs-mount-everest","title":"lakeFS Mount (Everest)","text":"<p>Info</p> <p>lakeFS Mount is available in lakeFS Cloud and lakeFS Enterprise</p> <p>lakeFS Mount allows users to virtually mount a remote lakeFS repository onto a local directory. Once mounted, users can access the data as if it resides on their local filesystem, using any tool, library, or framework that reads from a local filesystem.</p>"},{"location":"understand/architecture/#spark-metadata-client","title":"Spark Metadata Client","text":"<p>The lakeFS Spark Metadata Client makes it easy to perform operations related to lakeFS metadata, at scale. Examples include garbage collection or exporting data from lakeFS.</p>"},{"location":"understand/architecture/#lakefs-hadoop-filesystem","title":"lakeFS Hadoop FileSystem","text":"<p>Thanks to the S3 Gateway, it's possible to interact with lakeFS using Hadoop's S3AFIleSystem, but due to limitations of the S3 API, doing so requires reading and writing data objects through the lakeFS server. Using lakeFSFileSystem increases Spark ETL jobs performance by executing the metadata operations on the lakeFS server, and all data operations directly through the same underlying object store that lakeFS uses.</p>"},{"location":"understand/architecture/#how-lakefs-clients-and-gateway-handle-metadata-and-data-access","title":"How lakeFS Clients and Gateway Handle Metadata and Data Access","text":"<p>When using any of the native integrations such as the Python SDK, lakectl, Everest or the lakeFS Spark client - these clients communicate with the lakeFS server to retrieve metadata. </p> <p>For example, they may query lakeFS to understand which version of a file is needed or to track changes in branches and commits.  This communication does not include the actual data transfer, but instead involves passing only metadata about data locations and versions.</p> <p>Once the client knows the exact data location from the lakeFS metadata, it directly accesses the data in the underlying object storage (potentially using presigned URLs) without routing through lakeFS. </p> <p>Example</p> <p>if data is stored in S3, the Spark client will retrieve the S3 paths as pre-signed URLs from lakeFS, then directly read and/or write to those URLs in S3 without involving lakeFS in the data transfer.</p> <p></p>"},{"location":"understand/data-structure/","title":"How Does lakeFS Store Your Data","text":"<p>lakeFS being a data versioning engine, requires the ability to save multiple versions of the same object. As a result, lakeFS stores objects in the object store in way that allows it to version the data in an efficient way. This might cause confusion when trying to understand where our data is actually being stored. This page will try to shed a light on this subject.</p>"},{"location":"understand/data-structure/#lakefs-repository-namespace-structure","title":"lakeFS Repository Namespace Structure","text":"<p>lakeFS stores repository data and metadata under the repository's namespace. The lakeFS repository namespace is a dedicated path under the object store used by lakeFS to manage a repository. Listing a repository storage namespace in the object store will provide the following output:</p> <pre><code>aws s3 ls s3://&lt;storage_namespace&gt;/\n                        PRE _lakefs/\n                        PRE data/\n</code></pre> <p>lakeFS stores the actual user data under the <code>data/</code> prefix. The <code>_lakefs/</code> prefix is used to store commit metadata which includes range and meta-range files and internal lakeFS data. Since lakeFS manages immutable data, objects are not saved using their logical name - these might get overwritten, violating the immutability guarantee. This means that when you upload a csv file called <code>allstar_games_stats.csv</code> to branch main, lakeFS will generate a random physical address under the <code>data/</code> prefix and upload it to there. Mapping from a path to an object changes as you upload, commit, and merge on lakeFS. When updating an object, lakeFS will create a new physical address for that version preserving other versions of that object. lakeFS will link between the object's logical address and its physical address - and store that relation under the given commit metadata (range and meta-range)</p> <p>lakeFS uses its object store immutably i.e. anything uploaded is never changed or overridden (Refer to GC for explanation on how and when lakeFS actually deletes data from the storage). To find data, lakeFS uses the logical address e.g. <code>lakefs://my-repo/main/allstar_games_stats.csv</code>, indicating a repository and branch. Using the KV metadata store, lakeFS will first try to find any uncommitted version of the object in the given branch. If no uncommitted version exist, it will take the latest committed version from the branch head (which is the top commit of the branch)</p> <ol> <li>In the KV metadata store under the current staging token of branch main. This will return any uncommitted changes for the given object</li> <li>Read it from the branch's head meta-range and range (which are saved under the <code>_lakefs</code> prefix in the object store. This will return the metadata for the object as it was stored in the latest commit for branch main. The physical path returned will be in the form of <code>s3://&lt;storage_namespace&gt;/data/gp0n1l7d77pn0cke6jjg/cg6p50nd77pn0cke6jk0</code>. The same object in lakeFS might have several physical addresses, one for each version where it exists.</li> </ol>"},{"location":"understand/data-structure/#finding-an-objects-location-on-your-object-store","title":"Finding an object's location on your object store","text":"<p>One way to determine the physical location of an object is using the <code>lakectl fs stat</code> command:</p> <pre><code>lakectl fs stat --pre-sign=false lakefs://my-repo/main/allstar_games_stats.csv\nPath: allstar_games_stats.csv\nModified Time: 2024-08-02 10:13:33 -0400 EDT\nSize: 0 bytes\nHuman Size: 0 B\nPhysical Address: s3://niro-test/repos/docs/data/data/geh1jurck6tfom0s1t8g/cqmej33ck6tfom0s1tvg\nChecksum: d41d8cd98f00b204e9800998ecf8427e\nContent-Type: application/octet-stream\n</code></pre> <p>lakeFS can show any version of an object. For example: to see an object's physical location on branch <code>dev</code> from 3 versions ago, use reference dev~3:</p> <pre><code>lakectl fs stat lakefs://my-repo/dev~3/allstar_games_stats.csv\nPath: allstar_games_stats.csv\nModified Time: 2024-08-02 10:11:49 -0400 EDT\nSize: 916393 bytes\nHuman Size: 916.4 kB\nPhysical Address: s3://&lt;storage_namespace&gt;/data/data/geh1jurck6tfom0s1t8g/cqmei9bck6tfom0s1tt0\nChecksum: 48e04a4c072acdcf932ee6c43f46ef14\nContent-Type: application/octet-stream\n</code></pre> <p>This can be done using any lakeFS reference type.</p> <p>To learn more about the internals of lakeFS and how it stores your data, follow this blog post</p>"},{"location":"understand/faq/","title":"lakeFS Frequently Asked Questions (FAQ)","text":""},{"location":"understand/faq/#1-is-lakefs-open-source","title":"1. Is lakeFS open-source?","text":"<p>lakeFS is free, open-source, and licensed under the Apache 2.0 License. Code and issues are managed publicly on GitHub and a Slack channel for open discussions.</p>"},{"location":"understand/faq/#2-how-does-lakefs-data-versioning-work","title":"2. How does lakeFS data versioning work?","text":"<p>lakeFS uses zero-copy branching to avoid data duplication. That is, creating a new branch is a metadata-only operation: no objects are actually copied. Only when an object changes does lakeFS create another version of the data in the storage. </p> <p>Info</p> <p>For more information, see Versioning internals.</p>"},{"location":"understand/faq/#3-how-do-i-get-support-for-my-lakefs-installation","title":"3. How do I get support for my lakeFS installation?","text":"<p>We are extremely responsive on our Slack channel, and we make sure to prioritize the most pressing issues for the community. For SLA-based support, please contact us at support@treeverse.io.</p>"},{"location":"understand/faq/#4-do-you-collect-data-from-your-active-installations","title":"4. Do you collect data from your active installations?","text":"<p>We collect anonymous usage statistics to understand the patterns of use and to detect product gaps we may have so we can fix them. This is optional and may be turned off by setting <code>stats.enabled</code> to <code>false</code>. See the configuration reference for more details.</p> <p>The data we gather is limited to the following:</p> <ol> <li>A <code>UUID</code> which is generated when setting up lakeFS for the first time and contains no personal or otherwise identifiable information,</li> <li>The lakeFS version currently running,</li> <li>The OS and architecture lakeFS is running on,</li> <li>Metadata regarding the database used (version, installed extensions and parameters such as DB Timezone and work memory),</li> <li>Periodic aggregated action counters (e.g. how many \"get_object\" operations occurred).</li> </ol>"},{"location":"understand/faq/#5-how-is-lakefs-different-from-delta-lake-hudi-iceberg","title":"5. How is lakeFS different from Delta Lake / Hudi / Iceberg?","text":"<p>Delta Lake, Apache Hudi, and Apache Iceberg all define dedicated, structured data formats that allow deletes and upserts. lakeFS is format-agnostic and enables consistent cross-collection versioning of your data using Git-like operations. Read our comparison for a more detailed comparison. </p>"},{"location":"understand/faq/#6-what-inspired-the-lakefs-logo","title":"6. What inspired the lakeFS logo?","text":"<p>The Axolotl \u2013 a species of salamander, also known as the Mexican Lake Monster or the Peter Pan of the animal kingdom. It's a magical creature, living in a lake - just like us! :)</p> <p></p> <p> copyright </p>"},{"location":"understand/glossary/","title":"Glossary","text":"<p>This page has definition and explanations of all terms related to lakeFS technical internals and the architecture.</p>"},{"location":"understand/glossary/#auditing","title":"Auditing","text":"<p>Data auditing is data assessment to ensure its accuracy, security, and efficacy for specific usage. It also involves assessing data quality through its lifecycle and understanding the impact of poor quality data on the organization's performance and revenue. Ensuring data reproducibility, auditability, and governance is one of the key concerns of data engineers today. lakeFS commit history helps the data teams to keep track of all changes to the data, supporting data auditing.</p>"},{"location":"understand/glossary/#branch","title":"Branch","text":"<p>Branches in lakeFS allow users to create their own \"isolated\" view of the repository. Read more.</p>"},{"location":"understand/glossary/#collection","title":"Collection","text":"<p>A collection, roughly speaking, is a set of data. Collections may be structured or unstructured; a structured collection is often referred to as a table.</p>"},{"location":"understand/glossary/#commit","title":"Commit","text":"<p>Using commits, you can view a repository at a certain point in its history and you're guaranteed that the data you see is exactly as it was at the point of committing it. Read More.</p>"},{"location":"understand/glossary/#cross-collection-consistency","title":"Cross-Collection Consistency","text":"<p>It is unfortunate that the word 'consistency' has multiple meanings, at least four of them according to Martin Kleppmann. Consistency in the context of lakeFS and data versioning is, the guarantee that operations in a transaction are performed accurately, correctly and most important, atomically. </p> <p>A repository (and thus a branch) in lakeFS, can span multiple tables or collections. By providing branch, commit, merge and revert operations atomically on a branch, lakeFS achieves consistency guarantees across different logical collections. That is, data versioning is consistent across multiple collections within a repository.</p> <p>It is sometimes referred as multi-table transactions. That is, lakeFS offers transactional guarantees across multiple tables.</p>"},{"location":"understand/glossary/#data-lake-governance","title":"Data Lake Governance","text":"<p>The goal of data lake governance is to apply policies, standards and processes on the data. This allows creating high-quality data and ensuring that it\u2019s used appropriately across the organization. Data lake governance improves the data quality and increases data usage for business decision-making, leading to operational improvements, better-informed business strategies, and stronger financial performance. lakeFS Cloud offers advanced data lake management features such as: Role-Based Access Control, Branch Aware Managed Garbage Collection, Data Lineage and Audit log.</p>"},{"location":"understand/glossary/#data-lifecycle-management","title":"Data Lifecycle Management","text":"<p>In data-intensive applications, data should be managed through its entire lifecycle similar to how teams manage code. By doing so, we could leverage the best practices and tools from application lifecycle management (like CI/CD operations) and apply them to data. lakeFS offers data lifecycle management via isolated data development environments instead of shared buckets.</p>"},{"location":"understand/glossary/#data-pipeline-reproducibility","title":"Data Pipeline Reproducibility","text":"<p>Reproducibility in data pipelines is the ability to repeat a process. An example of this is recreating an issue that occurred in the production pipeline. Reproducibility allows for the controlled manufacture of an error to debug and troubleshoot it at a later point in time. Reproducing a data pipeline issue is a challenge that most data engineers face on a daily basis. Learn more about how lakeFS supports data pipeline reproducibility. Other use cases include running ad-hoc queries (useful for data science), review, and backfill.</p>"},{"location":"understand/glossary/#data-quality-testing","title":"Data Quality Testing","text":"<p>This term describes ways to test data for its accuracy, completeness, consistency, timeliness, validity, and integrity. lakeFS hooks can be used to implement and run data quality tests before promoting staging data into production. </p>"},{"location":"understand/glossary/#data-versioning","title":"Data Versioning","text":"<p>To version data means creating a unique point-in-time reference for data that can be accessed later. This reference can take the form of a query, an ID, or also commonly, a DateTime identifier. Data versioning may also include saving an entire copy of the data under a new name or file path every time you want to create a version of it. More advanced versioning solutions like lakeFS perform versioning through zero-copy data operations. lakeFS also optimizes storage usage between versions and exposes special operations to manage them.</p>"},{"location":"understand/glossary/#git-like-operations","title":"Git-like Operations","text":"<p>lakeFS allows teams to treat their data lake as a Git repository.   Git is used for code versioning, whereas lakeFS is used for data versioning.  lakeFS provides Git-like operations such as branch, commit, merge and revert.</p>"},{"location":"understand/glossary/#graveler","title":"Graveler","text":"<p>Graveler is the core versioning engine of lakeFS. It handles versioning by translating lakeFS addresses to the actual stored objects. See the versioning internals section to learn how lakeFS stores metadata.</p>"},{"location":"understand/glossary/#hooks","title":"Hooks","text":"<p>lakeFS hooks allow you to automate and ensure that a given set of checks and validations happens before important lifecycle events. They are similar conceptually to Git Hooks, but in contrast, they run remotely on a server. Currently, lakeFS allows executing hooks when two types of events occur: pre-commit events that run before a commit is acknowledged and pre-merge events that trigger right before a merge operation. </p>"},{"location":"understand/glossary/#isolated-data-snapshot","title":"Isolated Data Snapshot","text":"<p>Creating a branch in lakeFS provides an isolated environment containing a snapshot of your repository. While working on your branch in isolation, all other data users will be looking at the repository's main branch. So they won't see your changes, and you also won't see the changes applied to the main branch. All of this happens without any data duplication but metadata management.</p>"},{"location":"understand/glossary/#main-branch","title":"Main Branch","text":"<p>Every Git repository has the main branch (unless you take explicit steps to remove it) and it plays a key role in the software development process. In most projects, it represents the source of truth - all the code that works has been tested and is ready to be pushed to production. Similarly, main branch in lakeFS could be used as the single source of truth. For example, the live production data can be on the main branch. </p>"},{"location":"understand/glossary/#metadata-management","title":"Metadata Management","text":"<p>Where there is data, there is also metadata. lakeFS uses metadata to define schema, data types, data versions, relations to other datasets, etc. This helps to improve discoverability and manageability. lakeFS performs data versioning through metadata operations. </p>"},{"location":"understand/glossary/#merge","title":"Merge","text":"<p>lakeFS merge command, similar to the Git merge functionality, allows you to merge data branches. Once you commit data, you can review it and then merge the committed data into the target branch. A merge generates a commit on the target branch with all your changes. lakeFS guarantees atomic merges that are fast, given they don\u2019t involve copying data. Read More.</p>"},{"location":"understand/glossary/#object-metadata","title":"Object Metadata","text":"<p>In lakeFS, each object can have two types of metadata:</p> <ul> <li>System metadata: Automatically captured attributes such as object path, size, last modified time, and the committer who made the change.</li> <li>User-defined metadata: Custom key-value pairs (e.g., labels, annotations, tags) added during data ingestion, processing, or curation to enrich context.</li> </ul> <p>Like data itself, object metadata is versioned in lakeFS. This means metadata evolves alongside your data and can be easily managed, queried, and reproduced.</p>"},{"location":"understand/glossary/#repository","title":"Repository","text":"<p>In lakeFS, a repository is a set of related objects (or collections of objects). Read More.</p>"},{"location":"understand/glossary/#rollback","title":"Rollback","text":"<p>A rollback is an atomic operation reversing the effects of a previous commit. If a developer introduces a new code version to production and discovers that it has a critical bug, they can simply roll back to the previous version. In lakeFS, a rollback is an atomic action that prevents the data consumers from receiving low-quality data until the issue is resolved. Learn more about how lakeFS supports the rollback operation.</p>"},{"location":"understand/glossary/#storage-namespace","title":"Storage Namespace","text":"<p>The storage namespace is a location in the underlying storage dedicated to a specific repository. lakeFS uses it to store the repository's objects and some of its metadata.</p>"},{"location":"understand/glossary/#underlying-storage","title":"Underlying Storage","text":"<p>The underlying storage is a location in some object store where lakeFS keeps your objects and some metadata.</p>"},{"location":"understand/glossary/#tag","title":"Tag","text":"<p>Tags are a way to give a meaningful name to a specific commit. Read More.</p>"},{"location":"understand/glossary/#fluffy","title":"Fluffy","text":"<p>lakeFS Enterprise Single-Sign-On service, it's delegated with lakeFS authentication requests and replies back to lakeFS with the authentication response.</p>"},{"location":"understand/model/","title":"lakeFS Concepts and Model","text":"<p>lakeFS blends concepts from object stores such as S3 with concepts from Git. This reference defines the common concepts of lakeFS.</p>"},{"location":"understand/model/#objects","title":"Objects","text":"<p>lakeFS is an interface to manage objects in an object store.</p> <p>Tip</p> <p>The actual data itself is not stored inside lakeFS directly but in an underlying object store. lakeFS manages pointers and additional metadata about these objects.</p>"},{"location":"understand/model/#version-control","title":"Version Control","text":"<p>lakeFS is spearheading version control semantics for data. Most of these concepts will be familiar to Git users:</p>"},{"location":"understand/model/#repository","title":"Repository","text":"<p>In lakeFS, a repository is a set of related objects (or collections of objects). In many cases, these represent tables of various formats for tabular data, semi-structured data such as JSON or log files - or a set of unstructured objects such as images, videos, sensor data, etc.</p> <p>lakeFS represents repositories as a logical namespace used to group together objects, branches, and commits - analogous to a repository in Git.</p> <p>lakeFS repository naming requirements are as follows:</p> <ul> <li>Start with a lower case letter or number</li> <li>Contain only lower case letters, numbers and hyphens</li> <li>Be between 3 and 63 characters long</li> </ul>"},{"location":"understand/model/#commits","title":"Commits","text":"<p>Using commits, you can view a repository at a certain point in its history and you're guaranteed that the data you see is exactly as it was at the point of committing it.</p> <p>These commits are immutable \"checkpoints\" containing all contents of a repository at a given point in the repository's history.</p> <p>Each commit contains metadata - the committer, timestamp, a commit message, as well as arbitrary key/value pairs you can choose to add.</p> <p>Identifying Commits</p> <p>A commit is identified by its commit ID, a digest of all contents of the commit.  Commit IDs are by nature long, so you may use a unique prefix to abbreviate them. A commit may also be identified by using a textual definition, called a ref.  Examples of refs include tags, branch names, and expressions.</p>"},{"location":"understand/model/#branches","title":"Branches","text":"<p>Branches in lakeFS allow users to create their own \"isolated\" view of the repository.</p> <p>Changes on one branch do not appear on other branches. Users can take changes from one branch and apply it to another by merging them.</p>"},{"location":"understand/model/#zero-copy-branching","title":"Zero-copy branching","text":"<p>Under the hood, branches are simply a pointer to a commit along with a set of uncommitted changes. Creating a branch is a zero-copy operation; instead of duplicating data, it involves creating a pointer to the source commit for the branch.</p>"},{"location":"understand/model/#tags","title":"Tags","text":"<p>Tags are a way to give a meaningful name to a specific commit. Using tags allow users to reference specific releases, experiments, or versions by using a human friendly name.</p> <p>Example tags:</p> <ul> <li><code>v2.3</code> to mark a release.</li> <li><code>dev-jane-before-v2.3-merge</code> to mark Jane's private temporary point.</li> </ul> <p>Tag names adhere to the same rules as git ref names.</p>"},{"location":"understand/model/#history","title":"History","text":"<p>The history of the branch is the list of commits from the branch tip through the first parent of each commit. Histories go back in time.</p>"},{"location":"understand/model/#merge","title":"Merge","text":"<p>Merging is the way to integrate changes from a branch into another branch. The result of a merge is a new commit, with the destination as the first parent and the source as the second.</p> <p>Info</p> <p>To learn more about how merging works in lakeFS, see the merge reference</p>"},{"location":"understand/model/#ref-expressions","title":"Ref expressions","text":"<p>lakeFS also supports expressions for creating a ref. These are similar to revisions in Git; indeed all <code>~</code> and <code>^</code> examples at the end of that section will work unchanged in lakeFS.</p> <ul> <li>A branch or a tag are ref expressions.</li> <li>If <code>&lt;ref&gt;</code> is a ref expression, then:</li> <li><code>&lt;ref&gt;^</code> is a ref expression referring to its first parent.</li> <li><code>&lt;ref&gt;^N</code> is a ref expression referring to its N'th parent; in particular <code>&lt;ref&gt;^1</code> is the     same as <code>&lt;ref&gt;^</code>.</li> <li><code>&lt;ref&gt;~</code> is a ref expression referring to its first parent; in particular <code>&lt;ref&gt;~</code> is the     same as <code>&lt;ref&gt;^</code> and <code>&lt;ref&gt;~</code>.</li> <li><code>&lt;ref&gt;~N</code> is a ref expression referring to its N'th parent, always traversing to the first     parent.  So <code>&lt;ref&gt;~N</code> is the same as <code>&lt;ref&gt;^^...^</code> with N consecutive carets <code>^</code>.</li> </ul>"},{"location":"understand/model/#concepts-unique-to-lakefs","title":"Concepts unique to lakeFS","text":"<p>The underlying storage is a location in an object store where lakeFS keeps your objects and some immutable metadata.</p> <p>When creating a lakeFS repository, you assign it with a storage namespace. The repository's storage namespace is a location in the underlying storage where data for this repository will be stored.</p> <p>We sometimes refer to underlying storage as physical. The path used to store the contents of an object is then termed a physical path. Once lakeFS saves an object in the underlying storage it is never modified, except to remove it entirely during some cleanups.</p> <p>A lot of what lakeFS does is to manage how lakeFS paths translate to physical paths on the object store. This mapping is generally not straightforward. Importantly (and contrary to many object stores), lakeFS may map multiple paths to the same object on backing storage, and always does this for objects that are unchanged across versions.</p>"},{"location":"understand/model/#lakefs-protocol-uris","title":"<code>lakefs</code> protocol URIs","text":"<p>lakeFS uses a specific format for path URIs. The URI <code>lakefs://&lt;REPO&gt;/&lt;REF&gt;/&lt;KEY&gt;</code> is a path to objects in the given repo and ref expression under key. </p> <p>This is used both for path prefixes and for full paths.  In similar fashion, <code>lakefs://&lt;REPO&gt;/&lt;REF&gt;</code> identifies the repository at a ref expression, and <code>lakefs://&lt;REPO&gt;</code> identifies a repo.</p>"},{"location":"understand/performance-best-practices/","title":"Performance Best Practices","text":""},{"location":"understand/performance-best-practices/#overview","title":"Overview","text":"<p>Use this guide to achieve the best performance with lakeFS.</p>"},{"location":"understand/performance-best-practices/#avoid-concurrent-commitsmerges","title":"Avoid concurrent commits/merges","text":"<p>Just like in Git, branch history is composed by commits and is linear by nature. Concurrent commits/merges on the same branch result in a race. The first operation will finish successfully while the rest will retry.</p>"},{"location":"understand/performance-best-practices/#perform-meaningful-commits","title":"Perform meaningful commits","text":"<p>It's a good idea to perform commits that are meaningful in the senese that they represent a logical point in your data's lifecycle. While lakeFS supports arbirartily large commits, avoiding commits with a huge number of objects will result in a more comprehensible commit history.</p>"},{"location":"understand/performance-best-practices/#use-zero-copy-import","title":"Use zero-copy import","text":"<p>To import object into lakeFS, either a single time or regularly, lakeFS offers a [zero-copy import][use-zero-copy-import] feature. Use this feature to import a large number of objects to lakeFS, instead of simply copying them into your repository. This feature will create a reference to the existing objects on your bucket and avoids the copy.</p>"},{"location":"understand/performance-best-practices/#read-data-using-the-commit-id","title":"Read data using the commit ID","text":"<p>In cases where you are only interested in reading committed data:</p> <ul> <li>Use a commit ID (or a tag ID) in your path (e.g: <code>lakefs://repo/a1b2c3</code>).</li> <li>Add <code>@</code> before the path  <code>lakefs://repo/main@/path</code>.</li> </ul> <p>When accessing data using the branch name (e.g. <code>lakefs://repo/main/path</code>) lakeFS will also try to fetch uncommitted data, which may result in reduced performance. For more information, see how uncommitted data is managed in lakeFS.</p>"},{"location":"understand/performance-best-practices/#operate-directly-on-the-storage","title":"Operate directly on the storage","text":"<p>Sometimes, storage operations can become a bottleneck. For example, when your data pipelines upload many big objects. In such cases, it can be beneficial to perform only versioning operations on lakeFS, while performing storage reads/writes directly on the object store. lakeFS offers multiple ways to do that:</p> <ul> <li>The <code>lakectl fs upload --pre-sign</code> command (or download).</li> <li>The lakeFS Hadoop Filesystem.</li> <li>The staging API which can be used to add lakeFS references to objects after having written them to the storage.</li> </ul> <p>Accessing the object store directly is a faster way to interact with your data.</p>"},{"location":"understand/performance-best-practices/#zero-copy","title":"Zero-copy","text":"<p>lakeFS provides a zero-copy mechanism to data. Instead of copying the data, we can check out to a new branch. Creating a new branch will take constant time as the new branch points to the same data as its parent. It will also lower the storage cost.</p>"},{"location":"understand/how/kv/","title":"Internal database structure","text":"<p>Starting at version 0.80.2, lakeFS abandoned the tight coupling to PostgreSQL and moved all database operations to work over Key-Value Store</p> <p>While SQL databases, and Postgres among them, have their obvious advantages, we felt that the tight coupling to Postgres is limiting our users and so, lakeFS with Key Value Store is introduced.</p> <p>Our KV Store implements a generic interface, with methods for Get, Set, Compare-and-Set, Delete and Scan. Each entry is represented by a [partition, key, value] triplet. All these fields are generic byte-array, and the using module has maximal flexibility on the format to use for each field</p> <p>Under the hood, our KV implementation relies on a backing DB, which persists the data. Theoretically, it could be any type of database and out of the box, we already implemented drivers for DynamoDB, for AWS users, and PostgreSQL, using its relational nature to store a KV Store. </p> <p>More databases will be supported in the future, and lakeFS users and contributors can develop their own driver to use their own favorite database. For experimenting purposes, an in-memory KV store can be used, though it obviously lack the persistency aspect</p> <p>In order to store its metadata objects (that is Repositories, Branches, Commits, Tags, and Uncommitted Objects), lakeFS implements another layer over the generic KV Store, which supports serialization and deserialization of these objects as protobuf. </p> <p>As this layer relies on the generic interface of the KV Store layer, it is totally agnostic to whichever store implementation is in use, gaining our users the maximal flexibility</p> <p>Further Reading</p> <p>For a deeper explanation, see to our KV Design</p>"},{"location":"understand/how/kv/#optimistic-locking-with-kv","title":"Optimistic Locking with KV","text":"<p>One important key difference between SQL databases and Key Value Store is the ability to lock resources. </p> <p>While this is a common practice with relational databases, Key Value stores not always support this ability.  When designing our KV Store, we tried to support the most simplistic straight-forward interface, with flexibility in backing DB selection, and so, we decided not to support locking. </p> <p>This decision brought some concurrency challenges we had to overcome. Let us take a look at a common lakeFS flow, Commit, during which several database operations are performed:</p> <ul> <li>All relevant (Branch correlated) uncommitted objects are collected and marked as committed </li> <li>A new Commit object is created</li> <li>The relevant Branch is updated to point to the new commit</li> </ul> <p>The Commit flow includes multiple database accesses and modifications, and is very sensitive to concurrent executions: If 2 Commit flows run in parallel, we must guarantee correctness of the data.</p> <p>lakeFS with PostgreSQL simply locks the Branch for the entire Commit operation, preventing concurrent execution of such flows.</p> <p>Now, with KV Store replacing the SQL database, this easy solution is no longer available. Instead, we implemented an Optimistic Locking algorithm, which leverages the KV Store Compare-And-Set (CAS) functionality to remember the Branch state at the beginning of the Commit flow, and updating the branch at the end, only if it remains unchanged, using CAS, with the former Branch state, used as a comparison criteria. </p> <p>If the sampled Branch state and the current state differ, it could only mean that another, later, Commit is in progress, causing the first Commit to fail, and give the later Commit a chance to complete.</p> <p>Here's a running example:</p> <ul> <li>Commit A sets the StagingToken to tokenA and samples the Branch,</li> <li>Commit B sets the StagingToken to tokenB and samples the Branch,</li> <li>Commit A finishes, tries to update the Branch and fails due to the recent modification by Commit B - the StagingToken is set to tokenB and not tokenA as expected by Commit A,</li> <li>Commit B finishes and updates the branch, as tokenB is set as StagingToken and it matches the flow expectation</li> </ul> <p>An important detail to note, is that as a Commit starts, and the StagingToken is set a new value, the former value is added to a list of 'still valid' StagingToken_s - _SealedToken - on the Branch, which makes sure no StagingToken and no object are lost due to a failed Commit</p> <p>Further Reading</p> <p>You can read more on the Commit Flow in the dedicated section in the KV Design</p>"},{"location":"understand/how/kv/#db-transactions-and-atomic-updates","title":"DB Transactions and Atomic Updates","text":"<p>Another notable difference is the existence of DB transactions with PostgreSQL, ability that our KV Store lacks.</p> <p>This ability was leveraged by lakeFS to construct several DB updates, into one \"atomic\" operation - each failure, in each step, rolled back the entire operation, keeping the DB consistent and clean. With KV Store, this ability is gone, and we had to come up with various solutions. As a starting point, the DB consistency is, obviously, not anything we can risk. On the other hand, maintaining the DB clean, and as a result smaller, is something that can be sacrificed, at least as a first step. Let us take a look at a relatively simple flow of a new Repository creation:</p> <p>A brand new Repository has 3 objects: The Repository object itself, an initial Branch object and an initial Commit, which the Branch points to. </p> <p>With SQL DB, it was as simple as creating all 3 objects in the DB under one transaction (at this order). Any failure resulted in a rollback and no redundant leftovers in our DB. With no transaction in KV Store, if for example the Branch creation fails, it will leave the Repository without an initial Branch (or a Branch at all), yet the Repository will be accessible.  Trying to delete the Repository as a response to Branch creation failure is ony a partial solution as this operation can fail as well.</p> <p>To mitigate this we introduced a per-Repository-partition, which holds all repository related objects (the Branch and Commit in this scenario).  The partition key can only be derived from the specific Repository instance itself. In addition we first create the Repository objects, the Commit and the Branch, under the Repository's partition key, and then the Repository is created.  The Repository and its objects will be accessible only after a successful creation of all 3 entities. </p> <p>A failure in this flow might leave some dangling objects, but consistency is maintained.</p> <p>The number of such dangling objects is not expected to be significant, and we plan to implement a cleaning algorithm to keep our KV Store neat and clean</p>"},{"location":"understand/how/kv/#so-which-approach-is-better","title":"So, Which Approach is Better?","text":"<p>This documents provides a peek into our new database approach - Key Value Store instead of a Relational SQL. It discusses the challenges we faced, and the solutions we provided to overcome these challenges. Considering the fact that lakeFS over with relational database did work, you might ask yourself why did we bother to develop another solution. The simple answer, is that while PostgreSQL was not a bad option, it was the only option, and any drawback of PostgreSQL, reflected on our users:</p> <ul> <li>PostgreSQL can only scale vertically and that is a limitation. At some point this might not hold.</li> <li>PostgreSQL is not a managed solution, meaning that users had to take care of all maintenance tasks, including the above mentioned scale (when needed)</li> <li>As an unmanaged database, scaling means downtime - is that acceptable?</li> <li>It might even get to the point that your organization is not willing to work with PostgreSQL due to various business considerations</li> </ul> <p>If none of the above apply, and you have no seemingly reason to switch from PostgreSQL, it can definitely still be used as an excellent option for the backing database for the lakeFS KV Store. If you do need another solution, you have DynamoDB support, out of the box. DynamoDB, as a fully managed solution, with horizontal scalability support and optimized partitions support, answers all the pain-points specified above. It is definitely an option to consider, if you need to overcome these And, of course, you can always decide to implement your own KV Store driver to use your database of choice - we would love to add your contribution to lakeFS</p>"},{"location":"understand/how/merge/","title":"Merges in lakeFS","text":"<p>The merge operation in lakeFS is similar to Git. It incorporates changes from a merge source (a commit/reference) into a merge destination (a branch). </p>"},{"location":"understand/how/merge/#how-does-it-work","title":"How does it work?","text":"<p>lakeFS first finds the merge base: the nearest common ancestor of the two commits. It can now perform a three-way merge, by examining the presence and identity of files in each commit. In the table below, \"A\", \"B\" and \"C\" are possible file contents, \"X\" is a missing file, and \"conflict\" (which only appears as a result) is a merge failure.</p> In base In source In destination Result Comment A A A A Unchanged file A B B B Files changed on both sides in same way A B C conflict Files changed on both sides differently A A B B File changed only on one branch A B A B File changed only on one branch A X X X Files deleted on both sides A B X conflict File changed on one side, deleted on the other A X B conflict File changed on one side, deleted on the other A A X X File deleted on one side A X A X File deleted on one side"},{"location":"understand/how/merge/#merge-strategies","title":"Merge Strategies","text":"<p>The API and <code>lakectl</code> allow passing an optional <code>strategy</code> flag with the following values:</p>"},{"location":"understand/how/merge/#source-wins","title":"<code>source-wins</code>","text":"<p>In case of a conflict, merge will pick the source objects.</p> <p>Example</p> <pre><code>lakectl merge lakefs://example-repo/validated-data lakefs://example-repo/production --strategy source-wins\n</code></pre> <p>When a merge conflict arises, the conflicting objects in the <code>validated-data</code> branch will be chosen to end up in <code>production</code>.</p>"},{"location":"understand/how/merge/#dest-wins","title":"<code>dest-wins</code>","text":"<p>In case of a conflict, merge will pick the destination objects.</p> <p>Example</p> <pre><code>lakectl merge lakefs://example-repo/validated-data lakefs://example-repo/production --strategy dest-wins\n</code></pre> <p>When a merge conflict arises, the conflicting objects in the <code>production</code> branch will be chosen to end up in <code>validated-data</code>. The <code>production</code> branch will not be affected by object changes from <code>validated-data</code> conflicting objects.</p> <p>The strategy will affect all conflicting objects in the merge if it is set. Currently it is not possible to treat conflicts individually.</p> <p>As a format-agnostic system, lakeFS currently merges by complete files. Format-specific and other user-defined merge strategies for handling conflicts are on the roadmap.</p>"},{"location":"understand/how/versioning-internals/","title":"Versioning Internals","text":""},{"location":"understand/how/versioning-internals/#overview","title":"Overview","text":"<p>Since commits in lakeFS are immutable, they are easy to store on an immutable object store.</p> <p>Older commits are rarely accessed, while newer commits are accessed very frequently, a tiered storage approach can work very well - the object store is the source of truth, while local disk and even RAM can be used to cache the more frequently accessed ones.</p> <p>Since they are immutable - once cached, you only need to evict them when space is running out. There\u2019s no complex invalidation that needs to happen.</p> <p>In terms of storage format, commits are be stored as SSTables, compatible with RocksDB.</p> <p>SSTables were chosen as a storage format for 3 major reasons:</p> <ol> <li>Extremely high read throughput on modern hardware: using commits representing a 200m object repository (modeled after the S3 inventory of one of our design partners), we were able to achieve close to 500k random GetObject calls / second. This provides a very high throughput/cost ratio, probably as high as can be achieved on public clouds.</li> <li>Being a known storage format means it\u2019s relatively easy to generate and consume. Storing it in the object store makes it accessible to data engineering tools for analysis and distributed computation, effectively reducing the silo effect of storing it in an operational database.</li> <li>The SSTable format supports delta encoding for keys which makes them very space efficient for data lakes where many keys share the same common prefixes.</li> </ol> <p>Each lakeFS commit is represented as a set of contiguous, non-overlapping SSTables that make up the entire keyspace of a repository at that commit.</p>"},{"location":"understand/how/versioning-internals/#sstable-file-format-graveler-file","title":"SSTable File Format (\"Graveler File\")","text":"<p>lakeFS metadata is encoded into a format called \"Graveler\" - a standardized way to encode content-addressable key value pairs. This is what a Graveler file looks like:</p> <p></p> <p>Each Key/Value pair (\"ValueRecord\") is constructed of a <code>key</code>, <code>identity</code>, and <code>value</code>.</p> <p>A simple identity could be, for example, a sha256 hash of the value\u2019s bytes. It could be any sequence of bytes that uniquely identifies the value. As far as the Graveler is concerned, two <code>ValueRecord</code>s are considered identical if their key and identity fields are equal.</p> <p>A Graveler file itself is content-addressable, i.e., similarly to Git, the name of the file is its identity. File identity is calculated based on the identity of the ValueRecords the file contains:</p> <p>valueRecordID = h(h(valueRecord.key) || h(valueRecord.Identity)) fileID = h(valueRecordID<sub>1</sub> + \u2026 + valueRecordID<sub>N</sub>)</p>"},{"location":"understand/how/versioning-internals/#constructing-a-consistent-view-of-the-keyspace-ie-a-commit","title":"Constructing a consistent view of the keyspace (i.e., a commit)","text":"<p>We have two additional requirements for the storage format:</p> <ol> <li>Be space and time efficient when creating a commit - assuming a commit changes a single object out of a billion, we don\u2019t want to write a full snapshot of the entire repository. Ideally, we\u2019ll be able to reuse some data files that haven\u2019t changed to make the commit operations (in both space and time) proportional to the size of the difference as opposed to the total size of the repository.</li> <li>Allow an efficient diff between commits which runs in time proportional to the size of their difference and not their absolute sizes.</li> </ol> <p>To support these requirements, we decided to essentially build a 2-layer Merkle tree composed of a set of leaf nodes (\"Range\") addressed by their content address, and a \"Meta Range\", which is a special range containing all ranges, thus representing an entire consistent view of the keyspace:</p> <p></p> <p>Assuming commit B is derived from commit A, and only changed files in range <code>e-f</code>, it can reuse all ranges except for SSTable #N (the one containing the modified range of keys), which will be recreated with a new hash representing the state as exists after applying commit B\u2019s changes. This will, in turn, also create a new Metarange since its hash is now changed as well (as it is derived from the hash of all contained ranges).</p> <p>Assuming most commits usually change related objects (i.e., that are likely to share some common prefix), the reuse ratio could be very high. We tested this assumption using S3 inventory from 2 design partners - we partitioned the keyspace to an arbitrary number of simulated blocks and measured their change over time. We saw a daily change rate of about 5-20%.</p> <p>Given the size of the repositories, it's safe to assume that a single day would translate into multiple commits. At a modest 20 commits per day, a commit is expected to reuse &gt;= 99% of the previous commit blocks, so acceptable in terms of write amplification generated on commit.</p> <p>On the object store, ranges are stored in the following hierarchy:</p> <pre><code>&lt;lakefs root&gt;\n    _lakefs/\n        &lt;range hash1&gt;\n        &lt;range hash2&gt;\n        &lt;range hashN&gt;\n        ...\n        &lt;metarange hash1&gt;\n        &lt;metarange hash2&gt;\n        &lt;metarange hashN&gt;\n        ...\n    &lt;data object hash1&gt;\n    &lt;data object hash2&gt;\n    &lt;data object hashN&gt;\n    ...\n</code></pre> <p>Note: This relatively flat structure could be modified in the future. Looking at the diagram above, it imposes no real limitations on the depth of the tree. A tree could easily be made recursive by having Meta Ranges point to other Meta Ranges - and still provide all the same characteristics. For simplicity, we decided to start with a fixed 2-level hierarchy.</p>"},{"location":"understand/how/versioning-internals/#representing-references-and-uncommitted-metadata","title":"Representing references and uncommitted metadata","text":"<p>lakeFS always stores the object data in the storage namespace in the user's object store, committed and uncommitted data alike.</p> <p>However, the lakeFS object metadata might be stored in either the object store or a key-value store.</p> <p>Unlike committed metadata which is immutable, uncommitted (or \"staged\") metadata experiences frequent random writes and is very mutable in nature. This is also true for \"refs\" - in particular, branches, which are simply pointers to an underlying commit, are modified frequently: on every commit or merge operation.</p> <p>Both these types of metadata are not only mutable, but also require strong consistency guarantees while also being fault tolerant. If we can\u2019t access the current pointer of the main branch, a big portion of the system is essentially down. </p> <p>Luckily, this is also much smaller set of metadata compared to the committed metadata.</p> <p>References and uncommitted metadata are currently stored on a key-value store (See supported databases) for consistency guarantees.</p>"},{"location":"understand/use_cases/cicd_for_data/","title":"CI/CD for Data","text":""},{"location":"understand/use_cases/cicd_for_data/#why-do-i-need-cicd","title":"Why do I need CI/CD?","text":"<p>Data pipelines feed processed data from data lakes to downstream consumers like business dashboards and machine learning models. As more and more organizations rely on data to enable business critical decisions, data reliability and trust are of paramount concern. Thus, it's important to ensure that production data adheres to the data governance policies of businesses. These data governance requirements can be as simple as a file format validation, schema check, or an exhaustive PII(Personally Identifiable Information) data removal from all of organization's data. </p> <p>Thus, to ensure the quality and reliability at each stage of the data lifecycle, data quality gates need to be implemented. That is, we need to run Continuous Integration(CI) tests on the data, and only if data governance requirements are met can the data can be promoted to production for business use. </p> <p>Everytime there is an update to production data, the best practice would be to run CI tests and then promote(deploy) the data to production. </p>"},{"location":"understand/use_cases/cicd_for_data/#how-do-i-implement-cicd-for-data-with-lakefs","title":"How do I implement CI/CD for data with lakeFS?","text":"<p>lakeFS makes implementing CI/CD pipelines for data simpler. lakeFS provides a feature called hooks that allow automation of checks and validations of data on lakeFS branches. These checks can be triggered by certain data operations like committing, merging, etc. </p> <p>Functionally, lakeFS hooks are similar to Git Hooks. lakeFS hooks are run remotely on a server, and they are guaranteed to run when the appropriate event is triggered.</p> <p>Here are some examples of the hooks lakeFS supports: * pre-merge * pre-commit * post-merge * post-commit * pre-create-branch * post-create-branch</p> <p>and so on.</p> <p>By leveraging the pre-commit and pre-merge hooks with lakeFS, you can implement CI/CD pipelines on your data lakes.</p> <p>Specific trigger rules, quality checks and the branch on which the rules are to be applied are declared in <code>actions.yaml</code> file. When a specific event (say, pre-merge) occurs, lakeFS runs all the validations declared in <code>actions.yaml</code> file. If validations error out, the merge event is blocked.</p> <p>Here is a sample <code>actions.yaml</code> file that has pre-merge hook configured to allow only parquet and delta lake file formats on main branch.</p> <pre><code>name: ParquetOnlyInProduction\ndescription: This webhook ensures that only parquet files are written under production/\non:\n  pre-merge:\n    branches:\n      - main\nhooks:\n  - id: production_format_validator\n    type: webhook\n    description: Validate file formats\n    properties:\n      url: \"http://lakefs-hooks:5001/webhooks/format\"\n      query_params:\n        allow: [\"parquet\", \"delta_lake\"]\n        prefix: analytics/\n</code></pre>"},{"location":"understand/use_cases/cicd_for_data/#using-hooks-as-data-quality-gates","title":"Using hooks as data quality gates","text":"<p>Hooks are run on a remote server that can serve http requests from lakeFS server. lakeFS supports two types of hooks. 1. webhooks (run remotely on a web server. e.g.: flask server in python)  2. airflow hooks (a dag of complex data quality checks/tasks that can be run on airflow server) </p> <p>In this tutorial, we will show how to use webhooks (python flask webserver) to implement quality gates on your data branches. Specifically, how to configure hooks to allow only parquet and delta lake format files in the main branch.</p> <p>The tutorial provides a lakeFS environment, python flask server, a Jupyter notebook and sample data sets to demonstrate the integration of lakeFS hooks with Apache Spark and Python. It runs on Docker Compose.</p> <p>To understand how hooks work and how to configure hooks in your production system, refer to the documentation: Hooks. </p> <p></p> <p>Follow the steps below to try out CI/CD for data lakes.</p>"},{"location":"understand/use_cases/cicd_for_data/#implementing-cicd-pipeline-with-lakefs-demo","title":"Implementing CI/CD pipeline with lakeFS - Demo","text":"<p>The sample below provides a lakeFS environment, a Jupyter notebook, and a server on which for the lakeFS webhooks to run. </p>"},{"location":"understand/use_cases/cicd_for_data/#prerequisites-setup","title":"Prerequisites &amp; Setup","text":"<p>Before we get started, make sure Docker is installed on your machine.</p> <ul> <li>Start by cloning the lakeFS samples Git repository:</li> </ul> <pre><code>git clone https://github.com/treeverse/lakeFS-samples.git\ncd lakeFS-samples\n</code></pre> <ul> <li>Run following commands to start the components: </li> </ul> <pre><code>git submodule init\ngit submodule update\ndocker compose up\n</code></pre> <p>Open the local Jupyter Notebook and go to the <code>hooks-demo.ipynb</code> notebook.</p>"},{"location":"understand/use_cases/cicd_for_data/#resources","title":"Resources","text":"<p>To explore different checks and validations on your data, refer to pre-built hooks config by the lakeFS team. </p> <p>To understand the comprehensive list of hooks supported by lakeFS, refer to the documentation.</p>"},{"location":"understand/use_cases/etl_testing/","title":"ETL Testing with Isolated Dev/Test Environments","text":""},{"location":"understand/use_cases/etl_testing/#why-are-multiple-environments-so-important","title":"Why are multiple environments so important?","text":"<p>When working with a data lake, it's useful to have replicas of your production environment. These replicas allow you to test these ETLs and understand changes to your data without impacting the consumers of the production data.</p> <p>Running ETL and transformation jobs directly in production without proper ETL testing presents a huge risk of having data issues flow into dashboards, ML models, and other consumers sooner or later.</p> <p>The most common approach to avoid making changes directly in production is to create and maintain multiple data environments and perform ETL testing on them. Dev environments give you a space in which to develop the data pipelines and test environment where pipeline changes are tested before pushing it to production.</p> <p>Without lakeFS, the challenge with this approach is that it can be time-consuming and costly to maintain these separate dev/test environments to enable thorough effective ETL testing. And for larger teams it forces multiple people to share these environments, requiring significant coordination. Depending on the size of the data involved there can also be high costs due to the duplication of data.</p>"},{"location":"understand/use_cases/etl_testing/#how-does-lakefs-help-with-devtest-environments","title":"How does lakeFS help with Dev/Test environments?","text":"<p>lakeFS makes creating isolated dev/test environments for ETL testing quick and cheap. lakeFS uses zero-copy branching which means that there is no duplication of data when you create a new environment. This frees you from spending time on environment maintenance and makes it possible to create as many environments as needed.</p> <p>In a lakeFS repository, data is always located on a <code>branch</code>. You can think of each <code>branch</code> in lakeFS as its own environment. This is because branches are isolated, meaning changes on one branch have no effect other branches.</p> <p>Info</p> <p>Objects that remain unchanged between two branches are not copied, but rather shared to both branches via metadata pointers that lakeFS manages.</p> <p>If you make a change on one branch and want it reflected on another, you can perform a <code>merge</code> operation to update one branch with the changes from another.</p>"},{"location":"understand/use_cases/etl_testing/#using-branches-as-development-and-testing-environments","title":"Using branches as development and testing environments","text":"<p>The key difference when using lakeFS for isolated data environments is that you can create them immediately before testing a change. And once new data is merged into production, you can delete the branch - effectively deleting the old environment.</p> <p>This is different from creating a long-living test environment used as a staging area to test all the updates. With lakeFS, we create a new branch for each change to production that we want to make. One benefit of this is the ability to test multiple changes at one time.</p> <p></p>"},{"location":"understand/use_cases/etl_testing/#try-it-out-creating-devtest-environments-with-lakefs-for-etl-testing","title":"Try it out! Creating Dev/Test Environments with lakeFS for ETL Testing","text":"<p>lakeFS supports UI, CLI (<code>lakectl</code> command-line utility) and several clients for the API to run the Git-like operations. Let us explore how to create dev/test environments using each of these options below.</p> <p>There are two ways that you can try out lakeFS:</p> <ul> <li>The lakeFS Playground on lakeFS Cloud - fully managed lakeFS with a 30-day free trial</li> <li>Local Docker-based quickstart and samples</li> </ul> <p>You can also deploy lakeFS locally or self-managed on your cloud of choice.</p>"},{"location":"understand/use_cases/etl_testing/#using-lakefs-playground-on-lakefs-cloud","title":"Using lakeFS Playground on lakeFS Cloud","text":"<p>In this tutorial, we will use a lakeFS playground environment to create dev/test data environments for ETL testing. This allows you to spin up a lakeFS instance in a click, create different data environments by simply branching out of your data repository and develop &amp; test data pipelines in these isolated branches.</p> <p>First, let us spin up a playground instance. Once you have a live environment, login to your instance with access and secret keys. Then, you can work with the sample data repository <code>my-repo</code> that is created for you.</p> <p></p> <p>Click on <code>my-repo</code> and notice that by default, the repository has a <code>main</code> branch created and <code>sample_data</code> preloaded to work with.</p> <p></p> <p>You can create a new branch (say, <code>test-env</code>) by going to the Branches tab and clicking Create Branch. Once it is successful, you will see two branches under the repository: <code>main</code> and <code>test-env</code>.</p> <p></p> <p>Now you can add, modify or delete objects under the <code>test-env</code> branch without affecting the data in the main branch.</p>"},{"location":"understand/use_cases/etl_testing/#trying-out-lakefs-with-docker-and-jupyter-notebooks","title":"Trying out lakeFS with Docker and Jupyter Notebooks","text":"<p>This use case shows how to create dev/test data environments for ETL testing using lakeFS branches. The following tutorial provides a lakeFS environment, a Jupyter notebook, and Python SDK API to demonstrate integration of lakeFS with Spark. You can run this tutorial on your local machine.</p> <p>Follow the tutorial video below to get started with the playground and Jupyter notebook, or follow the instructions on this page.</p>"},{"location":"understand/use_cases/etl_testing/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, you will need Docker installed on your machine.</p>"},{"location":"understand/use_cases/etl_testing/#running-lakefs-and-jupyter-notebooks","title":"Running lakeFS and Jupyter Notebooks","text":"<p>Follow along the steps below to create dev/test environment with lakeFS.</p> <ul> <li> <p>Start by cloning the lakeFS samples Git repository:</p> <pre><code>git clone https://github.com/treeverse/lakeFS-samples.git\ncd lakeFS-samples\n</code></pre> </li> <li> <p>Run following commands to download and run Docker container which includes Python, Spark, Jupyter Notebook, JDK, Hadoop binaries, lakeFS Python SDK and Airflow (Docker image size is around 4.5GB):</p> <pre><code>git submodule init &amp;&amp; git submodule update\ndocker compose up\n</code></pre> </li> <li> <p>Open the local Jupyter Notebook and go to the <code>spark-demo.ipynb</code> notebook.</p> </li> </ul>"},{"location":"understand/use_cases/etl_testing/#configuring-lakefs-python-client","title":"Configuring lakeFS Python Client","text":"<p>Setup lakeFS access credentials for the lakeFS instance running. The defaults for these that the samples repository Docker Compose uses are shown here:</p> <pre><code>lakefs_access_key = 'AKIAIOSFODNN7EXAMPLE'\nlakefs_secret_key = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\nlakefs_endpoint = 'http://lakefs:8000'\n</code></pre> <p>Next, setup the storage namespace to a location in the bucket you have configured. The storage namespace is a location in the underlying storage where data for this repository will be stored.</p> <pre><code>storageNamespace = 's3://example/' \n</code></pre> <p>You can use lakeFS through the UI, API or <code>lakectl</code> command-line. For this use-case, we use python <code>lakefs</code> to run lakeFS core operations.</p> <pre><code>import lakefs\nfrom lakefs import Client\n\n# lakeFS credentials and endpoint\nclient = Client(\n    host=lakefs_endpoint,\n    username=lakefs_access_key,\n    password=lakefs_secret_key\n)\n</code></pre> <p>lakeFS can be configured to work with Spark in two ways:</p> <ul> <li>Access lakeFS using the S3-compatible API</li> <li>Access lakeFS using the lakeFS-specific Hadoop FileSystem</li> </ul>"},{"location":"understand/use_cases/etl_testing/#upload-the-sample-data-to-main-branch","title":"Upload the Sample Data to Main Branch","text":"<p>To upload an object to the <code>my-repo</code>, use the following command.</p> <pre><code>import os\nimport lakefs\n\nwith open('/data/lakefs_test.csv', 'rb') as f:\n    lakefs.repository(\"my-repo\", client=client).branch(\"main\").object(filenName).upload(data=f.read())\n</code></pre> <p>Once uploaded, commit the changes to the <code>main</code> branch and attach some metadata to the commit as well.</p> <pre><code>lakefs.repository(\"my-repo\", client=client).branch(\"main\").commit(message=\"Added my first object!\", metadata={'using': 'python'})\n</code></pre> <p>In this example, we use lakeFS S3A gateway to read data from the storage bucket.</p> <pre><code>dataPath = f\"s3a://my-repo/main/lakefs_test.csv\"\ndf = spark.read.csv(dataPath)\ndf.show()\n</code></pre>"},{"location":"understand/use_cases/etl_testing/#create-a-test-branch","title":"Create a Test Branch","text":"<p>Let us start by creating a new branch <code>test-env</code> on the example repository <code>my-repo</code>.</p> <pre><code>lakefs.repository(\"my-repo\", client=client).branch(\"test-env\").create(source_reference=\"main\")\n</code></pre> <p>Now we can use Spark to write the csv file from <code>main</code> branch as a Parquet file to the <code>test-env</code> of our lakeFS repository. Suppose we accidentally write the dataframe back to \"test-env\" branch again, this time in append mode.</p> <pre><code>df.write.mode('overwrite').parquet('s3a://my-repo/test-env/')\ndf.write.mode('append').parquet('s3a://my-repo/test-env/')\n</code></pre> <p>What happens if we re-read in the data on both branches and perform a count on the resulting DataFrames? There will be twice as many rows in <code>test-env</code> branch. That is, we accidentally duplicated our data! Oh no!</p> <p>Data duplication introduce errors into our data analytics, BI and machine learning efforts; hence we would like to avoid duplicating our data.</p> <p>On the <code>main</code> branch however, there is still just the original data - untouched by our Spark code. This shows the utility of branch-based isolated environments with lakeFS.</p> <p>You can safely continue working with the data from main which is unharmed due to lakeFS isolation capabilities.</p>"},{"location":"understand/use_cases/etl_testing/#further-reading","title":"Further Reading","text":"<ul> <li>Case Study: How Enigma use lakeFS for isolated development and staging environments</li> <li>Tutorial: ETL Testing Tutorial with lakeFS: Step-by-Step Guide</li> <li>ETL Testing: A Practical Guide</li> <li>Top 5 ETL Testing Challenges - Solved!</li> </ul>"},{"location":"understand/use_cases/reproducibility/","title":"Reproducibility","text":""},{"location":"understand/use_cases/reproducibility/#the-benefits-of-reproducible-data","title":"The Benefits of Reproducible Data","text":"<p>Data changes frequently. This makes the task of keeping track of its exact state over time difficult. Oftentimes, people maintain only one state of their data - its current state.</p> <p>This has a negative impact on the work, as it becomes hard to:</p> <ul> <li>Debug a data issue.</li> <li>Validate machine learning training accuracy (re-running a model over different data gives different results).</li> <li>Comply with data audits.</li> </ul> <p>In comparison, lakeFS exposes a Git-like interface to data that allows keeping track of more than just the current state of data. This makes reproducing its state at any point in time straightforward.</p>"},{"location":"understand/use_cases/reproducibility/#achieving-reproducibility-with-lakefs","title":"Achieving Reproducibility with lakeFS","text":"<p>To make data reproducible, we recommend taking a new commit of your lakeFS repository every time the data in it changes. As long as there\u2019s a commit taken, the process to reproduce a given state is as simple as reading the data from a path that includes the unique <code>commit_id</code> generated for each commit.</p> <p>To read data at it\u2019s current state, we can use a static path containing the repository and branch names. To give an example, if you have a repository named <code>example</code> with a branch named <code>main</code>, reading the latest state of this data into a Spark Dataframe is always:</p> <p>Example</p> <pre><code>df = spark.read.parquet(\"s3://example/main/\")\n</code></pre> <p>The code above assumes that all objects in the repository under this path are stored in parquet format. If a different format is used, the applicable Spark read method should be used.</p> <p>In a lakeFS repository, we are capable of taking many commits over the data, making many points in time reproducible. </p> <p></p> <p>In the repository above, a new commit is taken each time a model training script is run, and the commit message includes the specific run number. </p> <p>If we wanted to re-run the model training script and reproduce the exact same results for a historical run, say run #435, we could copy the commit ID associated with the run and read the data into a dataframe like so:</p> <pre><code>df = spark.read.parquet(\"s3://example/296e54fbee5e176f3f4f4aeb7e087f9d57515750e8c3d033b8b841778613cb23/training_dataset/\")\n</code></pre> <p>The ability to reference a specific <code>commit_id</code> in code simplifies reproducing the specific state a data collection or even multiple collections. This has many applications that are common in data development, such as historical debugging, identifying deltas in a data collection, audit compliance, and more.</p>"},{"location":"understand/use_cases/rollback/","title":"Rollbacks","text":""},{"location":"understand/use_cases/rollback/#what-is-a-rollback","title":"What Is a Rollback?","text":"<p>A rollback operation is used to to fix critical data errors immediately.</p> <p>What is a critical data error? Think of a situation where erroneous or misformatted data causes a significant issue with an important service or function. In such situations, the first thing to do is stop the bleeding.</p> <p>Rolling back returns data to a state in the past, before the error was present. You might not be showing all the latest data after a rollback, but at least you aren\u2019t showing incorrect data or raising errors.</p>"},{"location":"understand/use_cases/rollback/#why-rollbacks-are-useful","title":"Why Rollbacks Are Useful","text":"<p>A Rollback is used as a stopgap measure to \u201cput out the fire\u201d as quickly as possible while RCA (root cause analysis) is performed to understand 1) exactly how the error happened, and 2) what can be done to prevent it from happening again.</p> <p>It can be a pressured, stressful situation to deal with a critical data error. Having the ability to employ a rollback relieves some of the pressure and makes it more likely you can figure out what happened without creating additional issues.</p> <p>Example</p> <p>As a real world example, the 14-day outage some Atlassian users experienced in May 2022 could have been an uninteresting minor incident had rolling back the deleted customer data been an option.</p>"},{"location":"understand/use_cases/rollback/#performing-rollbacks-with-lakefs","title":"Performing Rollbacks with lakeFS","text":"<p>lakeFS lets you develop in your data lake in such a way that rollbacks are simple to perform. This starts by taking a commit over your lakeFS repository whenever a change to its state occurs.</p> <p>Using the lakeFS UI or CLI, you can set the current state, or HEAD, of a branch to any historical commit in seconds, effectively performing a rollback.</p> <p>To demonstrate how this works, let's take the example of a lakeFS repo with the following commit history:</p> <p></p> <p>As can be inferred from the history, this repo is updated every minute with a data sync from some data source. An example data sync is a typical ETL job that replicates data from an internal database or any other data source. After each sync, a commit is taken in lakeFS to save a snapshot of data at that point in time.</p>"},{"location":"understand/use_cases/rollback/#how-to-rollback-from-a-bad-data-sync","title":"How to Rollback From a Bad Data Sync?","text":"<p>Say a situation occurs where one of the syncs had bad data and is causing downstream dashboards to fail to load. Since we took a commit of the repo right after the sync ran, we can use a <code>revert</code> operation to undo the changes introduced in that sync.</p> <p></p> <p>Step 1: Copy the <code>commit_id</code> associated with the commit we want to revert. As the screenshot above shows, you can use the Copy ID to Clipboard button to do this.</p> <p>Step 2: Run the revert command using lakectl, the lakeFS CLI. In this example, the command will be as follows:</p> <pre><code>lakectl branch revert \"lakefs://example/main\" 9666d7c9daf37b3ba6964e733d08596ace2ec2c7bc3a4023ad8e80737a6c3e9d\n</code></pre> <p>This will undo the changes introduced by this commit, completing the rollback! </p> <p></p> <p>The rollback operation is that simple, even if many changes were introduced in a commit, spanning acrossmultiple data collections.</p> <p>In lakeFS, rolling back data is always a one-liner.</p>"}]}