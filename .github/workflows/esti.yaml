name: Esti
on:
  pull_request:
  push:
    paths-ignore:
      - "*.md"
      - "docs/**"
      - "webui/**"
      - "design/**"
    branches:
      - master

# These permissions are needed to interact with GitHub's OIDC Token endpoint.
permissions:
  id-token: write
  contents: read
  packages: write

jobs:
  check-secrets:
    name: Check if secrets are available.
    outputs:
      secretsavailable: ${{ steps.enablejobs.outputs.secretsavailable }}
    runs-on: ubuntu-20.04
    steps:
      - id: enablejobs
        env:
          ENABLE_NEXT_JOBS: ${{ secrets.AWS_ACCESS_KEY_ID }}
        run: |
          echo "Enable next jobs based on secrets existence: ${{ env.ENABLE_NEXT_JOBS != '' }}"
          echo "::set-output name=secretsavailable::${{ env.ENABLE_NEXT_JOBS != '' }}"


  gen-code:
    name: Generate code from latest lakeFS app
    runs-on: ubuntu-20.04
    steps:
      - name: Check-out code
        uses: actions/checkout@v2

      # No way to share code between workflows :-( If you change this, find and change the
      # same code wherever "Find Go module and build caches" appears!
      - name: Find Go module and build caches
        run: |
          echo GOMODCACHE=`go env GOMODCACHE` >> $GITHUB_ENV
          echo GOCACHE=`go env GOCACHE` >> $GITHUB_ENV
          cat $GITHUB_ENV

      - name: Setup Go
        uses: actions/setup-go@v2
        with:
          go-version: 1.17.8
        id: go

      - name: Cache Go modules and builds
        uses: actions/cache@v2
        env:
          cache-name: cache-go-modules
        with:
          path: |
            ${{ env.GOMODCACHE }}
            ${{ env.GOCACHE }}
          key: ${{ runner.os }}-build-${{ env.cache-name }}-${{ hashFiles('go.mod', 'go.sum') }}
          restore-keys:
            ${{ runner.os }}-build-${{ env.cache-name }}-
            ${{ runner.os }}-build-
            ${{ runner.os }}-

      - name: Generate code
        run: |
          make -j3 gen-api gen-ui
          tar -cf /tmp/generated.tar.gz .
      - name: Store generated code
        uses: actions/upload-artifact@v2
        with:
          name: generated-code
          path: /tmp/generated.tar.gz

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-region: us-east-1
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}

  deploy-image:
    name: Build and push Docker image
    needs: [check-secrets, gen-code]
    if: needs.check-secrets.outputs.secretsavailable == 'true'
    runs-on: ubuntu-20.04
    outputs:
      tag: ${{ steps.version.outputs.tag }}
    steps:
      - name: Extract version
        shell: bash
        run: echo "::set-output name=tag::sha-04c0c8d"
        id: version

  spark2:
    name: Test lakeFS with Spark 2.x
    needs: deploy-image
    runs-on: ubuntu-20.04
    env:
      TAG: ${{ needs.deploy-image.outputs.tag }}
      REPO: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.us-east-1.amazonaws.com
      SPARK_TAG: 2.4.6
    steps:
      - name: Check-out code
        uses: actions/checkout@v2

      - name: Setup Scala
        uses: olafurpg/setup-scala@v10
      - name: Copy a file from s3
        uses: prewk/s3-cp-action@v2
        with:
          source: 's3://yoni-test3/gencode/sonnets-246_2.11-0.1.0.jar'
          dest: 'test/spark/app/target'
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - uses: vemonet/setup-spark@v1
        with:
          spark-version: '2.4.8'
          hadoop-version: 2.7
      - name: Start lakeFS for Spark tests
        uses: ./.github/actions/bootstrap-test-lakefs
        with:
          compose-directory: test/spark
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          LAKEFS_BLOCKSTORE_TYPE: s3
          LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID: ${{ secrets.ESTI_AWS_ACCESS_KEY_ID }}
          LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY: ${{ secrets.ESTI_AWS_SECRET_ACCESS_KEY }}
      - name: Setup lakeFS for tests
        working-directory: test/spark
        run: ./setup-test.sh

      - name: Generate uniquifying value
        id: unique
        run:  echo "::set-output name=value::$RANDOM"

      - name: Test lakeFS S3 with Spark 2.x
        env:
          STORAGE_NAMESPACE: s3://esti-system-testing/${{ github.run_number }}-spark2/${{ steps.unique.outputs.value }}
          REPOSITORY: gateway-test-spark2
          SONNET_JAR: sonnets-246_2.11-0.1.0.jar
        working-directory: test/spark
        run: ./run-test.sh

      - name: lakeFS Logs on Spark with gateway failure
        if: ${{ failure() }}
        continue-on-error: true
        working-directory: test/spark
        run: docker-compose logs --tail=1000 lakefs

      - name: Build Spark direct-access client
        working-directory: clients/hadoopfs
        run: mvn -Passembly -Djar.finalName=client --batch-mode --update-snapshots package

      - name: Test lakeFS S3 with Spark 2.x thick client
        timeout-minutes: 8
        env:
          JARS: clients/hadoopfs/
          STORAGE_NAMESPACE: s3://esti-system-testing/${{ github.run_number }}-spark2-client/${{ steps.unique.outputs.value }}
          AWS_ACCESS_KEY_ID: ${{ secrets.ESTI_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ESTI_AWS_SECRET_ACCESS_KEY }}
          USE_DIRECT_ACCESS: "true"
          REPOSITORY: thick-client-test
          SONNET_JAR: sonnets-246_2.11-0.1.0.jar
        working-directory: test/spark
        run: ./run-test.sh

      - name: lakeFS Logs on Spark 2.x with client failure
        if: ${{ failure() }}
        continue-on-error: true
        working-directory: test/spark
        run: docker-compose logs --tail=1000 lakefs

  spark3:
    name: Test lakeFS with Spark 3.x
    needs: deploy-image
    runs-on: ubuntu-20.04
    env:
      TAG: ${{ needs.deploy-image.outputs.tag }}
      REPO: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.us-east-1.amazonaws.com
      SPARK_TAG: 3
    steps:
      - name: Check-out code
        uses: actions/checkout@v2

      - name: Setup Scala
        uses: olafurpg/setup-scala@v10

      - name: Package Spark App
        working-directory: test/spark/app
        run: sbt sonnets-311/package

      - name: Generate uniquifying value
        id: unique
        run:  echo "::set-output name=value::$RANDOM"

      - name: Start lakeFS for Spark tests
        uses: ./.github/actions/bootstrap-test-lakefs
        with:
          compose-directory: test/spark
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          LAKEFS_BLOCKSTORE_TYPE: s3
          LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID: ${{ secrets.ESTI_AWS_ACCESS_KEY_ID }}
          LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY: ${{ secrets.ESTI_AWS_SECRET_ACCESS_KEY }}
      - name: Setup lakeFS for tests
        working-directory: test/spark
        run: ./setup-test.sh

      - name: Test lakeFS S3 with Spark 3.x
        continue-on-error: true
        env:
          STORAGE_NAMESPACE: s3://esti-system-testing/${{ github.run_number }}-spark3/${{ steps.unique.outputs.value }}
          REPOSITORY: gateway-test-spark3
          SONNET_JAR: sonnets-311_2.12-0.1.0.jar
        working-directory: test/spark
        run: ./run-test.sh

      - name: lakeFS Logs on Spark with gateway failure
        if: ${{ failure() }}
        continue-on-error: true
        working-directory: test/spark
        run: docker-compose logs --tail=1000 lakefs

      - name: Build Spark direct-access client
        working-directory: clients/hadoopfs
        run: mvn -Passembly -Djar.finalName=client --batch-mode --update-snapshots package

      - name: Test lakeFS S3 with Spark 3.x thick client
        timeout-minutes: 8
        env:
          JARS: clients/hadoopfs/
          STORAGE_NAMESPACE: s3://esti-system-testing/${{ github.run_number }}-spark3-client/${{ steps.unique.outputs.value }}
          AWS_ACCESS_KEY_ID: ${{ secrets.ESTI_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ESTI_AWS_SECRET_ACCESS_KEY }}
          USE_DIRECT_ACCESS: "true"
          REPOSITORY: thick-client-test
          SONNET_JAR: sonnets-311_2.12-0.1.0.jar
        working-directory: test/spark
        run: ./run-test.sh

      - name: lakeFS Logs on Spark with client failure
        if: ${{ failure() }}
        continue-on-error: true
        working-directory: test/spark
        run: docker-compose logs --tail=1000 lakefs
