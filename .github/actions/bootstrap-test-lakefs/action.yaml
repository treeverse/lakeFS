name: 'Start lakeFS for testing'
description: 'Get generated code, authenticate to ECR, and run docker compose up'
inputs:
  compose-flags:
    description: flags to add to docker compose up command
    required: false
    default: "-d"
  compose-file:
    description: alternative path to docker compose file
    required: false
  compose-directory:
    description: working directory for the docker compose step
    required: false
    default: "."
  hadoop-version:
    description: Hadoop version to download
    required: false
    default: ""
  spark-extra-classpath:
    description: The classpath for Spark
    required: false
    default: ""
runs:
  using: "composite"
  steps:
    - name: Retrieve generated code
      uses: actions/download-artifact@v4.1.8
      with:
        name: generated-code
        path: /tmp/
    - name: Unpack generated code
      shell: bash
      run: tar -xzvf /tmp/generated.tar.gz

    - name: Resolve Hadoop version & CP
      id: h
      shell: bash
      run: |
        set -euo pipefail
        ver="${{ inputs.hadoop-version }}"
        if [[ -z "$ver" ]]; then
          case "${SPARK_TAG:-3}" in
            2* ) ver="3.2.1" ;;
            *  ) ver="3.3.6" ;;
          esac
        fi
        echo "ver=$ver" >> "$GITHUB_OUTPUT"
        home="/tmp/hadoop-$ver"
        echo "home=$home" >> "$GITHUB_OUTPUT"
        H_IN_CONT="/opt/hadoop/share/hadoop"
        default_cp="${H_IN_CONT}/common/*:${H_IN_CONT}/common/lib/*:${H_IN_CONT}/hdfs/*:${H_IN_CONT}/hdfs/lib/*:${H_IN_CONT}/mapreduce/*:${H_IN_CONT}/mapreduce/lib/*:${H_IN_CONT}/yarn/*:${H_IN_CONT}/yarn/lib/*:${H_IN_CONT}/aws/*"
        spark_cp="${{ inputs.spark-extra-classpath }}"
        if [[ -z "$spark_cp" ]]; then
          spark_cp="$default_cp"
        fi
        echo "spark_cp=$spark_cp" >> "$GITHUB_OUTPUT"

    - name: Cache Hadoop
      uses: actions/cache@v4
      id: hadoop-cache
      with:
        path: ${{ steps.h.outputs.home }}
        key: hadoop-${{ steps.h.outputs.ver }}

    - name: Download Hadoop if missing
      if: steps.hadoop-cache.outputs.cache-hit != 'true'
      shell: bash
      run: |
        set -euo pipefail
        curl -fsSL "https://archive.apache.org/dist/hadoop/common/hadoop-${{ steps.h.outputs.ver }}/hadoop-${{ steps.h.outputs.ver }}.tar.gz" \
        | tar -xz -C /tmp

    - name: Export env & write .env
      shell: bash
      run: |
        echo "HADOOP_HOME_HOST=${{ steps.h.outputs.home }}" >> "$GITHUB_ENV"
        echo "SPARK_EXTRA_CLASSPATH=${{ steps.h.outputs.spark_cp }}" >> "$GITHUB_ENV"
        printf "HADOOP_HOME_HOST=%s\nSPARK_EXTRA_CLASSPATH=%s\n" \
          "${{ steps.h.outputs.home }}" \
          "${{ steps.h.outputs.spark_cp }}" \
          > "${{ inputs.compose-directory }}/.env"


    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    - name: Start docker compose
      env:
        LAKEFS_STATS_ENABLED: "false"
        LAKEFS_GATEWAYS_S3_DOMAIN_NAME: s3.docker.lakefs.io:8000
        COMPOSE_PARALLEL_LIMIT: 1 # Workaround for https://github.com/docker/compose/issues/12747 until compose >= v2.36.0
      shell: bash
      run: |
        set -euo pipefail
        echo "HADOOP_HOME_HOST=${HADOOP_HOME_HOST:-<empty>}"
        echo "SPARK_EXTRA_CLASSPATH=${SPARK_EXTRA_CLASSPATH:-<empty>}"
        [[ -z "${{ inputs.compose-file }}" ]] && flags="" || flags="-f ${{ inputs.compose-file }}"
        docker compose --project-directory ${{ inputs.compose-directory }} ${flags} up ${{ inputs.compose-flags }}
