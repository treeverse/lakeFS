name: 'Start lakeFS for testing'
description: 'Get generated code, authenticate to ECR, and run docker compose up'
inputs:
  compose-flags:
    description: flags to add to docker compose up command
    required: false
    default: "-d"
  compose-file:
    description: alternative path to docker compose file
    required: false
  compose-directory:
    description: working directory for the docker compose step
    required: false
    default: "."
  hadoop-version:
    description: Hadoop version to download
    required: false
    default: ""
  spark-extra-classpath:
    description: The classpath for Spark
    required: false
    default: ""
runs:
  using: "composite"
  steps:
    - name: Retrieve generated code
      uses: actions/download-artifact@v4.1.8
      with:
        name: generated-code
        path: /tmp/
    - name: Unpack generated code
      shell: bash
      run: tar -xzvf /tmp/generated.tar.gz

    - name: Resolve Hadoop version
      id: v
      shell: bash
      run: |
        set -euo pipefail
        ver="${{ inputs.hadoop-version }}"
        if [[ -z "$ver" ]]; then
          case "${SPARK_TAG:-3}" in
            2* ) ver="3.2.1" ;;
            *  ) ver="3.3.6" ;;
          esac
        fi
        echo "ver=$ver"  >> "$GITHUB_OUTPUT"
        echo "home=/tmp/hadoop-$ver" >> "$GITHUB_OUTPUT"

    - name: Cache Hadoop
      uses: actions/cache@v4
      id: hadoop-cache
      with:
        path: ${{ steps.v.outputs.home }}
        key: hadoop-${{ steps.v.outputs.ver }}

    - name: Download Hadoop if missing
      if: steps.hadoop-cache.outputs.cache-hit != 'true'
      shell: bash
      run: |
        set -euo pipefail
        curl -fsSL "https://archive.apache.org/dist/hadoop/common/hadoop-${{ steps.v.outputs.ver }}/hadoop-${{ steps.v.outputs.ver }}.tar.gz" \
        | tar -xz -C /tmp

    - name: Build minimal Spark CP from Hadoop
      id: cp
      shell: bash
      run: |
        set -euo pipefail
        home="${{ steps.v.outputs.home }}"
        H_HOST="$home/share/hadoop"

        if compgen -G "$H_HOST/tools/lib/hadoop-aws-*.jar" > /dev/null; then
          aws_path="$H_HOST/tools/lib/hadoop-aws-*.jar"
        elif compgen -G "$H_HOST/aws/hadoop-aws-*.jar" > /dev/null; then
          aws_path="$H_HOST/aws/hadoop-aws-*.jar"
        else
          echo "::error::Could not find hadoop-aws under $H_HOST"; exit 1
        fi

        mapfile -t JARS_HOST < <(
          compgen -G "$H_HOST/common/hadoop-common-*.jar"
          compgen -G "$H_HOST/common/lib/hadoop-auth-*.jar"
          compgen -G "$aws_path"

          compgen -G "$H_HOST/common/lib/woodstox-core-*.jar"
          compgen -G "$H_HOST/common/lib/stax2-api-*.jar"

          compgen -G "$H_HOST/common/lib/aws-java-sdk-*.jar" || true
          compgen -G "$H_HOST/common/lib/aws-java-sdk-bundle-*.jar" || true
          compgen -G "$H_HOST/common/lib/jackson-*.jar"
          compgen -G "$H_HOST/common/lib/http*.jar"
          compgen -G "$H_HOST/common/lib/commons-*.jar"
          compgen -G "$H_HOST/common/lib/joda-time-*.jar"
        )

        # sanity
        if [[ ${#JARS_HOST[@]} -lt 5 ]]; then
          printf "::error::Too few jars collected (%d). Found:\n%s\n" "${#JARS_HOST[@]}" "${JARS_HOST[*]}"; exit 1
        fi

        CP_CONT=""
        for p in "${JARS_HOST[@]}"; do
          p_cont="${p/$home/\/opt\/hadoop}"
          CP_CONT="${CP_CONT:+${CP_CONT}:}${p_cont}"
        done

        echo "spark_cp_min=$CP_CONT" >> "$GITHUB_OUTPUT"

    - name: Export env & write .env
      shell: bash
      run: |
        echo "HADOOP_HOME_HOST=${{ steps.v.outputs.home }}" >> "$GITHUB_ENV"
        echo "SPARK_EXTRA_CLASSPATH=${{ steps.cp.outputs.spark_cp_min }}" >> "$GITHUB_ENV"
        cat > "${{ inputs.compose-directory }}/.env" <<EOF
        HADOOP_HOME_HOST=${{ steps.v.outputs.home }}
        SPARK_EXTRA_CLASSPATH=${{ steps.cp.outputs.spark_cp_min }}
        EOF

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    - name: Start docker compose
      env:
        LAKEFS_STATS_ENABLED: "false"
        LAKEFS_GATEWAYS_S3_DOMAIN_NAME: s3.docker.lakefs.io:8000
        COMPOSE_PARALLEL_LIMIT: 1 # Workaround for https://github.com/docker/compose/issues/12747 until compose >= v2.36.0
      shell: bash
      run: |
        [[ -z "${{ inputs.compose-file }}" ]] && flags="" || flags="-f ${{ inputs.compose-file }}"
        docker compose --project-directory ${{ inputs.compose-directory }} ${flags} up ${{ inputs.compose-flags }}
