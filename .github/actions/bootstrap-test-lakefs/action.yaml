name: 'Start lakeFS for testing'
description: 'Get generated code, authenticate to ECR, and run docker compose up'
inputs:
  compose-flags:
    description: flags to add to docker compose up command
    required: false
    default: "-d"
  compose-file:
    description: alternative path to docker compose file
    required: false
  compose-directory:
    description: working directory for the docker compose step
    required: false
    default: "."
  hadoop-version:
    description: Hadoop version to download
    required: false
    default: ""
  spark-extra-classpath:
    description: The classpath for Spark
    required: false
    default: ""
runs:
  using: "composite"
  steps:
    - name: Retrieve generated code
      uses: actions/download-artifact@v4.1.8
      with:
        name: generated-code
        path: /tmp/
    - name: Unpack generated code
      shell: bash
      run: tar -xzvf /tmp/generated.tar.gz

    - name: Resolve Hadoop version
      id: h
      shell: bash
      run: |
        set -euo pipefail
        ver="${{ inputs.hadoop-version }}"
        if [[ -z "$ver" ]]; then
          case "${SPARK_TAG:-3}" in
            2* ) ver="3.2.1" ;;
            *  ) ver="3.3.6" ;;
          esac
        fi
        echo "ver=$ver"  >> "$GITHUB_OUTPUT"
        echo "home=/tmp/hadoop-$ver" >> "$GITHUB_OUTPUT"

    - name: Cache Hadoop
      uses: actions/cache@v4
      id: hadoop-cache
      with:
        path: ${{ steps.h.outputs.home }}
        key: hadoop-${{ steps.h.outputs.ver }}

    - name: Download Hadoop if missing
      if: steps.hadoop-cache.outputs.cache-hit != 'true'
      shell: bash
      run: |
        set -euo pipefail
        curl -fsSL "https://archive.apache.org/dist/hadoop/common/hadoop-${{ steps.h.outputs.ver }}/hadoop-${{ steps.h.outputs.ver }}.tar.gz" \
        | tar -xz -C /tmp

    - name: Build minimal Hadoop CP (after download)
      id: hcp
      shell: bash
      run: |
        set -euo pipefail
        home="${{ steps.h.outputs.home }}"
        H_HOST="$home/share/hadoop"
        [[ -d "$H_HOST" ]] || { echo "Missing $H_HOST"; find "$home" -maxdepth 2 -type d -print; exit 1; }

        echo "::group::Hadoop layout"; find "$H_HOST" -maxdepth 2 -type d -print; echo "::endgroup::"

        # חובה: hadoop-common + hadoop-auth
        mapfile -t COMMON < <(find "$H_HOST/common" -maxdepth 1 -type f -name "hadoop-common-*.jar" -print)
        mapfile -t AUTH   < <(find "$H_HOST/common/lib" -maxdepth 1 -type f -name "hadoop-auth-*.jar" -print)

        mapfile -t AWS_CORE < <(
          find "$H_HOST" \( -path "$H_HOST/tools/lib/*" -o -path "$H_HOST/aws/*" -o -path "$H_HOST/client/*" \) \
               -maxdepth 1 -type f -name "hadoop-aws-*.jar" -print 2>/dev/null || true
        )

        AWS_DEPS=()
        if [[ ${#AWS_CORE[@]} -gt 0 ]]; then
          d="$(dirname "${AWS_CORE[0]}")"
          while IFS= read -r j; do AWS_DEPS+=("$j"); done < <(
            find "$d" -maxdepth 1 -type f \
              \( -name "aws-java-sdk-*.jar" -o -name "aws-java-sdk-bundle-*.jar" \
                 -o -name "jackson-*.jar" -o -name "http*.jar" \
                 -o -name "commons-*.jar" -o -name "joda-time-*.jar" \) \
              ! -name "netty*.jar" -print
          )
        fi

        ALL=( "${COMMON[@]}" "${AUTH[@]}" "${AWS_CORE[@]}" "${AWS_DEPS[@]}" )
        if [[ ${#ALL[@]} -lt 2 ]]; then
          echo "Did not find minimal Hadoop jars under $H_HOST"
          echo "found:"; find "$H_HOST" -type f -name "*.jar" | sed 's,^,  ,'
          exit 1
        fi

        CP_CONT=""
        for p in "${ALL[@]}"; do
          p_cont="${p/$home/\/opt\/hadoop}"
          CP_CONT="${CP_CONT:+${CP_CONT}:}${p_cont}"
        done
        echo "spark_cp=$CP_CONT" >> "$GITHUB_OUTPUT"

    - name: Export env & write .env
      shell: bash
      run: |
        set -euo pipefail
        echo "HADOOP_HOME_HOST=${{ steps.h.outputs.home }}" >> "$GITHUB_ENV"
        # אם ה-input spark-extra-classpath לא ריק – תן לו עדיפות; אחרת קח את ה-autodetect
        if [[ -n "${{ inputs.spark-extra-classpath }}" ]]; then
          echo "SPARK_EXTRA_CLASSPATH=${{ inputs.spark-extra-classpath }}" >> "$GITHUB_ENV"
          SPARK_CP="${{ inputs.spark-extra-classpath }}"
        else
          echo "SPARK_EXTRA_CLASSPATH=${{ steps.hcp.outputs.spark_cp }}" >> "$GITHUB_ENV"
          SPARK_CP="${{ steps.hcp.outputs.spark_cp }}"
        fi
        mkdir -p "${{ inputs.compose-directory }}"
        printf "HADOOP_HOME_HOST=%s\nSPARK_EXTRA_CLASSPATH=%s\n" \
          "${{ steps.h.outputs.home }}" \
          "$SPARK_CP" \
          > "${{ inputs.compose-directory }}/.env"

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    - name: Start docker compose
      env:
        LAKEFS_STATS_ENABLED: "false"
        LAKEFS_GATEWAYS_S3_DOMAIN_NAME: s3.docker.lakefs.io:8000
        COMPOSE_PARALLEL_LIMIT: 1 # Workaround for https://github.com/docker/compose/issues/12747 until compose >= v2.36.0
      shell: bash
      run: |
        set -euo pipefail
        echo "HADOOP_HOME_HOST=${HADOOP_HOME_HOST:-<empty>}"
        echo "SPARK_EXTRA_CLASSPATH=${SPARK_EXTRA_CLASSPATH:-<empty>}"
        [[ -z "${{ inputs.compose-file }}" ]] && flags="" || flags="-f ${{ inputs.compose-file }}"
        docker compose --project-directory ${{ inputs.compose-directory }} ${flags} up ${{ inputs.compose-flags }}
