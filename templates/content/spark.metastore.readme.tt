{{contenttype "text/markdown" -}}
# Spark Quickstart - {{ .Query.repo }}

Congratulations on creating your Spark repository. The "{{ .Query.repo }}" repository was created with the branch "{{ .Query.branch }}".
You can now start using it with your Spark code.

## Completed steps

1. Created repository {{ .Query.repo }} and branch {{ .Query.branch }}.
{{if .Query.import_location -}}
1. Imported data from {{ .Query.import_location }}.
{{- end}}
1. Generated Spark configuration.

## Getting started

As lakeFS is S3 compatible, all you need to do is to configure your Spark application's S3A Hadoop filesystem with your lakeFS endpoint and credentials provided by the Spark Quickstart wizard.

## Metastore Configuration

The [`lakectl metastore` commands](https://docs.lakefs.io/integrations/glue_hive_metastore.html#commands) can run on Glue or Hive metastore.
To use `lakectl`, you will need to download the binary. [Learn more](https://www.youtube.com/watch?v=8nO7RT411nA).

Add the following to the lakectl configuration file (by default ~/.lakectl.yaml):

### Hive

```conf
metastore:
  type: hive
  hive:
    uri: <hive-metastore-uri>:<hive-metastore-port>
```

### Glue

```conf
metastore:
  type: glue
  glue:
    catalog-id: <catalog id>
    region: <AWS region>
    profile: default # optional, implies using a credentials file
    credentials:
      access_key_id: <AWS access key id>
      secret_access_key: <AWS secret access key>
```

## Suggested model

For simplicity, we recommend creating a schema for each branch. That way, you can use the same table name across different schemas.
For example, using Trino:
`CREATE SCHEMA {{ .Query.branch }}_schema with (location = 's3://{{ .Query.repo }}/{{ .Query.branch }}/');`
