{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78797e22",
   "metadata": {},
   "source": [
    "# Exploring LakeFS with PySpark\n",
    "\n",
    "This uses the [Everything Bagel](https://github.com/treeverse/lakeFS/tree/master/deployments/compose) Docker Compose environment.\n",
    "\n",
    "[@rmoff](https://twitter.com/rmoff/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a642edb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89718426",
   "metadata": {},
   "source": [
    "### Install libraries\n",
    "\n",
    "(could be built into the `Dockerfile`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fa1da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lakefs_client\n",
      "  Downloading lakefs_client-0.98.0-py3-none-any.whl (312 kB)\n",
      "     |████████████████████████████████| 312 kB 3.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.25.3 in /opt/conda/lib/python3.9/site-packages (from lakefs_client) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.9/site-packages (from lakefs_client) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil->lakefs_client) (1.16.0)\n",
      "Installing collected packages: lakefs-client\n",
      "Successfully installed lakefs-client-0.98.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install lakefs_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b43d979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deltalake\n",
      "  Downloading deltalake-0.8.1-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (17.3 MB)\n",
      "     |████████████████████████████████| 17.3 MB 27.8 MB/s            \n",
      "\u001b[?25hCollecting pyarrow>=7\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (32.7 MB)\n",
      "     |████████████████████████████████| 32.7 MB 9.5 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.9/site-packages (from pyarrow>=7->deltalake) (1.20.3)\n",
      "Installing collected packages: pyarrow, deltalake\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 6.0.1\n",
      "    Uninstalling pyarrow-6.0.1:\n",
      "      Successfully uninstalled pyarrow-6.0.1\n",
      "Successfully installed deltalake-0.8.1 pyarrow-11.0.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install deltalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ba6754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: /opt/conda/bin/python\n",
      "Python version: 3.9.7 | packaged by conda-forge | (default, Oct 10 2021, 15:08:54) \n",
      "[GCC 9.4.0]\n",
      "PySpark version: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Kernel:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "import pyspark\n",
    "print(\"PySpark version:\", pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3db35",
   "metadata": {},
   "source": [
    "###  Spark\n",
    "\n",
    "_With the necessary Delta Lake config too_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d3747db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.session import SparkSession\n",
    "# sc = SparkContext('local[*]')\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.0.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\")\n",
    "    .getOrCreate()\n",
    ")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a7daa",
   "metadata": {},
   "source": [
    "#### Test delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70664be",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot write to already existent path file:/tmp/delta-table without setting OVERWRITE = 'true'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_850/939553335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/delta-table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot write to already existent path file:/tmp/delta-table without setting OVERWRITE = 'true'."
     ]
    }
   ],
   "source": [
    "data = spark.range(0, 5)\n",
    "data.write.format(\"delta\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824b929",
   "metadata": {},
   "source": [
    "### LakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e44584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient\n",
    "from lakefs_client.api import branches_api\n",
    "from lakefs_client.api import commits_api\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = 'AKIA-EXAMPLE-KEY'\n",
    "configuration.password = 'EXAMPLE-SECRET'\n",
    "configuration.host = 'http://lakefs:8000'\n",
    "\n",
    "client = LakeFSClient(configuration)\n",
    "api_client = lakefs_client.ApiClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0047139f",
   "metadata": {},
   "source": [
    "#### List the current branches in the repository\n",
    "\n",
    "https://pydocs.lakefs.io/docs/BranchesApi.html#list_branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9651e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo='example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in client.branches.list_branches(repo).results:\n",
    "    display(b.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample parquet file is Apache 2.0 licensed so perhaps include it in the Everything Bagel distribution? \n",
    "url='https://github.com/Teradata/kylo/blob/master/samples/sample-data/parquet/userdata1.parquet?raw=true'\n",
    "sc.addFile(url)\n",
    "df = spark.read.parquet(\"file://\" + SparkFiles.get(\"userdata1.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22768b",
   "metadata": {},
   "source": [
    "How many rows of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec747fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f17847",
   "metadata": {},
   "source": [
    "What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6268496",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.show(n=1,vertical=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41dab2b",
   "metadata": {},
   "source": [
    "## Write data to S3 (on the `main` branch)\n",
    "\n",
    "N.B. the connection to s3a is configured in the Docker Compose's `./etc/hive-site.xml` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch='main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68718621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').parquet('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963a378",
   "metadata": {},
   "source": [
    "### The data as seen from LakeFS\n",
    "\n",
    "https://pydocs.lakefs.io/docs/ObjectsApi.html#list_objects\n",
    "\n",
    "Note the `physical_address` and its match in the S3 output in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e64655",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.objects.list_objects(repo,branch).results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(0, 5)\n",
    "data.write.format(\"delta\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f721d61",
   "metadata": {},
   "source": [
    "### The data as seen from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36742058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ff2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in s3.Bucket(repo).objects.all():\n",
    "    print(o.last_modified, o.key, o.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc64687",
   "metadata": {},
   "source": [
    "### List diff of branch in LakeFS (this is kinda like a `git status`)\n",
    "\n",
    "https://pydocs.lakefs.io/docs/BranchesApi.html#diff_branch\n",
    "\n",
    "_Note that the files show **`'type': 'added'`**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a273e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "\n",
    "api_response = api_instance.diff_branch(repo, branch)\n",
    "if api_response.pagination.results==0:\n",
    "    display(\"Nothing to commit\")\n",
    "else:\n",
    "    for r in api_response.results:\n",
    "        display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdcb3c",
   "metadata": {},
   "source": [
    "### Commit the new file in `main`\n",
    "\n",
    "https://pydocs.lakefs.io/docs/CommitsApi.html#commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"Everything Bagel - commit users data (original)\",\n",
    "    metadata={\n",
    "        \"foo\": \"bar\",\n",
    "    }\n",
    ") \n",
    "\n",
    "api_instance.commit(repo, branch, commit_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf82895",
   "metadata": {},
   "source": [
    "### List branch status again - nothing returned means that there is nothing uncommitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "\n",
    "api_response = api_instance.diff_branch(repo, branch)\n",
    "if api_response.pagination.results==0:\n",
    "    display(\"Nothing to commit\")\n",
    "else:\n",
    "    for r in api_response.results:\n",
    "        display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841e702",
   "metadata": {},
   "source": [
    "_Similar to a `git status` showing `Your branch is up to date with 'main'` / `nothing to commit, working tree clean`_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c5e94",
   "metadata": {},
   "source": [
    "## Create a branch\n",
    "\n",
    "https://pydocs.lakefs.io/docs/BranchesApi.html#create_branch\n",
    "\n",
    "**TODO** Show that there's no additional object created on object store (http://localhost:9001/buckets/example/browse login `minioadmin`/`minioadmin`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa39ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch='add_more_user_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client.model.branch_creation import BranchCreation\n",
    "\n",
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "branch_creation = BranchCreation(\n",
    "    name=branch,\n",
    "    source=\"main\",\n",
    ") \n",
    "\n",
    "api_response = api_instance.create_branch(repo, branch_creation)\n",
    "display(api_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e1a416",
   "metadata": {},
   "source": [
    "### List the current branches in the `example` repository\n",
    "\n",
    "https://pydocs.lakefs.io/docs/BranchesApi.html#list_branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118732d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in client.branches.list_branches(repo).results:\n",
    "    display(b.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5926aa",
   "metadata": {},
   "source": [
    "## Confirm that you can see the same data on the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ff5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xform_df = spark.read.parquet('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ca0c3",
   "metadata": {},
   "source": [
    "How many rows of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(xform_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be899d4b",
   "metadata": {},
   "source": [
    "What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10219381",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(xform_df.show(n=1,vertical=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185214d",
   "metadata": {},
   "source": [
    "### Note that on S3 there is still just the original 78k object - we've not duplicated any data for the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in s3.Bucket(repo).objects.all():\n",
    "    print(o.last_modified, o.key, o.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1378435c",
   "metadata": {},
   "source": [
    "## Add some new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc637861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample parquet file is Apache 2.0 licensed so perhaps include it in the Everything Bagel distribution? \n",
    "url='https://github.com/Teradata/kylo/blob/master/samples/sample-data/parquet/userdata2.parquet?raw=true'\n",
    "sc.addFile(url)\n",
    "df = spark.read.parquet(\"file://\" + SparkFiles.get(\"userdata2.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabc77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d337977",
   "metadata": {},
   "source": [
    "## Write the data to the new branch and commit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32412585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('append').parquet('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef8bc98",
   "metadata": {},
   "source": [
    "LakeFS sees that there is an uncommited change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "\n",
    "api_response = api_instance.diff_branch(repo, branch)\n",
    "if api_response.pagination.results==0:\n",
    "    display(\"Nothing to commit\")\n",
    "else:\n",
    "    for r in api_response.results:\n",
    "        display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a958b",
   "metadata": {},
   "source": [
    "Commit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ec372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"Everything Bagel - add more user data\",\n",
    "    metadata={\n",
    "        \"foo\": \"bar\",\n",
    "    }\n",
    ") \n",
    "\n",
    "api_instance.commit(repo, branch, commit_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247a98a",
   "metadata": {},
   "source": [
    "## Re-read `main` and `add_more_user_data` branches and count rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639f82a",
   "metadata": {},
   "source": [
    "Original branch (`main`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b53af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "main = spark.read.parquet('s3a://'+repo+'/main/demo/users')\n",
    "display(main.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45d669",
   "metadata": {},
   "source": [
    "New branch (`add_more_user_data`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdefe7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_more_user_data = spark.read.parquet('s3a://'+repo+'/add_more_user_data/demo/users')\n",
    "display(add_more_user_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24fcd35",
   "metadata": {},
   "source": [
    "### Look at the view in LakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127c37e",
   "metadata": {},
   "source": [
    "#### `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.objects.list_objects(repo,'main').results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa60e7",
   "metadata": {},
   "source": [
    "#### `add_more_user_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.objects.list_objects(repo,'add_more_user_data').results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a630e6",
   "metadata": {},
   "source": [
    "### The data as seen from S3\n",
    "\n",
    "Note that there are just two 78k files; there is no duplication of data shared by branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2183e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in s3.Bucket(repo).objects.all():\n",
    "    print(o.last_modified, o.key, o.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6049a1",
   "metadata": {},
   "source": [
    "## Create a new branch and test removing some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch='remove_pii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client.model.branch_creation import BranchCreation\n",
    "\n",
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "branch_creation = BranchCreation(\n",
    "    name=branch,\n",
    "    source=\"main\",\n",
    ") \n",
    "\n",
    "api_response = api_instance.create_branch(repo, branch_creation)\n",
    "display(api_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63156cd",
   "metadata": {},
   "source": [
    "### List the current branches in the `example` repository\n",
    "\n",
    "https://pydocs.lakefs.io/docs/BranchesApi.html#list_branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in client.branches.list_branches(repo).results:\n",
    "    display(b.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335d022",
   "metadata": {},
   "source": [
    "### Confirm that you can see the same data on the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ecf052",
   "metadata": {},
   "outputs": [],
   "source": [
    "xform_df = spark.read.parquet('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d065d2",
   "metadata": {},
   "source": [
    "How many rows of data? \n",
    "\n",
    "_Note that this shows 1000 per `main`, and not 2000 per the `add_more_user_data` branch above since this has not been merged to `main`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9e8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(xform_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeace3b0",
   "metadata": {},
   "source": [
    "If you are reading and write a file from the same place, you need to use `.cache()` otherwise the write will fail with an error like this: \n",
    "\n",
    "```\n",
    "Caused by: java.io.FileNotFoundException: \n",
    "No such file or directory: s3a://example/remove_pii/demo/users/part-00000-7a0bbe79-a3e2-4355-984e-bd8b950a4e0c-c000.snappy.parquet\n",
    "```\n",
    "\n",
    "[solution src](https://stackoverflow.com/a/65330116/350613)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25872e",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95aa00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=xform_df.drop('ip_address','birthdate','salary','email').cache()\n",
    "# You need to do something to access the DF otherwise the `cache()` won't have any effect\n",
    "df2.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff402ece",
   "metadata": {},
   "source": [
    "### Write data back to the branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7252b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.mode('overwrite').parquet('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e6baf6",
   "metadata": {},
   "source": [
    "### Commit changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2347aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"Remove PII\",\n",
    ") \n",
    "\n",
    "api_instance.commit(repo, branch, commit_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b8319",
   "metadata": {},
   "source": [
    "## Re-read all branches and inspect data for isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16909b0",
   "metadata": {},
   "source": [
    "Original branch (`main`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "main = spark.read.parquet('s3a://'+repo+'/main/demo/users')\n",
    "display(main.count())\n",
    "main.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc188c",
   "metadata": {},
   "source": [
    "New branch (`add_more_user_data`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_more_user_data = spark.read.parquet('s3a://'+repo+'/add_more_user_data/demo/users')\n",
    "display(add_more_user_data.count())\n",
    "add_more_user_data.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be8072",
   "metadata": {},
   "source": [
    "New branch (`remove_pii`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_pii = spark.read.parquet('s3a://'+repo+'/remove_pii/demo/users')\n",
    "display(remove_pii.count())\n",
    "remove_pii.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9cf92",
   "metadata": {},
   "source": [
    "### Look at the view in LakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9545ec95",
   "metadata": {},
   "source": [
    "#### `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5042d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.objects.list_objects(repo,'main').results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40d620",
   "metadata": {},
   "source": [
    "#### `add_more_user_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.objects.list_objects(repo,'add_more_user_data').results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603d5fb",
   "metadata": {},
   "source": [
    "#### `remove_pii`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8770f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.objects.list_objects(repo,'remove_pii').results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed727c06",
   "metadata": {},
   "source": [
    "## Merge `remove_pii` into `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a53f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.refs.merge_into_branch(repository=repo, source_ref='remove_pii', destination_branch='main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254ff43",
   "metadata": {},
   "source": [
    "Original branch (`main`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "main = spark.read.parquet('s3a://'+repo+'/main/demo/users')\n",
    "display(main.count())\n",
    "main.show(n=1,vertical=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
