{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e13cd9",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.lakefs.io/assets/logo.svg\" alt=\"lakeFS logo\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78797e22",
   "metadata": {},
   "source": [
    "## Write-Audit-Publish (WAP) pattern in lakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f89a51c",
   "metadata": {},
   "source": [
    "_This notebook uses the [Everything Bagel](https://github.com/treeverse/lakeFS/tree/master/deployments/compose) Docker Compose environment._\n",
    "\n",
    "**New to Write-Audit-Publish? This [talk](https://www.youtube.com/watch?v=fXHdeBnpXrg&t=1001s) explains it well.**\n",
    "\n",
    "[@rmoff](https://twitter.com/rmoff/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89718426",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824b929",
   "metadata": {},
   "source": [
    "#### Set up the connection to lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e44584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient\n",
    "from lakefs_client.api import branches_api\n",
    "from lakefs_client.api import commits_api\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = 'AKIAIOSFODNN7EXAMPLE'\n",
    "configuration.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\n",
    "configuration.host = 'http://lakefs:8000'\n",
    "\n",
    "client = LakeFSClient(configuration)\n",
    "api_client = lakefs_client.ApiClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0047139f",
   "metadata": {},
   "source": [
    "##### Test the connection by listing out the repositories present in lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57897f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'creation_date': 1682422095,\n",
       "  'default_branch': 'main',\n",
       "  'id': 'example',\n",
       "  'storage_namespace': 's3://example'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.repositories.list_repositories().results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e906a",
   "metadata": {},
   "source": [
    "#### Set up Spark and load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36742058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0de1043fb8b2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Jupyter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff5dcca620>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Jupyter\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00796d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ef4a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS nyc.permits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad496308",
   "metadata": {},
   "outputs": [
    {
     "ename": "SparkRuntimeException",
     "evalue": "[LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`nyc`.`permits`, as its associated location 'file:/home/jovyan/work/spark-warehouse/nyc.db/permits' already exists. Please pick a different table name, or remove the existing location first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSparkRuntimeException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mjson(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/data/nyc_film_permits.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnyc.permits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1521\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mSparkRuntimeException\u001b[0m: [LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`nyc`.`permits`, as its associated location 'file:/home/jovyan/work/spark-warehouse/nyc.db/permits' already exists. Please pick a different table name, or remove the existing location first."
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferSchema\",\"true\").option(\"multiline\",\"true\").json(\"/home/jovyan/data/nyc_film_permits.json\")\n",
    "df.write.saveAsTable(\"nyc.permits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f17847",
   "metadata": {},
   "source": [
    "What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6268496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>borough</th>\n",
       "        <th>permit_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Queens</td>\n",
       "        <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Brooklyn</td>\n",
       "        <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Staten Island</td>\n",
       "        <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Manhattan</td>\n",
       "        <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Bronx</td>\n",
       "        <td>28</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+------------+\n",
       "|       borough | permit_cnt |\n",
       "+---------------+------------+\n",
       "|        Queens |        168 |\n",
       "|      Brooklyn |        334 |\n",
       "| Staten Island |          7 |\n",
       "|     Manhattan |        463 |\n",
       "|         Bronx |         28 |\n",
       "+---------------+------------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT borough, count(*) permit_cnt\n",
    "FROM nyc.permits\n",
    "GROUP BY borough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41dab2b",
   "metadata": {},
   "source": [
    "## Write data to S3 (on the `main` branch)\n",
    "\n",
    "N.B. the connection to s3a is configured in the Docker Compose's `./etc/hive-site.xml` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40be34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch='main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68718621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').parquet('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963a378",
   "metadata": {},
   "source": [
    "### The data as seen from LakeFS\n",
    "\n",
    "https://pydocs.lakefs.io/docs/ObjectsApi.html#list_objects\n",
    "\n",
    "Note the `physical_address` and its match in the S3 output in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15e64655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'checksum': 'd41d8cd98f00b204e9800998ecf8427e',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663316305,\n",
       "  'path': 'demo/users/_SUCCESS',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/5e91c0a09d3a4415abd33bc460662e18',\n",
       "  'size_bytes': 0},\n",
       " {'checksum': 'addfab49d691a5d4a18ae0b127a792a0',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663316304,\n",
       "  'path': 'demo/users/part-00000-142e4126-aaec-477c-bbcf-c1dd68011369-c000.snappy.parquet',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/2895bba44b5b4f5199bf58d80144a9de',\n",
       "  'size_bytes': 78869}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.objects.list_objects(repo,branch).results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f721d61",
   "metadata": {},
   "source": [
    "### The data as seen from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df3ff2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-16 08:18:24.769000+00:00 1984e4e69777452fb03800bd4c6b1b05 0\n",
      "2022-09-16 08:18:23.607000+00:00 2895bba44b5b4f5199bf58d80144a9de 78869\n",
      "2022-09-16 08:18:22.900000+00:00 49ad588d52b942c9b682f271f228cb9c 0\n",
      "2022-09-16 08:18:25.034000+00:00 5e91c0a09d3a4415abd33bc460662e18 0\n",
      "2022-09-16 08:18:24.277000+00:00 c4cae27b7c0a46beb14f79347d77c29d 0\n",
      "2022-09-16 08:17:18.615000+00:00 dummy 70\n"
     ]
    }
   ],
   "source": [
    "for o in s3.Bucket(repo).objects.all():\n",
    "    print(o.last_modified, o.key, o.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc64687",
   "metadata": {},
   "source": [
    "### List diff of branch in LakeFS (this is kinda like a `git status`)\n",
    "\n",
    "https://pydocs.lakefs.io/docs/BranchesApi.html#diff_branch\n",
    "\n",
    "_Note that the files show **`'type': 'added'`**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a273e345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'demo/users/_SUCCESS',\n",
       " 'path_type': 'object',\n",
       " 'size_bytes': 78869,\n",
       " 'type': 'added'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'path': 'demo/users/part-00000-142e4126-aaec-477c-bbcf-c1dd68011369-c000.snappy.parquet',\n",
       " 'path_type': 'object',\n",
       " 'size_bytes': 78869,\n",
       " 'type': 'added'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "\n",
    "api_response = api_instance.diff_branch(repo, branch)\n",
    "if api_response.pagination.results==0:\n",
    "    display(\"Nothing to commit\")\n",
    "else:\n",
    "    for r in api_response.results:\n",
    "        display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdcb3c",
   "metadata": {},
   "source": [
    "### Commit the new file in `main`\n",
    "\n",
    "https://pydocs.lakefs.io/docs/CommitsApi.html#commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0078cc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1663316414,\n",
       " 'id': 'de2ae0f24961f65bf7b525d983b3dbc869ce49a0dea43529920743154ce0ddd0',\n",
       " 'message': 'Everything Bagel - commit users data (original)',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {'foo': 'bar'},\n",
       " 'parents': ['a3bfc3317b697cb671665ea44b05b4009a06404c95c56a705fe638a824f1c04e']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"Everything Bagel - commit users data (original)\",\n",
    "    metadata={\n",
    "        \"foo\": \"bar\",\n",
    "    }\n",
    ") \n",
    "\n",
    "api_instance.commit(repo, branch, commit_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf82895",
   "metadata": {},
   "source": [
    "### List branch status again - nothing returned means that there is nothing uncommitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f758c8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nothing to commit'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "\n",
    "api_response = api_instance.diff_branch(repo, branch)\n",
    "if api_response.pagination.results==0:\n",
    "    display(\"Nothing to commit\")\n",
    "else:\n",
    "    for r in api_response.results:\n",
    "        display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841e702",
   "metadata": {},
   "source": [
    "_Similar to a `git status` showing `Your branch is up to date with 'main'` / `nothing to commit, working tree clean`_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c5e94",
   "metadata": {},
   "source": [
    "## Create a branch\n",
    "\n",
    "https://pydocs.lakefs.io/docs/BranchesApi.html#create_branch\n",
    "\n",
    "**TODO** Show that there's no additional object created on object store (http://localhost:9001/buckets/example/browse login `minioadmin`/`minioadmin`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fa39ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch='add_more_user_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fecd1769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de2ae0f24961f65bf7b525d983b3dbc869ce49a0dea43529920743154ce0ddd0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lakefs_client.model.branch_creation import BranchCreation\n",
    "\n",
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "branch_creation = BranchCreation(\n",
    "    name=branch,\n",
    "    source=\"main\",\n",
    ") \n",
    "\n",
    "api_response = api_instance.create_branch(repo, branch_creation)\n",
    "display(api_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e1a416",
   "metadata": {},
   "source": [
    "### List the current branches in the `example` repository\n",
    "\n",
    "https://pydocs.lakefs.io/docs/BranchesApi.html#list_branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "118732d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add_more_user_data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'main'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for b in client.branches.list_branches(repo).results:\n",
    "    display(b.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5926aa",
   "metadata": {},
   "source": [
    "## Confirm that you can see the same data on the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "684ff5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xform_df = spark.read.parquet('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ca0c3",
   "metadata": {},
   "source": [
    "How many rows of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eeb9c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xform_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be899d4b",
   "metadata": {},
   "source": [
    "What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10219381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 07:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " email             | ajordan0@com.com    \n",
      " gender            | Female              \n",
      " ip_address        | 1.197.201.2         \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " birthdate         | 3/8/1971            \n",
      " salary            | 49756.53            \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xform_df.show(n=1,vertical=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185214d",
   "metadata": {},
   "source": [
    "### Note that on S3 there is still just the original 78k object - we've not duplicated any data for the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f5b9723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-16 08:18:24.769000+00:00 1984e4e69777452fb03800bd4c6b1b05 0\n",
      "2022-09-16 08:18:23.607000+00:00 2895bba44b5b4f5199bf58d80144a9de 78869\n",
      "2022-09-16 08:18:22.900000+00:00 49ad588d52b942c9b682f271f228cb9c 0\n",
      "2022-09-16 08:18:25.034000+00:00 5e91c0a09d3a4415abd33bc460662e18 0\n",
      "2022-09-16 08:20:14.890000+00:00 _lakefs/6f2ea862d75cb1a2cdb4a3a76969c2bb134c51538c74c0d144b781ed12165b83 1390\n",
      "2022-09-16 08:20:14.903000+00:00 _lakefs/ae52ce4551b3d0fd1eb49a8568f4a65db007f222953dd2770d55e0f13c4aaeb9 1239\n",
      "2022-09-16 08:18:24.277000+00:00 c4cae27b7c0a46beb14f79347d77c29d 0\n",
      "2022-09-16 08:17:18.615000+00:00 dummy 70\n"
     ]
    }
   ],
   "source": [
    "for o in s3.Bucket(repo).objects.all():\n",
    "    print(o.last_modified, o.key, o.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1378435c",
   "metadata": {},
   "source": [
    "## Add some new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc637861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample parquet file is Apache 2.0 licensed so perhaps include it in the Everything Bagel distribution? \n",
    "url='https://github.com/Teradata/kylo/blob/master/samples/sample-data/parquet/userdata2.parquet?raw=true'\n",
    "sc.addFile(url)\n",
    "df = spark.read.parquet(\"file://\" + SparkFiles.get(\"userdata2.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cabc77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------\n",
      " registration_dttm | 2016-02-03 13:36:39  \n",
      " id                | 1                    \n",
      " first_name        | Donald               \n",
      " last_name         | Lewis                \n",
      " email             | dlewis0@clickbank... \n",
      " gender            | Male                 \n",
      " ip_address        | 102.22.124.20        \n",
      " cc                |                      \n",
      " country           | Indonesia            \n",
      " birthdate         | 7/9/1972             \n",
      " salary            | 140249.37            \n",
      " title             | Senior Financial ... \n",
      " comments          |                      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d337977",
   "metadata": {},
   "source": [
    "## Write the data to the new branch and commit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32412585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('append').parquet('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef8bc98",
   "metadata": {},
   "source": [
    "LakeFS sees that there is an uncommited change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "054d469b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'demo/users/part-00000-b90910aa-58a0-42af-84ec-8ec557c56850-c000.snappy.parquet',\n",
       " 'path_type': 'object',\n",
       " 'size_bytes': 78729,\n",
       " 'type': 'added'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "\n",
    "api_response = api_instance.diff_branch(repo, branch)\n",
    "if api_response.pagination.results==0:\n",
    "    display(\"Nothing to commit\")\n",
    "else:\n",
    "    for r in api_response.results:\n",
    "        display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a958b",
   "metadata": {},
   "source": [
    "Commit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e1ec372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1663316722,\n",
       " 'id': '9d7f809d1733ee0f48d89fd6d5d1d915aaf79200e10f26e997db1437b3414794',\n",
       " 'message': 'Everything Bagel - add more user data',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {'foo': 'bar'},\n",
       " 'parents': ['de2ae0f24961f65bf7b525d983b3dbc869ce49a0dea43529920743154ce0ddd0']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"Everything Bagel - add more user data\",\n",
    "    metadata={\n",
    "        \"foo\": \"bar\",\n",
    "    }\n",
    ") \n",
    "\n",
    "api_instance.commit(repo, branch, commit_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247a98a",
   "metadata": {},
   "source": [
    "## Re-read `main` and `add_more_user_data` branches and count rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639f82a",
   "metadata": {},
   "source": [
    "Original branch (`main`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "66b53af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main = spark.read.parquet('s3a://'+repo+'/main/demo/users')\n",
    "display(main.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45d669",
   "metadata": {},
   "source": [
    "New branch (`add_more_user_data`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdefe7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "add_more_user_data = spark.read.parquet('s3a://'+repo+'/add_more_user_data/demo/users')\n",
    "display(add_more_user_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24fcd35",
   "metadata": {},
   "source": [
    "### Look at the view in LakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127c37e",
   "metadata": {},
   "source": [
    "#### `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f94ce60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'checksum': 'd41d8cd98f00b204e9800998ecf8427e',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663316305,\n",
       "  'path': 'demo/users/_SUCCESS',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/5e91c0a09d3a4415abd33bc460662e18',\n",
       "  'size_bytes': 0},\n",
       " {'checksum': 'addfab49d691a5d4a18ae0b127a792a0',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663316304,\n",
       "  'path': 'demo/users/part-00000-142e4126-aaec-477c-bbcf-c1dd68011369-c000.snappy.parquet',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/2895bba44b5b4f5199bf58d80144a9de',\n",
       "  'size_bytes': 78869}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.objects.list_objects(repo,'main').results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa60e7",
   "metadata": {},
   "source": [
    "#### `add_more_user_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52bb4450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'checksum': 'd41d8cd98f00b204e9800998ecf8427e',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663316556,\n",
       "  'path': 'demo/users/_SUCCESS',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/8acafaeee021459ba6cc29cfb35bb06b',\n",
       "  'size_bytes': 0},\n",
       " {'checksum': 'addfab49d691a5d4a18ae0b127a792a0',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663316304,\n",
       "  'path': 'demo/users/part-00000-142e4126-aaec-477c-bbcf-c1dd68011369-c000.snappy.parquet',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/2895bba44b5b4f5199bf58d80144a9de',\n",
       "  'size_bytes': 78869},\n",
       " {'checksum': '90bee97b5d4bf0675f2684664e5993dc',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663316555,\n",
       "  'path': 'demo/users/part-00000-b90910aa-58a0-42af-84ec-8ec557c56850-c000.snappy.parquet',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/ece3333dd6bd4d6d9ffab3d53fd4e6b2',\n",
       "  'size_bytes': 78729}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.objects.list_objects(repo,'add_more_user_data').results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a630e6",
   "metadata": {},
   "source": [
    "### The data as seen from S3\n",
    "\n",
    "Note that there are just two 78k files; there is no duplication of data shared by branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2183e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-16 08:18:24.769000+00:00 1984e4e69777452fb03800bd4c6b1b05 0\n",
      "2022-09-16 08:18:23.607000+00:00 2895bba44b5b4f5199bf58d80144a9de 78869\n",
      "2022-09-16 08:22:35.628000+00:00 31d780e5af4140e895a7c3779fac985d 0\n",
      "2022-09-16 08:22:36.105000+00:00 3aec1fd8560f43aaa8667d0b2d3a70be 0\n",
      "2022-09-16 08:18:22.900000+00:00 49ad588d52b942c9b682f271f228cb9c 0\n",
      "2022-09-16 08:18:25.034000+00:00 5e91c0a09d3a4415abd33bc460662e18 0\n",
      "2022-09-16 08:22:36.360000+00:00 8acafaeee021459ba6cc29cfb35bb06b 0\n",
      "2022-09-16 08:25:22.454000+00:00 _lakefs/1ddeab3bef4929b69d52e44b94048f47fe17269821ccdf3ba6701c43d691fb34 1239\n",
      "2022-09-16 08:20:14.890000+00:00 _lakefs/6f2ea862d75cb1a2cdb4a3a76969c2bb134c51538c74c0d144b781ed12165b83 1390\n",
      "2022-09-16 08:20:14.903000+00:00 _lakefs/ae52ce4551b3d0fd1eb49a8568f4a65db007f222953dd2770d55e0f13c4aaeb9 1239\n",
      "2022-09-16 08:25:22.447000+00:00 _lakefs/dedf8f514dcf0bb4184e9ebd55ceddcfafc9fb9fd02888566a0ad2052cfb7c12 1609\n",
      "2022-09-16 08:22:34.689000+00:00 b3cf0d04badd411da874a6448eead32f 0\n",
      "2022-09-16 08:18:24.277000+00:00 c4cae27b7c0a46beb14f79347d77c29d 0\n",
      "2022-09-16 08:17:18.615000+00:00 dummy 70\n",
      "2022-09-16 08:22:35.017000+00:00 ece3333dd6bd4d6d9ffab3d53fd4e6b2 78729\n"
     ]
    }
   ],
   "source": [
    "for o in s3.Bucket(repo).objects.all():\n",
    "    print(o.last_modified, o.key, o.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6049a1",
   "metadata": {},
   "source": [
    "## Create a new branch and test removing some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be25ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch='remove_pii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2eb618e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de2ae0f24961f65bf7b525d983b3dbc869ce49a0dea43529920743154ce0ddd0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lakefs_client.model.branch_creation import BranchCreation\n",
    "\n",
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "branch_creation = BranchCreation(\n",
    "    name=branch,\n",
    "    source=\"main\",\n",
    ") \n",
    "\n",
    "api_response = api_instance.create_branch(repo, branch_creation)\n",
    "display(api_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63156cd",
   "metadata": {},
   "source": [
    "### List the current branches in the `example` repository\n",
    "\n",
    "https://pydocs.lakefs.io/docs/BranchesApi.html#list_branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3986834b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add_more_user_data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'main'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'remove_pii'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for b in client.branches.list_branches(repo).results:\n",
    "    display(b.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335d022",
   "metadata": {},
   "source": [
    "### Confirm that you can see the same data on the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d4ecf052",
   "metadata": {},
   "outputs": [],
   "source": [
    "xform_df = spark.read.parquet('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d065d2",
   "metadata": {},
   "source": [
    "How many rows of data? \n",
    "\n",
    "_Note that this shows 1000 per `main`, and not 2000 per the `add_more_user_data` branch above since this has not been merged to `main`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed9e8b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xform_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeace3b0",
   "metadata": {},
   "source": [
    "If you are reading and write a file from the same place, you need to use `.cache()` otherwise the write will fail with an error like this: \n",
    "\n",
    "```\n",
    "Caused by: java.io.FileNotFoundException: \n",
    "No such file or directory: s3a://example/remove_pii/demo/users/part-00000-7a0bbe79-a3e2-4355-984e-bd8b950a4e0c-c000.snappy.parquet\n",
    "```\n",
    "\n",
    "[solution src](https://stackoverflow.com/a/65330116/350613)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25872e",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e95aa00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 07:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " gender            | Female              \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=xform_df.drop('ip_address','birthdate','salary','email').cache()\n",
    "# You need to do something to access the DF otherwise the `cache()` won't have any effect\n",
    "df2.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff402ece",
   "metadata": {},
   "source": [
    "### Write data back to the branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4a7252b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.mode('overwrite').parquet('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e6baf6",
   "metadata": {},
   "source": [
    "### Commit changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f2347aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1663322293,\n",
       " 'id': '12718ce62f97df78beb1d49dcd4c5528a1977d2af1b220f22012f8305e72f768',\n",
       " 'message': 'Remove PII',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {},\n",
       " 'parents': ['de2ae0f24961f65bf7b525d983b3dbc869ce49a0dea43529920743154ce0ddd0']}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"Remove PII\",\n",
    ") \n",
    "\n",
    "api_instance.commit(repo, branch, commit_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b8319",
   "metadata": {},
   "source": [
    "## Re-read all branches and inspect data for isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16909b0",
   "metadata": {},
   "source": [
    "Original branch (`main`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f0eb5364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 07:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " email             | ajordan0@com.com    \n",
      " gender            | Female              \n",
      " ip_address        | 1.197.201.2         \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " birthdate         | 3/8/1971            \n",
      " salary            | 49756.53            \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main = spark.read.parquet('s3a://'+repo+'/main/demo/users')\n",
    "display(main.count())\n",
    "main.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc188c",
   "metadata": {},
   "source": [
    "New branch (`add_more_user_data`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e1da3fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 07:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " email             | ajordan0@com.com    \n",
      " gender            | Female              \n",
      " ip_address        | 1.197.201.2         \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " birthdate         | 3/8/1971            \n",
      " salary            | 49756.53            \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add_more_user_data = spark.read.parquet('s3a://'+repo+'/add_more_user_data/demo/users')\n",
    "display(add_more_user_data.count())\n",
    "add_more_user_data.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be8072",
   "metadata": {},
   "source": [
    "New branch (`remove_pii`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e564292b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 07:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " gender            | Female              \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remove_pii = spark.read.parquet('s3a://'+repo+'/remove_pii/demo/users')\n",
    "display(remove_pii.count())\n",
    "remove_pii.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9cf92",
   "metadata": {},
   "source": [
    "### Look at the view in LakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9545ec95",
   "metadata": {},
   "source": [
    "#### `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a5042d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'checksum': 'd41d8cd98f00b204e9800998ecf8427e',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663321676,\n",
       "  'path': 'demo/users/_SUCCESS',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/998adbec404543d086b10521cf3b3b61',\n",
       "  'size_bytes': 0},\n",
       " {'checksum': 'b072c13161b5d0e7e71aada35cf2fade',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663321676,\n",
       "  'path': 'demo/users/part-00000-9dfcb3c4-1e1f-4ce0-9913-fd285fcc977f-c000.snappy.parquet',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/7776ca0d0f3543a69defb005defdf80c',\n",
       "  'size_bytes': 40559}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.objects.list_objects(repo,'main').results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40d620",
   "metadata": {},
   "source": [
    "#### `add_more_user_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9f18511c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'checksum': 'd41d8cd98f00b204e9800998ecf8427e',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663316556,\n",
       "  'path': 'demo/users/_SUCCESS',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/8acafaeee021459ba6cc29cfb35bb06b',\n",
       "  'size_bytes': 0},\n",
       " {'checksum': 'addfab49d691a5d4a18ae0b127a792a0',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663316304,\n",
       "  'path': 'demo/users/part-00000-142e4126-aaec-477c-bbcf-c1dd68011369-c000.snappy.parquet',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/2895bba44b5b4f5199bf58d80144a9de',\n",
       "  'size_bytes': 78869},\n",
       " {'checksum': '90bee97b5d4bf0675f2684664e5993dc',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663316555,\n",
       "  'path': 'demo/users/part-00000-b90910aa-58a0-42af-84ec-8ec557c56850-c000.snappy.parquet',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/ece3333dd6bd4d6d9ffab3d53fd4e6b2',\n",
       "  'size_bytes': 78729}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.objects.list_objects(repo,'add_more_user_data').results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603d5fb",
   "metadata": {},
   "source": [
    "#### `remove_pii`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3b8770f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'checksum': 'd41d8cd98f00b204e9800998ecf8427e',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663321676,\n",
       "  'path': 'demo/users/_SUCCESS',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/998adbec404543d086b10521cf3b3b61',\n",
       "  'size_bytes': 0},\n",
       " {'checksum': 'b072c13161b5d0e7e71aada35cf2fade',\n",
       "  'content_type': 'application/octet-stream',\n",
       "  'mtime': 1663321676,\n",
       "  'path': 'demo/users/part-00000-9dfcb3c4-1e1f-4ce0-9913-fd285fcc977f-c000.snappy.parquet',\n",
       "  'path_type': 'object',\n",
       "  'physical_address': 's3://example3/7776ca0d0f3543a69defb005defdf80c',\n",
       "  'size_bytes': 40559}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.objects.list_objects(repo,'remove_pii').results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed727c06",
   "metadata": {},
   "source": [
    "## Merge `remove_pii` into `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "41a53f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reference': 'bb4564d3811586db5db202e8553fa5bfc4c5a235c4d8b8ec22ab62f7ab43b34c',\n",
       " 'summary': {'added': 0, 'changed': 0, 'conflict': 0, 'removed': 0}}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.refs.merge_into_branch(repository=repo, source_ref='remove_pii', destination_branch='main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254ff43",
   "metadata": {},
   "source": [
    "Original branch (`main`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1b5d7daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 07:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " gender            | Female              \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main = spark.read.parquet('s3a://'+repo+'/main/demo/users')\n",
    "display(main.count())\n",
    "main.show(n=1,vertical=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
