{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78797e22",
   "metadata": {},
   "source": [
    "# lakeFS and Delta\n",
    "\n",
    "This uses the [Everything Bagel](https://github.com/treeverse/lakeFS/tree/master/deployments/compose) Docker Compose environment.\n",
    "\n",
    "[@rmoff](https://twitter.com/rmoff/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a642edb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ba6754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: /opt/conda/bin/python\n",
      "Python version: 3.9.7 | packaged by conda-forge | (default, Oct 10 2021, 15:08:54) \n",
      "[GCC 9.4.0]\n",
      "PySpark version: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Kernel:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "import pyspark\n",
    "print(\"PySpark version:\", pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3db35",
   "metadata": {},
   "source": [
    "###  Spark\n",
    "\n",
    "_With the necessary Delta Lake config too_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3747db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.0.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\")\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://lakefs:8000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"AKIA-EXAMPLE-KEY\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"EXAMPLE-SECRET\")    \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a7daa",
   "metadata": {},
   "source": [
    "#### Test delta - write/read local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e70664be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(0, 5)\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53a65c0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  0|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd24df2",
   "metadata": {},
   "source": [
    "#### Test delta - write/read lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1d64961",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(0, 5)\n",
    "df.write.format(\"delta\").mode('overwrite').save('s3a://example/main/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5b4b61e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  3|\n",
      "|  2|\n",
      "|  4|\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load('s3a://example/main/test')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824b929",
   "metadata": {},
   "source": [
    "### LakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda53fd",
   "metadata": {},
   "source": [
    "#### Install libraries\n",
    "\n",
    "(could be built into the `Dockerfile`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b7f73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lakefs_client in /opt/conda/lib/python3.9/site-packages (0.98.0)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /opt/conda/lib/python3.9/site-packages (from lakefs_client) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.9/site-packages (from lakefs_client) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil->lakefs_client) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install lakefs_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aad78b",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2e44584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient\n",
    "from lakefs_client.api import branches_api\n",
    "from lakefs_client.api import commits_api\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = 'AKIA-EXAMPLE-KEY'\n",
    "configuration.password = 'EXAMPLE-SECRET'\n",
    "configuration.host = 'http://lakefs:8000'\n",
    "\n",
    "client = LakeFSClient(configuration)\n",
    "api_client = lakefs_client.ApiClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0047139f",
   "metadata": {},
   "source": [
    "#### List the current branches in the repository\n",
    "\n",
    "https://pydocs.lakefs.io/docs/BranchesApi.html#list_branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9651e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo='example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bdaa7756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add_more_user_data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'main'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for b in client.branches.list_branches(repo).results:\n",
    "    display(b.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45118a",
   "metadata": {},
   "source": [
    "## Load some data into lakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22dfb5",
   "metadata": {},
   "source": [
    "Read a parquet file from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a87c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample parquet file is Apache 2.0 licensed so perhaps include it in the Everything Bagel distribution? \n",
    "url='https://github.com/Teradata/kylo/blob/master/samples/sample-data/parquet/userdata1.parquet?raw=true'\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.parquet(\"file://\" + SparkFiles.get(\"userdata1.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22768b",
   "metadata": {},
   "source": [
    "How many rows of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ec747fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f17847",
   "metadata": {},
   "source": [
    "What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6268496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 07:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " email             | ajordan0@com.com    \n",
      " gender            | Female              \n",
      " ip_address        | 1.197.201.2         \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " birthdate         | 3/8/1971            \n",
      " salary            | 49756.53            \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.show(n=1,vertical=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41dab2b",
   "metadata": {},
   "source": [
    "## Write data to lakeFS (on the `main` branch) in Delta format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40be34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch='main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68718621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode('overwrite').save('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963a378",
   "metadata": {},
   "source": [
    "#### 👉🏻[The data as seen from LakeFS](http://localhost:8000/repositories/example/objects?ref=main&path=demo%2Fusers%2F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdcb3c",
   "metadata": {},
   "source": [
    "### Commit the new file in `main`\n",
    "\n",
    "https://pydocs.lakefs.io/docs/CommitsApi.html#commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0078cc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1680719777,\n",
       " 'id': 'f293978bb9ca8fdbe0b7282310c1ef87bd66cafa9f6ea7b7989dccb622962353',\n",
       " 'message': 'Everything Bagel - commit users data (original)',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {'foo': 'bar'},\n",
       " 'parents': ['45576cecd3aa193aeb2a9e62133226b4b9c48e03e44e9d9be3de62d3a0b6977f']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"Everything Bagel - commit users data (original)\",\n",
    "    metadata={\n",
    "        \"foo\": \"bar\",\n",
    "    }\n",
    ") \n",
    "\n",
    "api_instance.commit(repo, branch, commit_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c5e94",
   "metadata": {},
   "source": [
    "## Create a branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fa39ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch='add_more_user_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fecd1769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f293978bb9ca8fdbe0b7282310c1ef87bd66cafa9f6ea7b7989dccb622962353'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lakefs_client.model.branch_creation import BranchCreation\n",
    "\n",
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "branch_creation = BranchCreation(\n",
    "    name=branch,\n",
    "    source=\"main\",\n",
    ") \n",
    "\n",
    "api_response = api_instance.create_branch(repo, branch_creation)\n",
    "display(api_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e1a416",
   "metadata": {},
   "source": [
    "### List the current branches in the `example` repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "118732d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add_more_user_data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'main'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for b in client.branches.list_branches(repo).results:\n",
    "    display(b.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5926aa",
   "metadata": {},
   "source": [
    "### Confirm that you can see the same data on the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "684ff5e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "`s3a://example/remove_pii/demo/users` is not a Delta table.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_528/2694840951.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxform_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3a://'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrepo\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbranch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/demo/users'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: `s3a://example/remove_pii/demo/users` is not a Delta table."
     ]
    }
   ],
   "source": [
    "xform_df = spark.read.format(\"delta\").load('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ca0c3",
   "metadata": {},
   "source": [
    "How many rows of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0eeb9c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xform_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1378435c",
   "metadata": {},
   "source": [
    "## Add some new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc637861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample parquet file is Apache 2.0 licensed so perhaps include it in the Everything Bagel distribution? \n",
    "url='https://github.com/Teradata/kylo/blob/master/samples/sample-data/parquet/userdata2.parquet?raw=true'\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.parquet(\"file://\" + SparkFiles.get(\"userdata2.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cabc77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------\n",
      " registration_dttm | 2016-02-03 13:36:39  \n",
      " id                | 1                    \n",
      " first_name        | Donald               \n",
      " last_name         | Lewis                \n",
      " email             | dlewis0@clickbank... \n",
      " gender            | Male                 \n",
      " ip_address        | 102.22.124.20        \n",
      " cc                |                      \n",
      " country           | Indonesia            \n",
      " birthdate         | 7/9/1972             \n",
      " salary            | 140249.37            \n",
      " title             | Senior Financial ... \n",
      " comments          |                      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d337977",
   "metadata": {},
   "source": [
    "## Write the data to the new branch and commit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32412585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode('append').save('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a958b",
   "metadata": {},
   "source": [
    "Commit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e1ec372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1680719919,\n",
       " 'id': '0b96f0bcc8fd718ae0e35dabf870b128b870961c9b2819399cdaf84db724b473',\n",
       " 'message': 'Everything Bagel - add more user data',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {'foo': 'bar'},\n",
       " 'parents': ['f293978bb9ca8fdbe0b7282310c1ef87bd66cafa9f6ea7b7989dccb622962353']}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"Everything Bagel - add more user data\",\n",
    "    metadata={\n",
    "        \"foo\": \"bar\",\n",
    "    }\n",
    ") \n",
    "\n",
    "api_instance.commit(repo, branch, commit_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247a98a",
   "metadata": {},
   "source": [
    "## Re-read `main` and `add_more_user_data` branches and count rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639f82a",
   "metadata": {},
   "source": [
    "Original branch (`main`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66b53af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main = spark.read.format(\"delta\").load('s3a://'+repo+'/main/demo/users')\n",
    "display(main.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45d669",
   "metadata": {},
   "source": [
    "New branch (`add_more_user_data`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdefe7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "add_more_user_data = spark.read.format(\"delta\").load('s3a://'+repo+'/add_more_user_data/demo/users')\n",
    "display(add_more_user_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24fcd35",
   "metadata": {},
   "source": [
    "### Look at the view in LakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127c37e",
   "metadata": {},
   "source": [
    "#### 👉🏻 [`main`](http://localhost:8000/repositories/example/objects?ref=main&path=demo%2Fusers%2F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa60e7",
   "metadata": {},
   "source": [
    "#### 👉🏻 [`add_more_user_data`](http://localhost:8000/repositories/example/objects?ref=add_more_user_data&path=demo%2Fusers%2F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6049a1",
   "metadata": {},
   "source": [
    "## Create a new branch and test removing some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be25ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch='remove_pii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2eb618e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f293978bb9ca8fdbe0b7282310c1ef87bd66cafa9f6ea7b7989dccb622962353'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lakefs_client.model.branch_creation import BranchCreation\n",
    "\n",
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "branch_creation = BranchCreation(\n",
    "    name=branch,\n",
    "    source=\"main\",\n",
    ") \n",
    "\n",
    "api_response = api_instance.create_branch(repo, branch_creation)\n",
    "display(api_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63156cd",
   "metadata": {},
   "source": [
    "### List the current branches in the `example` repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3986834b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add_more_user_data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'main'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'remove_pii'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for b in client.branches.list_branches(repo).results:\n",
    "    display(b.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335d022",
   "metadata": {},
   "source": [
    "### Confirm that you can see the same data on the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4ecf052",
   "metadata": {},
   "outputs": [],
   "source": [
    "xform_df = spark.read.format(\"delta\").load('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d065d2",
   "metadata": {},
   "source": [
    "How many rows of data? \n",
    "\n",
    "_Note that this shows 1000 per `main`, and not 2000 per the `add_more_user_data` branch above since this has not been merged to `main`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed9e8b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xform_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25872e",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e95aa00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 07:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " gender            | Female              \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=xform_df.drop('ip_address','birthdate','salary','email').cache()\n",
    "# You need to do something to access the DF otherwise the `cache()` won't have any effect\n",
    "df2.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff402ece",
   "metadata": {},
   "source": [
    "### Write data back to the branch and Commit changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a7252b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.format(\"delta\").mode('overwrite').save('s3a://'+repo+'/'+branch+'/demo/users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f2347aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1680722786,\n",
       " 'id': 'b83408039f815b5190be8acc61112370aca8c343d78e072d189bbefcd9a4e399',\n",
       " 'message': 'Remove PII',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {},\n",
       " 'parents': ['70f4715363ba3231b92848de647550a3b25092af1fbcc1f52c4cbfbc27f9658a']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"Remove PII\",\n",
    ") \n",
    "\n",
    "api_instance.commit(repo, branch, commit_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b8319",
   "metadata": {},
   "source": [
    "## Re-read all branches and inspect data for isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16909b0",
   "metadata": {},
   "source": [
    "Original branch (`main`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f0eb5364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 07:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " email             | ajordan0@com.com    \n",
      " gender            | Female              \n",
      " ip_address        | 1.197.201.2         \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " birthdate         | 3/8/1971            \n",
      " salary            | 49756.53            \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main = spark.read.format(\"delta\").load('s3a://'+repo+'/main/demo/users')\n",
    "display(main.count())\n",
    "main.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc188c",
   "metadata": {},
   "source": [
    "New branch (`add_more_user_data`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e1da3fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 07:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " email             | ajordan0@com.com    \n",
      " gender            | Female              \n",
      " ip_address        | 1.197.201.2         \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " birthdate         | 3/8/1971            \n",
      " salary            | 49756.53            \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add_more_user_data = spark.read.format(\"delta\").load('s3a://'+repo+'/add_more_user_data/demo/users')\n",
    "display(add_more_user_data.count())\n",
    "add_more_user_data.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be8072",
   "metadata": {},
   "source": [
    "New branch (`remove_pii`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e564292b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 07:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " gender            | Female              \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remove_pii = spark.read.format(\"delta\").load('s3a://'+repo+'/remove_pii/demo/users')\n",
    "display(remove_pii.count())\n",
    "remove_pii.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9cf92",
   "metadata": {},
   "source": [
    "### Look at the view in LakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeaad6a",
   "metadata": {},
   "source": [
    "#### 👉🏻 [`main`](http://localhost:8000/repositories/example/objects?ref=main&path=demo%2Fusers%2F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2ac9d",
   "metadata": {},
   "source": [
    "#### 👉🏻 [`add_more_user_data`](http://localhost:8000/repositories/example/objects?ref=add_more_user_data&path=demo%2Fusers%2F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29392991",
   "metadata": {},
   "source": [
    "#### 👉🏻 [`remove_pii`](http://localhost:8000/repositories/example/objects?ref=remove_pii&path=demo%2Fusers%2F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed727c06",
   "metadata": {},
   "source": [
    "## Merge `remove_pii` into `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "41a53f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reference': '168643ec141c2535bf42704e88630f812642f98015a720bad9a89946c0cbb1a9',\n",
       " 'summary': {'added': 0, 'changed': 0, 'conflict': 0, 'removed': 0}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.refs.merge_into_branch(repository=repo, source_ref='remove_pii', destination_branch='main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254ff43",
   "metadata": {},
   "source": [
    "Original branch (`main`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b5d7daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 07:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " gender            | Female              \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main = spark.read.format(\"delta\").load('s3a://'+repo+'/main/demo/users')\n",
    "display(main.count())\n",
    "main.show(n=1,vertical=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
