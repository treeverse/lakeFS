<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<title>grand-theft-s3-client.html</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>

</head>

<body>

<h1 id="getting-s3-clients-in-spark-clients">Getting S3 Clients in Spark Clients</h1>
<h2 id="why">Why?</h2>
<p>We have 2 clients for Spark that work on lakeFS but also need to access AWS S3 directly:</p>
<table>
<tr>
<th>
Client
</th>
<th>
Where used
</th>
<th>
Why it needs an S3 client
</th>
</tr>
<tr>
<td>
Spark Metadata client
</td>
<td>
GC (both committed and uncommititted), Spark Export, and also available for users to use to access lakeFS metadata directly.
</td>
<td>
Accesses stored metadata directly on S3.
<td>
</tr>
<tr>
<td>
lakeFSFS
</td>
<td>
Reading and writing directly on lakeFS.
</td>
<td>
Read ETags of uploaded objects to put them in lakeFS metadata. In <em>some</em> Hadoop versions, the S3AFileSystem returns FileStatus objects with a S3AFileStatus.getETag method. Otherwise a separate call to S3 is needed.
</td>
</tr>
</table>
<figure>
<img src="https://static.wikia.nocookie.net/pinkpanther/images/7/76/David_Niven_-_01.webp/revision/latest?cb=20220531105637" alt="David Niven as Sir Charles Litton, The Pink Panther" /><figcaption aria-hidden="true">David Niven as Sir Charles Litton, The Pink Panther</figcaption>
</figure>
<p>These Spark clients cannot work without a working S3 client<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. This is:</p>
<ul>
<li><p><strong>Different</strong> between our two clients.</p>
<p>The Spark metadata client supports <em>only</em> authentication to S3 using access keys or STS, lakeFSFS supports <em>only</em> taking (“<em>stealing</em>”) clients from S3AFileSystem.</p></li>
<li><p><strong>Brittle</strong>.</p>
<p>Some users cannot use the authentication methods that we make available to them. The thievery code in lakeFSFS is subtle and greatly depends on an assumed underlying implementation; it can break when DataBricks introduce new features.</p></li>
<li><p><strong>Uninformative</strong> in the case of system or user error.</p>
<p>Users receive very poor error reports. If they get as far as an S3 client but it is misconfigured, S3 happily generates “400 Bad Request” messages. If client theft fails, it generates a report of the <em>last</em> failure – probably not the most <em>important</em> failure.</p></li>
</ul>
<p>There are numerous bug reports and user questions about this area.</p>
<h2 id="what">What?</h2>
<p>We propose to:</p>
<ol type="1">
<li><strong>Reduce friction.</strong> When S3A already works on a Spark installation, users should typically not have to add <em>any</em> S3-related configuration in order to use lakeFS clients.</li>
<li><strong>Unify</strong> S3 client acquisition between the two clients. Both clients will support the same configuration options: clients “stolen” from the underlying S3AFileSystem, and explicitly created clients with static access keys and STS. Prefer stealing clients to creating them – these are most likely to work.</li>
<li><strong>Improve</strong> error reporting. Report the stages attempted and how each one failed.</li>
<li><strong>Create a more general scheme</strong> for generating clients. Over time we can hope to support more underlying implementations.</li>
</ol>
<h2 id="design-principles">Design principles</h2>
<p>Unify client generation code into a single library. We will be able to test this library individually on various Spark setups. This will probably not usefully be automatic – there is no automatic source for <em>new</em> Spark setups, and it is not clear how often <em>existing</em> Spark setups change. But even being able to run a single command on a Spark cluster and get useful information will be very useful for investigation, helping customers probe their setup, and further development to support setups where we fail.</p>
<p>This library will define an interface for <em>client acquisition</em>: given various parameters TBD (perhaps a SparkContext or a Hadoop configuration), a path, and optionally also a FileSystem on that path, a client acquisition module returns a client or a failure message.</p>
<p>The library will include code that tries each of a list of modules, in order of desirability. It will return a client or throw some exception with a detailed method. And it will report which module was actually used to acquire the client. To increase performance, the library will cache to client used by FileSystem. This will typically mean that the acquisition code is called just once.</p>
<p>The list of modules will be configurable on a Hadoop property. Additionally we will create pre-populated lists, one recommended for no-hassle production use and the other consisting of all (or almost) modules that will be recommended for debugging. Users who explicitly wish to use a single module will simply configure that one module as the only option.</p>
<p>One complication is that many FileSystems are <em>layered</em> and modules to detect them may require some recursion or at least iteration. For instance, while S3A may support <code>S3AFileSystem.getAmazonS3Client</code>, on DataBricks we might have to unwrap it from <code>CredentialScopeFileSystem</code> using <code>CredentialScopeFileSystem.getReadDelegate</code>, and then try to acquire an S3 client from whatever is returned.</p>
<p>The type of the returned client is indeterminate to the caller. It <em>is</em> a AmazonS3Client with desired authentication and the ability to connect to the bucket. But it may well be one of a different version or package than the caller expects, and if the caller so much as attempts to cast it to AmazonS3Client it will get a nasty ClassCastException. Similarly for its <em>Request and </em>Response objects. Everything should be done using reflection, and the library should also help with this call.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<h3 id="example-information-a-module-might-return">Example: information a module might return</h3>
<p>One method of generating a FileSystem is to call <code>getWrappedFs</code> <em>if</em> that FileSystem has such a call, and recurse on that. When such a module fails, it should report:</p>
<ol type="1">
<li>The dynamic type of FileSystem that it received.</li>
<li>What failed:
<ol type="1">
<li><code>getWrappedFs</code>? For instance, if it received a FileSystem that does not have this method.</li>
<li>Acquiring a FileSystem from the wrapped instance? This is a recursive attempt, and its failures will also include information about failure.</li>
</ol></li>
</ol>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>lakeFSFS <em>might</em> not need the client, if it can find an ETag on returned FileStatus objects.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>The current code assumes that the expected Request object has the same actual type and is compatible. This is a bug and will surely break somewhere.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</body>
</html>
